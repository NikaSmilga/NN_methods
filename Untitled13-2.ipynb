{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled13.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zonlbzzslvAF",
        "outputId": "b481c53e-f86a-43ac-abb5-cb17a49547cc"
      },
      "source": [
        "!pip install torchmetrics"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.6.0-py3-none-any.whl (329 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 26.8 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 27.4 MB/s eta 0:00:01\r\u001b[K     |███                             | 30 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████                            | 40 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████                           | 51 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████                          | 61 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |███████                         | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████                        | 81 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 102 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 112 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 122 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 133 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 143 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 153 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 163 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 174 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 184 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 194 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 204 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 215 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 225 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 235 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 245 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 256 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 266 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 276 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 286 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 296 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 307 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 317 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 327 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 329 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.6)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 869
        },
        "id": "yA9IJdGzl_Ai",
        "outputId": "3b46bd37-5d25-4cfe-d364-bf9760724044"
      },
      "source": [
        "!pip install ipdb"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipdb\n",
            "  Downloading ipdb-0.13.9.tar.gz (16 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from ipdb) (57.4.0)\n",
            "Collecting ipython>=7.17.0\n",
            "  Downloading ipython-7.29.0-py3-none-any.whl (790 kB)\n",
            "\u001b[K     |████████████████████████████████| 790 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from ipdb) (0.10.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipdb) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.7.5)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.22-py3-none-any.whl (374 kB)\n",
            "\u001b[K     |████████████████████████████████| 374 kB 51.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.18.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (5.1.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (2.6.1)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.1.3)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (4.8.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.17.0->ipdb) (0.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.17.0->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.17.0->ipdb) (0.2.5)\n",
            "Building wheels for collected packages: ipdb\n",
            "  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipdb: filename=ipdb-0.13.9-py3-none-any.whl size=11648 sha256=c63983c0c4039e28d18b2ec01550ae572119a4650de3197db37df458cb642286\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/cd/cc/aaf92acae337a28fdd2aa4d632196a59745c8c39f76eaeed01\n",
            "Successfully built ipdb\n",
            "Installing collected packages: prompt-toolkit, ipython, ipdb\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.22 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.29.0 which is incompatible.\u001b[0m\n",
            "Successfully installed ipdb-0.13.9 ipython-7.29.0 prompt-toolkit-3.0.22\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqbNUZ2al_6p"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "from torchmetrics import F1\n",
        "from torchmetrics.functional import f1, recall\n",
        "import ipdb"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj7pzdcUmBVV",
        "outputId": "1529f0d3-890f-44a1-e664-04060f64a679"
      },
      "source": [
        "!wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv\n",
        "!wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-26 18:34:47--  https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/r6u59ljhhjdg6j0/negative.csv [following]\n",
            "--2021-11-26 18:34:47--  https://www.dropbox.com/s/raw/r6u59ljhhjdg6j0/negative.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2021-11-26 18:34:47 ERROR 404: Not Found.\n",
            "\n",
            "--2021-11-26 18:34:47--  https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/fnpq3z4bcnoktiv/positive.csv [following]\n",
            "--2021-11-26 18:34:48--  https://www.dropbox.com/s/raw/fnpq3z4bcnoktiv/positive.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2021-11-26 18:34:48 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcXFWXlgmHOA"
      },
      "source": [
        "pos_tweets = pd.read_csv('drive/My Drive/positive.csv', encoding='utf-8', sep=';', header=None,  names=[0,1,2,'text','tone',5,6,7,8,9,10,11])\n",
        "neg_tweets = pd.read_csv('drive/My Drive/negative.csv', encoding='utf-8', sep=';', header=None, names=[0,1,2,'text','tone',5,6,7,8,9,10,11] )\n",
        "neg_tweets['tone'] = 0"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDSC-8IWmcAc"
      },
      "source": [
        "all_tweets_data = pos_tweets.append(neg_tweets)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LluMFRJJmqvj"
      },
      "source": [
        "tweets_data = shuffle(all_tweets_data[['text','tone']])[:100000]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_97SXgHOoEA5"
      },
      "source": [
        "train_sentences, val_sentences = train_test_split(tweets_data, test_size=0.1)"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xeu0bKxoHBY"
      },
      "source": [
        "def preprocess(text):\n",
        "    tokens = text.split()\n",
        "    return tokens"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx2SWW-_f_pR"
      },
      "source": [
        "def preprocess1(text):\n",
        "    tokens = list(text)\n",
        "    return tokens"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQHI5q40plB7",
        "outputId": "0cb86692-0018-43ad-9510-32cc0fa13ed2"
      },
      "source": [
        "vocab = Counter()\n",
        "for text in tweets_data['text']:\n",
        "    vocab.update(preprocess(text))\n",
        "print('всего уникальных токенов:', len(vocab))"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "всего уникальных токенов: 305983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXUyTNg9fb2j",
        "outputId": "5f139916-da42-402a-d07a-d84e3f24bbe1"
      },
      "source": [
        "vocab_sym = Counter()\n",
        "for text in tweets_data['text']:\n",
        "    vocab_sym.update(preprocess1(text))\n",
        "print('всего уникальных токенов:', len(vocab_sym))"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "всего уникальных токенов: 371\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LPWyS7bpmp1",
        "outputId": "3ca66237-5d5a-4330-def2-b626a6468b89"
      },
      "source": [
        "filtered_vocab = set()\n",
        "\n",
        "for word in vocab:\n",
        "    if vocab[word] > 2:\n",
        "        filtered_vocab.add(word)\n",
        "print('уникальных токенов, вcтретившихся больше 2 раз:', len(filtered_vocab))"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "уникальных токенов, вcтретившихся больше 2 раз: 37523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqLGzzdcY6DB",
        "outputId": "77d3167d-f12e-4507-9d27-72c124770358"
      },
      "source": [
        "filtered_vocab_sym = set()\n",
        "\n",
        "for symbol in vocab_sym:\n",
        "    if vocab_sym[symbol] > 5:\n",
        "        filtered_vocab_sym.add(symbol)\n",
        "print('уникальных символов, втретившихся больше 5 раз:', len(filtered_vocab_sym))"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "уникальных символов, втретившихся больше 5 раз: 213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi2CmQaYprrB"
      },
      "source": [
        "word2id = {'PAD':0}\n",
        "\n",
        "for word in filtered_vocab:\n",
        "    word2id[word] = len(word2id)"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpLucfZEV7lT"
      },
      "source": [
        "symbol2id = {'PAD':0}\n",
        "\n",
        "for symbol in filtered_vocab_sym:\n",
        "    symbol2id[symbol] = len(symbol2id)"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4HGCuSrpuHj"
      },
      "source": [
        "id2word = {i:word for word, i in word2id.items()}"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkQzIqEsYbwo"
      },
      "source": [
        "id2symbol = {i:symbol for symbol, i in symbol2id.items()}"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXZo19n9pvTu",
        "outputId": "cc74ee65-e869-4696-9342-3fdb58e9f342"
      },
      "source": [
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "DEVICE"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRvOHM6ftozJ"
      },
      "source": [
        "class TweetsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, word2id, DEVICE):\n",
        "        self.dataset = dataset['text'].values\n",
        "        self.word2id = word2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = dataset['tone'].values\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index): \n",
        "        symbols = self.dataset[index].split()\n",
        "        ids = torch.LongTensor([self.word2id[symbol] for symbol in symbols if symbol in self.word2id])\n",
        "        y = [self.target[index]]\n",
        "        return ids, y\n",
        "\n",
        "    def collate_fn(self, batch): #этот метод можно реализовывать и отдельно,\n",
        "    # он понадобится для DataLoader во время итерации по батчам\n",
        "      ids, y = list(zip(*batch))\n",
        "      padded_ids = pad_sequence(ids, batch_first=True).to(self.device)\n",
        "      y = torch.Tensor(y).to(self.device)\n",
        "      return padded_ids, y"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUcgEN7lto8n"
      },
      "source": [
        "train_dataset = TweetsDataset(train_sentences, word2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n48EBNCYvAQI"
      },
      "source": [
        "val_dataset = TweetsDataset(val_sentences, word2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnssgFoYvAbN"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.conv =  nn.Conv1d(in_channels=180, out_channels=50, kernel_size=2, padding='same')\n",
        "        self.hidden = nn.Linear(in_features=50, out_features=1)\n",
        "        #self.dropout = nn.Dropout(p=0.5)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word):\n",
        "        embedded = self.embedding(word)\n",
        "        embedded = embedded.transpose(1,2)\n",
        "        feature_map_bigrams = self.bigrams(embedded)\n",
        "        feature_map_trigrams = self.trigrams(embedded)\n",
        "        concat = self.conv(torch.cat((feature_map_bigrams, feature_map_trigrams), 1))\n",
        "        pooling = self.pooling(concat).max(2)[0]\n",
        "        logits = self.hidden(pooling) \n",
        "        logits = self.out(logits)      \n",
        "        return logits"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3MpKf4vw9P5"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, print_v=True):\n",
        "    epoch_loss = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, (texts, ys) in enumerate(iterator): \n",
        "        optimizer.zero_grad()  \n",
        "        preds = model(texts) \n",
        "        loss = criterion(preds, ys) \n",
        "        loss.backward()\n",
        "        optimizer.step() \n",
        "        epoch_loss += loss.item() \n",
        "        if print_v == True:\n",
        "          if not (i + 1) % int(len(iterator)/5):\n",
        "              print(f'Train loss: {epoch_loss/i}')      \n",
        "    return  epoch_loss / len(iterator) "
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq3J2NVew_u7"
      },
      "source": [
        "def evaluate(model, iterator, criterion, print_v=True):\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for i, (texts, ys) in enumerate(iterator):   \n",
        "            preds = model(texts) \n",
        "            loss = criterion(preds, ys)  \n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = f1(preds.round().long(), ys.long(), ignore_index=0)\n",
        "            epoch_metric += batch_metric\n",
        "            if print_v == True:\n",
        "              if not (i + 1) % int(len(iterator)/5):\n",
        "                print(f'Val loss: {epoch_loss/(i+1)}, Val f1: {epoch_metric/(i+1)}')\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSoFQpdSxEUY"
      },
      "source": [
        "batch, y = next(iter(train_iterator))\n",
        "batch, y = batch.to(device='cpu'), y.to(device='cpu')\n",
        "loss = nn.BCELoss()\n",
        "\n",
        "model = CNN(len(word2id), 8)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNB4C4DTxEdU",
        "outputId": "6577e5ea-a07a-40bf-9d4d-8af36f89b959"
      },
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(20):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.7286937236785889\n",
            "Train loss: 0.6984131679390416\n",
            "Train loss: 0.684371041059494\n",
            "Train loss: 0.6745014404183003\n",
            "Train loss: 0.6663311145135334\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.625055565553553, Val f1: 0.6470467448234558\n",
            "Val loss: 0.6256747982081245, Val f1: 0.6463380455970764\n",
            "Val loss: 0.6262958938000249, Val f1: 0.6451961398124695\n",
            "Val loss: 0.6264229136354783, Val f1: 0.644739031791687\n",
            "Val loss: 0.6268647902152118, Val f1: 0.6434727311134338\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6277703642845154, Val f1: 0.6479059457778931\n",
            "Val loss: 0.6253319978713989, Val f1: 0.6455658674240112\n",
            "Val loss: 0.6272576153278351, Val f1: 0.6416605710983276\n",
            "Val loss: 0.6293370053172112, Val f1: 0.6441807150840759\n",
            "Val loss: 0.629428094625473, Val f1: 0.6415868997573853\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.663555920124054\n",
            "Train loss: 0.6401610410574711\n",
            "Train loss: 0.6294171917438507\n",
            "Train loss: 0.6244921915566743\n",
            "Train loss: 0.6200444840249562\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5939146701027366, Val f1: 0.6728751063346863\n",
            "Val loss: 0.5935535623746759, Val f1: 0.6770548224449158\n",
            "Val loss: 0.5940851534114164, Val f1: 0.6778370141983032\n",
            "Val loss: 0.5945054555640501, Val f1: 0.6779258847236633\n",
            "Val loss: 0.5952225152183982, Val f1: 0.6775022745132446\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6004745960235596, Val f1: 0.6770904064178467\n",
            "Val loss: 0.5981444716453552, Val f1: 0.6737810969352722\n",
            "Val loss: 0.599875251452128, Val f1: 0.668044924736023\n",
            "Val loss: 0.6015741229057312, Val f1: 0.6708852648735046\n",
            "Val loss: 0.6013722002506257, Val f1: 0.6688247919082642\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.631047822535038\n",
            "Train loss: 0.6100666938406049\n",
            "Train loss: 0.6018182408809661\n",
            "Train loss: 0.5979306030629287\n",
            "Train loss: 0.5939754183803286\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5762919538161334, Val f1: 0.6622412204742432\n",
            "Val loss: 0.5749306591118083, Val f1: 0.6693395376205444\n",
            "Val loss: 0.5759529705141105, Val f1: 0.6658464670181274\n",
            "Val loss: 0.5756206705289728, Val f1: 0.6657403111457825\n",
            "Val loss: 0.5749305234235875, Val f1: 0.6664003729820251\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5832887887954712, Val f1: 0.665282130241394\n",
            "Val loss: 0.5819579213857651, Val f1: 0.6572452783584595\n",
            "Val loss: 0.5835900704065958, Val f1: 0.6522144079208374\n",
            "Val loss: 0.5854058936238289, Val f1: 0.654626727104187\n",
            "Val loss: 0.5842509269714355, Val f1: 0.6536266207695007\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.6123826242983341\n",
            "Train loss: 0.5896179061947446\n",
            "Train loss: 0.5831259262561798\n",
            "Train loss: 0.578398891349337\n",
            "Train loss: 0.5746864548751286\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5523045974619248, Val f1: 0.6934942007064819\n",
            "Val loss: 0.5566038576995626, Val f1: 0.6901665329933167\n",
            "Val loss: 0.555090699710098, Val f1: 0.6908294558525085\n",
            "Val loss: 0.5555261338458342, Val f1: 0.6912432312965393\n",
            "Val loss: 0.5552798951373381, Val f1: 0.6905440092086792\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5660163164138794, Val f1: 0.6865392327308655\n",
            "Val loss: 0.5660721808671951, Val f1: 0.6788065433502197\n",
            "Val loss: 0.5680366158485413, Val f1: 0.6727182269096375\n",
            "Val loss: 0.5694640800356865, Val f1: 0.6773507595062256\n",
            "Val loss: 0.5685056567192077, Val f1: 0.6766247749328613\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.5958674773573875\n",
            "Train loss: 0.5726382985259547\n",
            "Train loss: 0.5651288461685181\n",
            "Train loss: 0.5604110208909903\n",
            "Train loss: 0.5571773371526173\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.54582481173908, Val f1: 0.7182708978652954\n",
            "Val loss: 0.5416154055034413, Val f1: 0.7211225032806396\n",
            "Val loss: 0.5418234327260185, Val f1: 0.7206403017044067\n",
            "Val loss: 0.5417968420421376, Val f1: 0.7185667753219604\n",
            "Val loss: 0.5412915931028478, Val f1: 0.7185287475585938\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5533066987991333, Val f1: 0.7125197649002075\n",
            "Val loss: 0.5535758286714554, Val f1: 0.7065833806991577\n",
            "Val loss: 0.5556904375553131, Val f1: 0.7025836706161499\n",
            "Val loss: 0.5567484945058823, Val f1: 0.7080115079879761\n",
            "Val loss: 0.5566937088966369, Val f1: 0.7044070959091187\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.5680490881204605\n",
            "Train loss: 0.5525844638997858\n",
            "Train loss: 0.5470991230010986\n",
            "Train loss: 0.5428585908306178\n",
            "Train loss: 0.5416061253774733\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5290759135695065, Val f1: 0.7326623201370239\n",
            "Val loss: 0.5277073926785413, Val f1: 0.7343224287033081\n",
            "Val loss: 0.5271832580659904, Val f1: 0.7351119518280029\n",
            "Val loss: 0.5277787552160376, Val f1: 0.734903872013092\n",
            "Val loss: 0.5270753615042743, Val f1: 0.7355396151542664\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.542200893163681, Val f1: 0.7274057865142822\n",
            "Val loss: 0.5436761528253555, Val f1: 0.7222152948379517\n",
            "Val loss: 0.546160896619161, Val f1: 0.7164379358291626\n",
            "Val loss: 0.546764962375164, Val f1: 0.7219251990318298\n",
            "Val loss: 0.5463339507579803, Val f1: 0.7186498641967773\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.5593005027621984\n",
            "Train loss: 0.5429909870480046\n",
            "Train loss: 0.534382410645485\n",
            "Train loss: 0.5318706520457765\n",
            "Train loss: 0.5273908642785889\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5110873807879055, Val f1: 0.734269917011261\n",
            "Val loss: 0.5106911396279055, Val f1: 0.7364588379859924\n",
            "Val loss: 0.5117898267858169, Val f1: 0.7367882132530212\n",
            "Val loss: 0.5123035806943389, Val f1: 0.7365652918815613\n",
            "Val loss: 0.5114643030306872, Val f1: 0.737438976764679\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5286184549331665, Val f1: 0.7277809381484985\n",
            "Val loss: 0.5312526822090149, Val f1: 0.7205516695976257\n",
            "Val loss: 0.5334244271119436, Val f1: 0.7148856520652771\n",
            "Val loss: 0.5339396968483925, Val f1: 0.7201980948448181\n",
            "Val loss: 0.5335838735103607, Val f1: 0.7173750400543213\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.53976871073246\n",
            "Train loss: 0.5231546461582184\n",
            "Train loss: 0.5190676683187485\n",
            "Train loss: 0.515570332310093\n",
            "Train loss: 0.5124958767777398\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.49179287342464223, Val f1: 0.7349615097045898\n",
            "Val loss: 0.49163659004604116, Val f1: 0.7316117882728577\n",
            "Val loss: 0.49484937798743156, Val f1: 0.7302033305168152\n",
            "Val loss: 0.495646810706924, Val f1: 0.7298219799995422\n",
            "Val loss: 0.49582704971818364, Val f1: 0.7303093075752258\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5196169912815094, Val f1: 0.7160874605178833\n",
            "Val loss: 0.5213086158037186, Val f1: 0.7088816165924072\n",
            "Val loss: 0.5231457650661469, Val f1: 0.7071475386619568\n",
            "Val loss: 0.5237995460629463, Val f1: 0.7104289531707764\n",
            "Val loss: 0.5227169036865235, Val f1: 0.7077831625938416\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.5229388531297445\n",
            "Train loss: 0.5080660852518949\n",
            "Train loss: 0.5014882075786591\n",
            "Train loss: 0.4999087177105804\n",
            "Train loss: 0.4981890746525356\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4845012408845565, Val f1: 0.7454688549041748\n",
            "Val loss: 0.48432933670632977, Val f1: 0.7485645413398743\n",
            "Val loss: 0.48466704639734004, Val f1: 0.7483100295066833\n",
            "Val loss: 0.48354941518867717, Val f1: 0.7488516569137573\n",
            "Val loss: 0.4829821965273689, Val f1: 0.7492281198501587\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.509470209479332, Val f1: 0.7287980318069458\n",
            "Val loss: 0.5125932469964027, Val f1: 0.7200166583061218\n",
            "Val loss: 0.5135698467493057, Val f1: 0.7182985544204712\n",
            "Val loss: 0.5139491222798824, Val f1: 0.7242953777313232\n",
            "Val loss: 0.5131558805704117, Val f1: 0.7210615873336792\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.5127113666385412\n",
            "Train loss: 0.49591972520857147\n",
            "Train loss: 0.4911044317483902\n",
            "Train loss: 0.48803359492501214\n",
            "Train loss: 0.4854764799986567\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4710228110060972, Val f1: 0.7391120195388794\n",
            "Val loss: 0.47380152782973123, Val f1: 0.7375478744506836\n",
            "Val loss: 0.47303366368892147, Val f1: 0.7380590438842773\n",
            "Val loss: 0.47273435618947535, Val f1: 0.7380417585372925\n",
            "Val loss: 0.4719133906504687, Val f1: 0.7373369932174683\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5025663524866104, Val f1: 0.7204998731613159\n",
            "Val loss: 0.5066776648163795, Val f1: 0.7096097469329834\n",
            "Val loss: 0.5080489466587702, Val f1: 0.7072917222976685\n",
            "Val loss: 0.5085445232689381, Val f1: 0.7110130190849304\n",
            "Val loss: 0.5071463525295258, Val f1: 0.7062177658081055\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.5001665595918894\n",
            "Train loss: 0.485263343110229\n",
            "Train loss: 0.47976567685604093\n",
            "Train loss: 0.4765310594395025\n",
            "Train loss: 0.4734593126035872\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4613649143892176, Val f1: 0.7481597065925598\n",
            "Val loss: 0.46196141313104067, Val f1: 0.748692512512207\n",
            "Val loss: 0.46110037611980065, Val f1: 0.7484054565429688\n",
            "Val loss: 0.45967112832209645, Val f1: 0.7495552897453308\n",
            "Val loss: 0.4588159515577204, Val f1: 0.750576913356781\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4941256195306778, Val f1: 0.7238002419471741\n",
            "Val loss: 0.4976209029555321, Val f1: 0.7171972990036011\n",
            "Val loss: 0.498792365193367, Val f1: 0.7142857909202576\n",
            "Val loss: 0.4992469698190689, Val f1: 0.719340443611145\n",
            "Val loss: 0.49783254265785215, Val f1: 0.7175451517105103\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.4930862132459879\n",
            "Train loss: 0.47522880723982147\n",
            "Train loss: 0.46811036884784696\n",
            "Train loss: 0.46466385117217673\n",
            "Train loss: 0.463473535719372\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4486655540326062, Val f1: 0.7654423713684082\n",
            "Val loss: 0.44869960932170644, Val f1: 0.7643404006958008\n",
            "Val loss: 0.44671682516733807, Val f1: 0.7641894221305847\n",
            "Val loss: 0.4469693807118079, Val f1: 0.7639715075492859\n",
            "Val loss: 0.44728999243063083, Val f1: 0.7637540102005005\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4846988916397095, Val f1: 0.7372868061065674\n",
            "Val loss: 0.4896107688546181, Val f1: 0.7299021482467651\n",
            "Val loss: 0.4903709242741267, Val f1: 0.727118968963623\n",
            "Val loss: 0.49112245440483093, Val f1: 0.7308006286621094\n",
            "Val loss: 0.4898248463869095, Val f1: 0.7293457388877869\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.4755960088223219\n",
            "Train loss: 0.46039214820572827\n",
            "Train loss: 0.45653290033340455\n",
            "Train loss: 0.45158882238971654\n",
            "Train loss: 0.45034929506835486\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.43557368832476, Val f1: 0.7619792222976685\n",
            "Val loss: 0.43441902364001556, Val f1: 0.7598413228988647\n",
            "Val loss: 0.4380531638276343, Val f1: 0.7581300735473633\n",
            "Val loss: 0.43938610658926125, Val f1: 0.7560631632804871\n",
            "Val loss: 0.4394041860804838, Val f1: 0.7568342685699463\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4821948707103729, Val f1: 0.7262163162231445\n",
            "Val loss: 0.4889000952243805, Val f1: 0.7219382524490356\n",
            "Val loss: 0.4890945454438527, Val f1: 0.7184538245201111\n",
            "Val loss: 0.4895790070295334, Val f1: 0.7212516069412231\n",
            "Val loss: 0.4877244770526886, Val f1: 0.7185523509979248\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.4607311449944973\n",
            "Train loss: 0.44846230564695416\n",
            "Train loss: 0.4434084904193878\n",
            "Train loss: 0.4424893082077824\n",
            "Train loss: 0.4415974400582768\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.42263532035491047, Val f1: 0.7904436588287354\n",
            "Val loss: 0.42481651376275453, Val f1: 0.7895699739456177\n",
            "Val loss: 0.4255889835310917, Val f1: 0.7896812558174133\n",
            "Val loss: 0.42808285487048764, Val f1: 0.7875220775604248\n",
            "Val loss: 0.42708430500591504, Val f1: 0.78805011510849\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4737738221883774, Val f1: 0.7574963569641113\n",
            "Val loss: 0.47913477569818497, Val f1: 0.7523480653762817\n",
            "Val loss: 0.47936322788397473, Val f1: 0.7479534149169922\n",
            "Val loss: 0.4799669533967972, Val f1: 0.7514876127243042\n",
            "Val loss: 0.47876615822315216, Val f1: 0.7488360404968262\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.45232170820236206\n",
            "Train loss: 0.4401567650563789\n",
            "Train loss: 0.4338760310411453\n",
            "Train loss: 0.4316037955568798\n",
            "Train loss: 0.4297641476704961\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.41060293246718016, Val f1: 0.7927636504173279\n",
            "Val loss: 0.41495515756747303, Val f1: 0.7896082401275635\n",
            "Val loss: 0.4149923575859444, Val f1: 0.7886727452278137\n",
            "Val loss: 0.41546325341743584, Val f1: 0.7892147898674011\n",
            "Val loss: 0.415784834763583, Val f1: 0.7886559963226318\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4676484018564224, Val f1: 0.7548770904541016\n",
            "Val loss: 0.47338732331991196, Val f1: 0.7490886449813843\n",
            "Val loss: 0.4735209693511327, Val f1: 0.7458803057670593\n",
            "Val loss: 0.47386493906378746, Val f1: 0.7495012879371643\n",
            "Val loss: 0.47283966839313507, Val f1: 0.7470003962516785\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.4441411644220352\n",
            "Train loss: 0.43212675325798267\n",
            "Train loss: 0.4253487879037857\n",
            "Train loss: 0.42130396081440485\n",
            "Train loss: 0.42005178722597303\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4073722309925977, Val f1: 0.7839112877845764\n",
            "Val loss: 0.41007571010028615, Val f1: 0.7838574051856995\n",
            "Val loss: 0.40895990180034264, Val f1: 0.7872523665428162\n",
            "Val loss: 0.4081077961360707, Val f1: 0.7874394059181213\n",
            "Val loss: 0.40725057580891777, Val f1: 0.7881903648376465\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.46268637478351593, Val f1: 0.7479667663574219\n",
            "Val loss: 0.46928002685308456, Val f1: 0.741926908493042\n",
            "Val loss: 0.4695260028044383, Val f1: 0.7415661811828613\n",
            "Val loss: 0.47025636211037636, Val f1: 0.7447913289070129\n",
            "Val loss: 0.46924318075180055, Val f1: 0.7429043054580688\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.4304438140243292\n",
            "Train loss: 0.42014698458440375\n",
            "Train loss: 0.41552522122859953\n",
            "Train loss: 0.4120022716806896\n",
            "Train loss: 0.410247755192575\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.39685622558874245, Val f1: 0.7963843941688538\n",
            "Val loss: 0.39467394877882567, Val f1: 0.7986064553260803\n",
            "Val loss: 0.3971417826764724, Val f1: 0.7968035936355591\n",
            "Val loss: 0.3967874050140381, Val f1: 0.7981045842170715\n",
            "Val loss: 0.39811978270025816, Val f1: 0.7982898354530334\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4606483578681946, Val f1: 0.759642481803894\n",
            "Val loss: 0.46673523634672165, Val f1: 0.753759503364563\n",
            "Val loss: 0.46617988248666126, Val f1: 0.7507098913192749\n",
            "Val loss: 0.4662746712565422, Val f1: 0.7541440725326538\n",
            "Val loss: 0.46475653946399687, Val f1: 0.7519890666007996\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.4163274522870779\n",
            "Train loss: 0.4074824271780072\n",
            "Train loss: 0.40328274965286254\n",
            "Train loss: 0.4023163042851348\n",
            "Train loss: 0.40115982861745925\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.38856083329986124, Val f1: 0.8080904483795166\n",
            "Val loss: 0.3887282618704964, Val f1: 0.8083315491676331\n",
            "Val loss: 0.3881618421451718, Val f1: 0.8077987432479858\n",
            "Val loss: 0.388661046676776, Val f1: 0.8075634241104126\n",
            "Val loss: 0.3888860579799203, Val f1: 0.8074899315834045\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4569265991449356, Val f1: 0.7638257741928101\n",
            "Val loss: 0.46228180080652237, Val f1: 0.7613072395324707\n",
            "Val loss: 0.4620727946360906, Val f1: 0.7576777935028076\n",
            "Val loss: 0.46260374784469604, Val f1: 0.761442244052887\n",
            "Val loss: 0.46178130209445956, Val f1: 0.760231614112854\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.41318269073963165\n",
            "Train loss: 0.40279470338965906\n",
            "Train loss: 0.39788303792476654\n",
            "Train loss: 0.3954464615280949\n",
            "Train loss: 0.3940588817709968\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3853318603599773, Val f1: 0.8135057091712952\n",
            "Val loss: 0.3795703500509262, Val f1: 0.819579005241394\n",
            "Val loss: 0.3807034708705603, Val f1: 0.817653238773346\n",
            "Val loss: 0.38144803573103514, Val f1: 0.8173506259918213\n",
            "Val loss: 0.3830671878422008, Val f1: 0.8167837262153625\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4575253129005432, Val f1: 0.7718273997306824\n",
            "Val loss: 0.46211133897304535, Val f1: 0.7696331739425659\n",
            "Val loss: 0.4614422470331192, Val f1: 0.7664203643798828\n",
            "Val loss: 0.4618358835577965, Val f1: 0.7684525847434998\n",
            "Val loss: 0.4610895961523056, Val f1: 0.7684246897697449\n",
            "\n",
            "starting Epoch 19\n",
            "Training...\n",
            "Train loss: 0.40621840208768845\n",
            "Train loss: 0.39118733731183136\n",
            "Train loss: 0.38671693921089173\n",
            "Train loss: 0.38558719184861256\n",
            "Train loss: 0.3851723436798368\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.369997617076425, Val f1: 0.8220078945159912\n",
            "Val loss: 0.3762599378824234, Val f1: 0.8161685466766357\n",
            "Val loss: 0.37637166006892336, Val f1: 0.8156190514564514\n",
            "Val loss: 0.3738106270046795, Val f1: 0.8167084455490112\n",
            "Val loss: 0.37329345801297353, Val f1: 0.8169693350791931\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.45021623373031616, Val f1: 0.7690938711166382\n",
            "Val loss: 0.4555612653493881, Val f1: 0.7671328783035278\n",
            "Val loss: 0.4548175036907196, Val f1: 0.7634465098381042\n",
            "Val loss: 0.4557584337890148, Val f1: 0.7659386396408081\n",
            "Val loss: 0.4549687594175339, Val f1: 0.7654962539672852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUui5ZU1hPEm"
      },
      "source": [
        "def predict(model, iterator):\n",
        "    model.eval()\n",
        "    fp = []\n",
        "    fn = []\n",
        "    tp = [] \n",
        "    tn = []\n",
        "    with torch.no_grad():\n",
        "        for i, (texts, ys) in enumerate(iterator):   \n",
        "            preds = model(texts)  # делаем предсказания на тесте \n",
        "            for pred, gold, text in zip(preds, ys, texts):\n",
        "              text = ' '.join([id2word[int(symbol)] for symbol in text if symbol !=0])\n",
        "              if round(pred.item()) > gold:\n",
        "                fp.append((text, gold))\n",
        "              elif round(pred.item()) < gold:\n",
        "                fn.append((text, gold))\n",
        "              elif round(pred.item()) == gold == 1:\n",
        "                tp.append((text, gold))\n",
        "              elif round(pred.item()) == gold == 0:\n",
        "                tn.append((text, gold))\n",
        "    accuracy = (len(tp)+len(tn))/(len(tp)+len(fp)+len(fn)+len(tn))\n",
        "    precision = len(tp)/(len(tp)+len(fp))\n",
        "    recall = len(tp)/(len(tp)+len(fn))\n",
        "    return fp, fn, tp, tn, accuracy, precision, recall"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "nsJPvP7iCMl6",
        "outputId": "7b27bc00-2cd2-439b-fd09-31c1a1facdc4"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(losses)\n",
        "plt.plot(losses_eval)\n",
        "plt.title('BCE loss value')\n",
        "plt.ylabel('BCE loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "fNCKDMtNCTHI",
        "outputId": "88609e7c-71f3-4e4b-eb74-40f7a6aa80a1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(f1s)\n",
        "plt.plot(f1s_eval)\n",
        "plt.title('f1 value')\n",
        "plt.ylabel('f1 value')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P8OYwMRhVg5"
      },
      "source": [
        "fp, fn, tp, tn, accuracy_old, precision_old, recall_old = predict(model, val_iterator)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG6qpKF7hbUM",
        "outputId": "ef9d0534-f6ac-4c24-a3fd-cba344c0de3a"
      },
      "source": [
        "print('accuracy:', accuracy_old)\n",
        "print('precision:', precision_old)\n",
        "print('recall:', recall_old)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.7586\n",
            "precision: 0.7533358749523447\n",
            "recall: 0.7791798107255521\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL9M6w2osZQ8"
      },
      "source": [
        "# Подбираем гиперпараметры для нашей модели 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNG-2wHwse4c"
      },
      "source": [
        "Я решила посмотреть два параметры, размеры эмбеддинга и learning rate. Сначала подберу наилучший размер эмбеддинга, затем -- learning rate. После этого переучу модель с лучшими параметрами и посмотрю, что выйдет. Подбираю я на 10 эпохах (иначе слишком долго), переучивать буду на 20, как учила в прошлой части."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T31rEutti6P1",
        "outputId": "64b03e6c-d31d-47f4-f72e-c0d6c250152a"
      },
      "source": [
        "best_acc = 0\n",
        "emb_val = ''\n",
        "\n",
        "for emb in [5, 10, 20, 50, 100]:\n",
        "    model = CNN(len(word2id), emb)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "    criterion = nn.BCELoss()  \n",
        "    model = model.to(DEVICE)\n",
        "    criterion = criterion.to(DEVICE)\n",
        "\n",
        "    losses = []\n",
        "    losses_eval = []\n",
        "    f1s = []\n",
        "    f1s_eval = []\n",
        "\n",
        "    for i in range(10):\n",
        "        print(f'starting Epoch {i}')\n",
        "        epoch_loss = train(model, train_iterator, optimizer, criterion, print_v=False)\n",
        "        losses.append(epoch_loss)\n",
        "        f1_on_train,_ = evaluate(model, train_iterator, criterion, print_v=False)\n",
        "        f1s.append(f1_on_train)\n",
        "        f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion, print_v=False)\n",
        "        losses_eval.append(epoch_loss_on_test)\n",
        "        f1s_eval.append(f1_on_test)\n",
        "    fp, fn, tp, tn, accuracy, precision, recall = predict(model, val_iterator)\n",
        "    print(f'results for embedding_size={emb}')\n",
        "    print('accuracy:', accuracy)\n",
        "    print('precision:', precision)\n",
        "    print('recall:', recall)\n",
        "    if accuracy > best_acc:\n",
        "      best_acc = accuracy\n",
        "      emb_val = emb\n",
        "print(f'____________________________________________')\n",
        "print(f'best accuracy is {best_acc} for emb_size {emb_val}')"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for embedding_size=5\n",
            "accuracy: 0.7061\n",
            "precision: 0.7044278320874066\n",
            "recall: 0.7245662460567823\n",
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for embedding_size=10\n",
            "accuracy: 0.7335\n",
            "precision: 0.7189376023285429\n",
            "recall: 0.7791798107255521\n",
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for embedding_size=20\n",
            "accuracy: 0.7594\n",
            "precision: 0.7617831893165751\n",
            "recall: 0.7647870662460567\n",
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for embedding_size=50\n",
            "accuracy: 0.7741\n",
            "precision: 0.7971688147052609\n",
            "recall: 0.7438880126182965\n",
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for embedding_size=100\n",
            "accuracy: 0.7839\n",
            "precision: 0.7927982297324482\n",
            "recall: 0.7770110410094637\n",
            "____________________________________________\n",
            "best accuracy is 0.7839 for emb_size 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMLeJ5zklIih",
        "outputId": "82def431-3d82-4f72-ccc7-6ddec6936b89"
      },
      "source": [
        "best_acc = 0\n",
        "lr_val = ''\n",
        "\n",
        "for lr in [0.0005, 0.001, 0.005, 0.01, 0.05]:\n",
        "    model = CNN(len(word2id), emb_val)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCELoss()  \n",
        "\n",
        "    # веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "    model = model.to(DEVICE)\n",
        "    criterion = criterion.to(DEVICE)\n",
        "\n",
        "    losses = []\n",
        "    losses_eval = []\n",
        "    f1s = []\n",
        "    f1s_eval = []\n",
        "\n",
        "    for i in range(10):\n",
        "        print(f'starting Epoch {i}')\n",
        "        epoch_loss = train(model, train_iterator, optimizer, criterion, print_v=False)\n",
        "        losses.append(epoch_loss)\n",
        "        f1_on_train,_ = evaluate(model, train_iterator, criterion, print_v=False)\n",
        "        f1s.append(f1_on_train)\n",
        "        f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion, print_v=False)\n",
        "        losses_eval.append(epoch_loss_on_test)\n",
        "        f1s_eval.append(f1_on_test)\n",
        "    fp, fn, tp, tn, accuracy, precision, recall = predict(model, val_iterator)\n",
        "    print(f'results for lr={lr}')\n",
        "    print('accuracy:', accuracy)\n",
        "    print('precision:', precision)\n",
        "    print('recall:', recall)\n",
        "    if accuracy > best_acc:\n",
        "      best_acc = accuracy\n",
        "      lr_val = lr\n",
        "print(f'____________________________________________')\n",
        "print(f'best accuracy is {best_acc} for lr {lr_val}')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for lr=0.0005\n",
            "accuracy: 0.7895\n",
            "precision: 0.7937042169867353\n",
            "recall: 0.7904179810725552\n",
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for lr=0.001\n",
            "accuracy: 0.8037\n",
            "precision: 0.8029623854999025\n",
            "recall: 0.8123028391167192\n",
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for lr=0.005\n",
            "accuracy: 0.8058\n",
            "precision: 0.8146361077603539\n",
            "recall: 0.7988958990536278\n",
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for lr=0.01\n",
            "accuracy: 0.8137\n",
            "precision: 0.8061438656744896\n",
            "recall: 0.8330047318611987\n",
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for lr=0.05\n",
            "accuracy: 0.5072\n",
            "precision: 0.5072\n",
            "recall: 1.0\n",
            "____________________________________________\n",
            "best accuracy is 0.8137 for lr 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3cnJYIZnX71",
        "outputId": "67b3303b-8e11-4a89-94ab-e55a31b4c26b"
      },
      "source": [
        "model = CNN(len(word2id), emb_val)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr_val)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)\n",
        "\n",
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(20):\n",
        "    print(f'starting Epoch {i}')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion, print_v=False)\n",
        "    losses.append(epoch_loss)\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion, print_v=False)\n",
        "    f1s.append(f1_on_train)\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion, print_v=False)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)\n",
        "fp, fn, tp, tn, accuracy_new, precision_new, recall_new = predict(model, val_iterator)\n",
        "print('Значения после подбора гипепараметров:')\n",
        "print(f'accuracy: {accuracy_new}, improved by {accuracy_new-accuracy_old}')\n",
        "print(f'precision: {precision_new}, improved by {precision_new-precision_old}')\n",
        "print(f'recall: {recall_new}, improved by {recall_new-recall_old}')"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "starting Epoch 10\n",
            "starting Epoch 11\n",
            "starting Epoch 12\n",
            "starting Epoch 13\n",
            "starting Epoch 14\n",
            "starting Epoch 15\n",
            "starting Epoch 16\n",
            "starting Epoch 17\n",
            "starting Epoch 18\n",
            "starting Epoch 19\n",
            "Значения после подбора гипепараметров:\n",
            "accuracy: 0.8082, improved by 0.04959999999999998\n",
            "precision: 0.818328623334679, improved by 0.06499274838233438\n",
            "recall: 0.7992902208201893, improved by 0.02011041009463721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRMsPzJYras5"
      },
      "source": [
        "# Делаем модельку с предварительно обученными нами w2v векторами."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Jdem-6DsVes"
      },
      "source": [
        "import gensim\n",
        "texts = all_tweets_data.text.apply(preprocess).tolist()\n",
        "w2v = gensim.models.Word2Vec(texts, size=100, window=5, min_count=1)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCoxdbQ7FhKY"
      },
      "source": [
        "weights = np.zeros((len(word2id), 100))\n",
        "count = 0\n",
        "for word, i in word2id.items():\n",
        "    if word == 'PAD':\n",
        "        continue   \n",
        "    try:\n",
        "        weights[i] = w2v.wv[word]    \n",
        "    except KeyError:\n",
        "      count += 1\n",
        "      weights[i] = np.random.normal(0,0.1,100)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVjmQw1tFxjr"
      },
      "source": [
        "class CNN_w2v(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.from_pretrained(torch.tensor(weights), freeze=True)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.conv =  nn.Conv1d(in_channels=180, out_channels=50, kernel_size=2, padding='same')\n",
        "        self.hidden = nn.Linear(in_features=50, out_features=1)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word):\n",
        "        embedded = self.embedding(word)\n",
        "        embedded = embedded.transpose(1,2)\n",
        "        feature_map_bigrams = self.bigrams(embedded)\n",
        "        feature_map_trigrams = self.trigrams(embedded)\n",
        "        concat = self.conv(torch.cat((feature_map_bigrams, feature_map_trigrams), 1))\n",
        "        pooling = concat.max(2)[0] \n",
        "        logits = self.hidden(pooling) \n",
        "        logits = self.out(logits)      \n",
        "        return logits"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iceORPe_GCsy"
      },
      "source": [
        "model_w2v = CNN_w2v(len(word2id), 100)\n",
        "optimizer = optim.Adam(model_w2v.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "# веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "model_w2v = model_w2v.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRtW4Io0GSiK",
        "outputId": "4001d486-d0e3-4425-eb7a-369e99d7b890"
      },
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(20):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model_w2v, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model_w2v, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model_w2v, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.7013823017477989\n",
            "Train loss: 0.6467938549590834\n",
            "Train loss: 0.6163051533699035\n",
            "Train loss: 0.5960972860677919\n",
            "Train loss: 0.581576108222916\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4984907788388869, Val f1: 0.7234277725219727\n",
            "Val loss: 0.49948356607381034, Val f1: 0.7245832085609436\n",
            "Val loss: 0.5025963397587047, Val f1: 0.7216693162918091\n",
            "Val loss: 0.5044030463870834, Val f1: 0.7200452089309692\n",
            "Val loss: 0.5042740394087398, Val f1: 0.7205828428268433\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5181401371955872, Val f1: 0.6995241641998291\n",
            "Val loss: 0.5186202079057693, Val f1: 0.7020862698554993\n",
            "Val loss: 0.516972541809082, Val f1: 0.7045501470565796\n",
            "Val loss: 0.514478363096714, Val f1: 0.7080731391906738\n",
            "Val loss: 0.5146209061145782, Val f1: 0.7096507549285889\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.530404532328248\n",
            "Train loss: 0.5093338543718512\n",
            "Train loss: 0.5003675669431686\n",
            "Train loss: 0.4954090073927125\n",
            "Train loss: 0.49101659939402625\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.45492546348010793, Val f1: 0.7572929263114929\n",
            "Val loss: 0.4535261401358773, Val f1: 0.7589429616928101\n",
            "Val loss: 0.45364656167871814, Val f1: 0.7591537833213806\n",
            "Val loss: 0.4538863323190633, Val f1: 0.7591987252235413\n",
            "Val loss: 0.45424723134321326, Val f1: 0.7587460875511169\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4714324474334717, Val f1: 0.7329601049423218\n",
            "Val loss: 0.4745657369494438, Val f1: 0.7330362796783447\n",
            "Val loss: 0.4756414939959844, Val f1: 0.731162965297699\n",
            "Val loss: 0.47305484861135483, Val f1: 0.7337751388549805\n",
            "Val loss: 0.4729985177516937, Val f1: 0.7321872115135193\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.48173627629876137\n",
            "Train loss: 0.46218666434288025\n",
            "Train loss: 0.4576049160957336\n",
            "Train loss: 0.4531413352311547\n",
            "Train loss: 0.4509604810958817\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4232824690201703, Val f1: 0.7909045219421387\n",
            "Val loss: 0.42402975611826954, Val f1: 0.7907785773277283\n",
            "Val loss: 0.424572581169652, Val f1: 0.7899534702301025\n",
            "Val loss: 0.4232251126976574, Val f1: 0.7906711101531982\n",
            "Val loss: 0.42220808968824497, Val f1: 0.791774332523346\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.45092882215976715, Val f1: 0.7647082805633545\n",
            "Val loss: 0.454763226211071, Val f1: 0.7622538208961487\n",
            "Val loss: 0.45715614159901935, Val f1: 0.7599234580993652\n",
            "Val loss: 0.45423344895243645, Val f1: 0.764702558517456\n",
            "Val loss: 0.454321363568306, Val f1: 0.7619884610176086\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.448147963732481\n",
            "Train loss: 0.43193723396821454\n",
            "Train loss: 0.4264835613965988\n",
            "Train loss: 0.42220748627363747\n",
            "Train loss: 0.4192307233100846\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3926311258007498, Val f1: 0.79888916015625\n",
            "Val loss: 0.3905712339807959, Val f1: 0.7997987270355225\n",
            "Val loss: 0.3910478575556886, Val f1: 0.8001165390014648\n",
            "Val loss: 0.3913530663532369, Val f1: 0.8010187745094299\n",
            "Val loss: 0.3910306229310877, Val f1: 0.8017657399177551\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.43800872564315796, Val f1: 0.7532064318656921\n",
            "Val loss: 0.4408641904592514, Val f1: 0.7542676329612732\n",
            "Val loss: 0.44365473588307697, Val f1: 0.7497252821922302\n",
            "Val loss: 0.44073041528463364, Val f1: 0.7552936673164368\n",
            "Val loss: 0.44016605615615845, Val f1: 0.7539839744567871\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.4141635484993458\n",
            "Train loss: 0.40319730657519715\n",
            "Train loss: 0.3984661662578583\n",
            "Train loss: 0.3954577619460092\n",
            "Train loss: 0.3925788143561\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.36344625669367175, Val f1: 0.8249161839485168\n",
            "Val loss: 0.36402179300785065, Val f1: 0.8260397911071777\n",
            "Val loss: 0.36374740799268085, Val f1: 0.8263826966285706\n",
            "Val loss: 0.3646809454349911, Val f1: 0.8251176476478577\n",
            "Val loss: 0.3651147169225356, Val f1: 0.8250730633735657\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.43062569200992584, Val f1: 0.7712464332580566\n",
            "Val loss: 0.4346434623003006, Val f1: 0.7677948474884033\n",
            "Val loss: 0.4373614390691121, Val f1: 0.768345832824707\n",
            "Val loss: 0.43434904515743256, Val f1: 0.7726117968559265\n",
            "Val loss: 0.4341084837913513, Val f1: 0.7706961631774902\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.3843889981508255\n",
            "Train loss: 0.37267089251315955\n",
            "Train loss: 0.3698654788732529\n",
            "Train loss: 0.36706531003339965\n",
            "Train loss: 0.36714680634793784\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.34001769563731027, Val f1: 0.8427696228027344\n",
            "Val loss: 0.33921221424551573, Val f1: 0.842326819896698\n",
            "Val loss: 0.3397093520444982, Val f1: 0.841414213180542\n",
            "Val loss: 0.33984072681735544, Val f1: 0.8423961400985718\n",
            "Val loss: 0.33976225782843195, Val f1: 0.8421550989151001\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4243571311235428, Val f1: 0.7784873247146606\n",
            "Val loss: 0.4283064976334572, Val f1: 0.7735817432403564\n",
            "Val loss: 0.43010928730169934, Val f1: 0.772718608379364\n",
            "Val loss: 0.42755362018942833, Val f1: 0.7780662775039673\n",
            "Val loss: 0.42707310020923617, Val f1: 0.7756915092468262\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.35722823441028595\n",
            "Train loss: 0.3486446322816791\n",
            "Train loss: 0.3463459914922714\n",
            "Train loss: 0.3448782100606321\n",
            "Train loss: 0.34317184558936525\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3181685524828294, Val f1: 0.8540474772453308\n",
            "Val loss: 0.3196347250657923, Val f1: 0.853085458278656\n",
            "Val loss: 0.31812778232144373, Val f1: 0.8547062277793884\n",
            "Val loss: 0.3171866172376801, Val f1: 0.8552548289299011\n",
            "Val loss: 0.31638894221361946, Val f1: 0.8560429215431213\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4218616932630539, Val f1: 0.7811322808265686\n",
            "Val loss: 0.4275183007121086, Val f1: 0.7770006656646729\n",
            "Val loss: 0.4299982686837514, Val f1: 0.7741431593894958\n",
            "Val loss: 0.4267052859067917, Val f1: 0.7799630165100098\n",
            "Val loss: 0.4258371442556381, Val f1: 0.7781621813774109\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.3373645041137934\n",
            "Train loss: 0.32791779438654584\n",
            "Train loss: 0.32426138401031496\n",
            "Train loss: 0.32209005773957095\n",
            "Train loss: 0.32071532486450105\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2933100479490617, Val f1: 0.8667199611663818\n",
            "Val loss: 0.29152729581384096, Val f1: 0.8694205284118652\n",
            "Val loss: 0.29205554606867773, Val f1: 0.8685932755470276\n",
            "Val loss: 0.29382961024256316, Val f1: 0.8659423589706421\n",
            "Val loss: 0.2937648752156426, Val f1: 0.8660296201705933\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4198314845561981, Val f1: 0.7837942838668823\n",
            "Val loss: 0.42568547278642654, Val f1: 0.7783589363098145\n",
            "Val loss: 0.4274834791819255, Val f1: 0.7748180627822876\n",
            "Val loss: 0.42546743899583817, Val f1: 0.7783324718475342\n",
            "Val loss: 0.4240227222442627, Val f1: 0.7789080142974854\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.31029896065592766\n",
            "Train loss: 0.3017114825320966\n",
            "Train loss: 0.2995178383588791\n",
            "Train loss: 0.29743736404091564\n",
            "Train loss: 0.2982240617275238\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.27304885317297545, Val f1: 0.8830703496932983\n",
            "Val loss: 0.2739783446578419, Val f1: 0.8819159865379333\n",
            "Val loss: 0.2729693294740191, Val f1: 0.8829568028450012\n",
            "Val loss: 0.27337190465015526, Val f1: 0.8827783465385437\n",
            "Val loss: 0.27276175583110135, Val f1: 0.8830628395080566\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4237557202577591, Val f1: 0.7982316017150879\n",
            "Val loss: 0.4290135130286217, Val f1: 0.7885660529136658\n",
            "Val loss: 0.4316620280345281, Val f1: 0.7871584296226501\n",
            "Val loss: 0.4290044493973255, Val f1: 0.790235698223114\n",
            "Val loss: 0.4288778483867645, Val f1: 0.7907649874687195\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.2890018131583929\n",
            "Train loss: 0.27864447371526196\n",
            "Train loss: 0.2775932523608208\n",
            "Train loss: 0.2763663107779489\n",
            "Train loss: 0.27675292179698036\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2554459598134546, Val f1: 0.8940946459770203\n",
            "Val loss: 0.25373684527242885, Val f1: 0.8946393132209778\n",
            "Val loss: 0.25439635649615644, Val f1: 0.893803596496582\n",
            "Val loss: 0.2542293994304012, Val f1: 0.8939400315284729\n",
            "Val loss: 0.25395735554835375, Val f1: 0.8941055536270142\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4300766736268997, Val f1: 0.8031609654426575\n",
            "Val loss: 0.43511223047971725, Val f1: 0.7958512902259827\n",
            "Val loss: 0.43774666885534924, Val f1: 0.7922322154045105\n",
            "Val loss: 0.4352838695049286, Val f1: 0.7949552536010742\n",
            "Val loss: 0.4348923534154892, Val f1: 0.7960764169692993\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.26921978406608105\n",
            "Train loss: 0.26124531707980414\n",
            "Train loss: 0.25997863203287125\n",
            "Train loss: 0.25765080794469636\n",
            "Train loss: 0.25609921309210004\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.23075891417615554, Val f1: 0.9037109613418579\n",
            "Val loss: 0.23181655038805568, Val f1: 0.903525173664093\n",
            "Val loss: 0.23031038776332258, Val f1: 0.9040461778640747\n",
            "Val loss: 0.22953656819813392, Val f1: 0.9049559235572815\n",
            "Val loss: 0.23047100007534027, Val f1: 0.9038335680961609\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4337335377931595, Val f1: 0.8000767230987549\n",
            "Val loss: 0.4397558495402336, Val f1: 0.7901272773742676\n",
            "Val loss: 0.44325125217437744, Val f1: 0.7885124683380127\n",
            "Val loss: 0.4408135935664177, Val f1: 0.7927065491676331\n",
            "Val loss: 0.4395475506782532, Val f1: 0.7927907705307007\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.23929480090737343\n",
            "Train loss: 0.23742748029304273\n",
            "Train loss: 0.23643509924411774\n",
            "Train loss: 0.23560086937982644\n",
            "Train loss: 0.23585031429926553\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2125617256935905, Val f1: 0.9122679829597473\n",
            "Val loss: 0.21410220466992436, Val f1: 0.9116266369819641\n",
            "Val loss: 0.21259900869107715, Val f1: 0.9114682674407959\n",
            "Val loss: 0.2113342668642016, Val f1: 0.9129074811935425\n",
            "Val loss: 0.21113784243078793, Val f1: 0.9127438068389893\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4445936232805252, Val f1: 0.7879141569137573\n",
            "Val loss: 0.44866275042295456, Val f1: 0.7869877815246582\n",
            "Val loss: 0.45195158819357556, Val f1: 0.7842837572097778\n",
            "Val loss: 0.4497113265097141, Val f1: 0.7893507480621338\n",
            "Val loss: 0.44792425632476807, Val f1: 0.7909533381462097\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.22594150993973017\n",
            "Train loss: 0.2190601383194779\n",
            "Train loss: 0.21845168888568878\n",
            "Train loss: 0.21850580710973314\n",
            "Train loss: 0.21812640077301435\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.1959865654216093, Val f1: 0.9228360652923584\n",
            "Val loss: 0.1953380905530032, Val f1: 0.9229925870895386\n",
            "Val loss: 0.19475016290066288, Val f1: 0.923689603805542\n",
            "Val loss: 0.19485850347315564, Val f1: 0.9235228300094604\n",
            "Val loss: 0.19512331433155958, Val f1: 0.9234188199043274\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.4544839560985565, Val f1: 0.7968958616256714\n",
            "Val loss: 0.456938199698925, Val f1: 0.7944430708885193\n",
            "Val loss: 0.45942466457684833, Val f1: 0.7906825542449951\n",
            "Val loss: 0.45745305716991425, Val f1: 0.7950949668884277\n",
            "Val loss: 0.45739821493625643, Val f1: 0.7972386479377747\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.20773206185549498\n",
            "Train loss: 0.20030103036851593\n",
            "Train loss: 0.1981151095032692\n",
            "Train loss: 0.1981846530046036\n",
            "Train loss: 0.19973131269216537\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.180402233320124, Val f1: 0.9290255904197693\n",
            "Val loss: 0.17716081308967926, Val f1: 0.9316626191139221\n",
            "Val loss: 0.17798531026232475, Val f1: 0.930864155292511\n",
            "Val loss: 0.17747871665393605, Val f1: 0.9319576621055603\n",
            "Val loss: 0.17738104410031263, Val f1: 0.9322425723075867\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.45984500646591187, Val f1: 0.7965713143348694\n",
            "Val loss: 0.46446505188941956, Val f1: 0.7939099073410034\n",
            "Val loss: 0.46675283710161847, Val f1: 0.7899796366691589\n",
            "Val loss: 0.46400052681565285, Val f1: 0.7949711680412292\n",
            "Val loss: 0.4634844332933426, Val f1: 0.7955150604248047\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.18736854754388332\n",
            "Train loss: 0.1838255109209003\n",
            "Train loss: 0.1844940713047981\n",
            "Train loss: 0.18406997487616183\n",
            "Train loss: 0.1840112954378128\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.1610795277006486, Val f1: 0.9385783672332764\n",
            "Val loss: 0.16185071538476384, Val f1: 0.9365959167480469\n",
            "Val loss: 0.16218284327609866, Val f1: 0.9367923140525818\n",
            "Val loss: 0.16208753932048292, Val f1: 0.9372138977050781\n",
            "Val loss: 0.16302494862500358, Val f1: 0.9366447925567627\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.48341384530067444, Val f1: 0.7910794019699097\n",
            "Val loss: 0.488814115524292, Val f1: 0.7905212640762329\n",
            "Val loss: 0.4909061888853709, Val f1: 0.7873802185058594\n",
            "Val loss: 0.4886796474456787, Val f1: 0.7925284504890442\n",
            "Val loss: 0.48591994047164916, Val f1: 0.7943034172058105\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.17252483777701855\n",
            "Train loss: 0.1677098043940284\n",
            "Train loss: 0.16743293195962905\n",
            "Train loss: 0.16795307420082947\n",
            "Train loss: 0.16857064196041652\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.14938955008983612, Val f1: 0.9440311193466187\n",
            "Val loss: 0.15000029740964665, Val f1: 0.9441906213760376\n",
            "Val loss: 0.14972532905784308, Val f1: 0.9448282122612\n",
            "Val loss: 0.1485851115601904, Val f1: 0.9456996917724609\n",
            "Val loss: 0.14763094923075507, Val f1: 0.9461463689804077\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.486763060092926, Val f1: 0.7983757257461548\n",
            "Val loss: 0.4923590272665024, Val f1: 0.7967649698257446\n",
            "Val loss: 0.49516913791497547, Val f1: 0.7940806150436401\n",
            "Val loss: 0.4922870174050331, Val f1: 0.7992974519729614\n",
            "Val loss: 0.4908740222454071, Val f1: 0.8005544543266296\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.15791826136410236\n",
            "Train loss: 0.15562321065050183\n",
            "Train loss: 0.15347044080495834\n",
            "Train loss: 0.15369954060262708\n",
            "Train loss: 0.15403929122147106\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.13572719737010844, Val f1: 0.9529862403869629\n",
            "Val loss: 0.1357048710479456, Val f1: 0.9510074853897095\n",
            "Val loss: 0.13616622940582387, Val f1: 0.9510834217071533\n",
            "Val loss: 0.13639750901390524, Val f1: 0.9507001042366028\n",
            "Val loss: 0.13624642158255856, Val f1: 0.9509299993515015\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5103117525577545, Val f1: 0.8006161451339722\n",
            "Val loss: 0.5151488333940506, Val f1: 0.7997502088546753\n",
            "Val loss: 0.5152793029944102, Val f1: 0.7970670461654663\n",
            "Val loss: 0.512947179377079, Val f1: 0.8012582063674927\n",
            "Val loss: 0.5125063538551331, Val f1: 0.8016652464866638\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.14301531948149204\n",
            "Train loss: 0.14099684809193466\n",
            "Train loss: 0.14040427401661873\n",
            "Train loss: 0.13948303360992403\n",
            "Train loss: 0.14068397782033398\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.12503093612544677, Val f1: 0.9556312561035156\n",
            "Val loss: 0.12215083530720543, Val f1: 0.9571605920791626\n",
            "Val loss: 0.1228720173239708, Val f1: 0.9567449688911438\n",
            "Val loss: 0.1227152920601999, Val f1: 0.9569259285926819\n",
            "Val loss: 0.12262729721910813, Val f1: 0.9567909240722656\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5209750831127167, Val f1: 0.7993313074111938\n",
            "Val loss: 0.529080256819725, Val f1: 0.797677755355835\n",
            "Val loss: 0.5287154217561086, Val f1: 0.7954890727996826\n",
            "Val loss: 0.5270106196403503, Val f1: 0.799358606338501\n",
            "Val loss: 0.5281100034713745, Val f1: 0.8003085255622864\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.12648811237886548\n",
            "Train loss: 0.12461401206074339\n",
            "Train loss: 0.12543751865625383\n",
            "Train loss: 0.12667643423400707\n",
            "Train loss: 0.12788025564735844\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.10873188253711252, Val f1: 0.9626882076263428\n",
            "Val loss: 0.10976152100107249, Val f1: 0.9628651738166809\n",
            "Val loss: 0.1094539650515014, Val f1: 0.9631957411766052\n",
            "Val loss: 0.11061855997232829, Val f1: 0.9626551270484924\n",
            "Val loss: 0.1111046593855409, Val f1: 0.9621169567108154\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5379235148429871, Val f1: 0.7958453893661499\n",
            "Val loss: 0.5459134876728058, Val f1: 0.7930099368095398\n",
            "Val loss: 0.5455168883005778, Val f1: 0.7904932498931885\n",
            "Val loss: 0.5415455996990204, Val f1: 0.7964224815368652\n",
            "Val loss: 0.5400485396385193, Val f1: 0.7976391911506653\n",
            "\n",
            "starting Epoch 19\n",
            "Training...\n",
            "Train loss: 0.11650781892240047\n",
            "Train loss: 0.11683473871512846\n",
            "Train loss: 0.11691737458109856\n",
            "Train loss: 0.1166369151046027\n",
            "Train loss: 0.11669037393516019\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.10288511085159638, Val f1: 0.964436948299408\n",
            "Val loss: 0.10365533193244654, Val f1: 0.9643402099609375\n",
            "Val loss: 0.10224560020016689, Val f1: 0.9656221270561218\n",
            "Val loss: 0.1013447582064306, Val f1: 0.9663577675819397\n",
            "Val loss: 0.10060872558285208, Val f1: 0.9666682481765747\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.561514139175415, Val f1: 0.8004530668258667\n",
            "Val loss: 0.5697419494390488, Val f1: 0.800556480884552\n",
            "Val loss: 0.5678864320119222, Val f1: 0.7975377440452576\n",
            "Val loss: 0.5652362704277039, Val f1: 0.8008860945701599\n",
            "Val loss: 0.5662264168262482, Val f1: 0.8009163737297058\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8rq22npGc59"
      },
      "source": [
        "fp, fn, tp, tn, accuracy_old, precision_old, recall_old = predict(model_w2v, val_iterator)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJCW6vv2GsJo",
        "outputId": "2123378a-6f80-41bd-cd2c-ef26fdc38928"
      },
      "source": [
        "print('accuracy:', accuracy_old)\n",
        "print('precision:', precision_old)\n",
        "print('recall:', recall_old)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.7958\n",
            "precision: 0.7914582531742979\n",
            "recall: 0.8111198738170347\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trAiNxb0uyJJ",
        "outputId": "3a258d84-0bda-4b6b-ebac-c7660323f0f7"
      },
      "source": [
        "best_acc = 0\n",
        "lr_val = ''\n",
        "\n",
        "for lr in [0.0005, 0.001, 0.005, 0.01, 0.05]:\n",
        "    model_w2v = CNN_w2v(len(word2id), emb_val)\n",
        "    optimizer = optim.Adam(model_w2v.parameters(), lr=lr)\n",
        "    criterion = nn.BCELoss()  \n",
        "\n",
        "    # веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "    model_w2v = model_w2v.to(DEVICE)\n",
        "    criterion = criterion.to(DEVICE)\n",
        "\n",
        "    losses = []\n",
        "    losses_eval = []\n",
        "    f1s = []\n",
        "    f1s_eval = []\n",
        "\n",
        "    for i in range(10):\n",
        "        print(f'starting Epoch {i}')\n",
        "        epoch_loss = train(model_w2v, train_iterator, optimizer, criterion, print_v=False)\n",
        "        losses.append(epoch_loss)\n",
        "        f1_on_train,_ = evaluate(model_w2v, train_iterator, criterion, print_v=False)\n",
        "        f1s.append(f1_on_train)\n",
        "        f1_on_test, epoch_loss_on_test = evaluate(model_w2v, val_iterator, criterion, print_v=False)\n",
        "        losses_eval.append(epoch_loss_on_test)\n",
        "        f1s_eval.append(f1_on_test)\n",
        "    fp, fn, tp, tn, accuracy_new, precision_new, recall_new = predict(model_w2v, val_iterator)\n",
        "    print(f'results for lr={lr}')\n",
        "    print('accuracy:', accuracy_new)\n",
        "    print('precision:', precision_new)\n",
        "    print('recall:', recall_new)\n",
        "    if accuracy_new > best_acc:\n",
        "      best_acc = accuracy_new\n",
        "      lr_val = lr\n",
        "print(f'____________________________________________')\n",
        "print(f'best accuracy is {best_acc} for lr {lr_val}')"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for lr=0.0005\n",
            "accuracy: 0.793\n",
            "precision: 0.7969924812030075\n",
            "recall: 0.7941640378548895\n",
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for lr=0.001\n",
            "accuracy: 0.7965\n",
            "precision: 0.8044916783637457\n",
            "recall: 0.7910094637223974\n",
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for lr=0.005\n",
            "accuracy: 0.807\n",
            "precision: 0.7936448598130841\n",
            "recall: 0.8371451104100947\n",
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for lr=0.01\n",
            "accuracy: 0.8135\n",
            "precision: 0.8115407033223236\n",
            "recall: 0.8235410094637224\n",
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for lr=0.05\n",
            "accuracy: 0.5072\n",
            "precision: 0.5072\n",
            "recall: 1.0\n",
            "____________________________________________\n",
            "best accuracy is 0.8135 for lr 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKfb8-mivZ3p",
        "outputId": "6fd1ab11-7040-4511-cacc-b8ffb5e06bbb"
      },
      "source": [
        "model_w2v = CNN_w2v(len(word2id), 100)\n",
        "optimizer = optim.Adam(model_w2v.parameters(), lr=lr_val)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "model_w2v = model_w2v.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)\n",
        "\n",
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(10):\n",
        "    print(f'starting Epoch {i}')\n",
        "    epoch_loss = train(model_w2v, train_iterator, optimizer, criterion, print_v=False)\n",
        "    losses.append(epoch_loss)\n",
        "    f1_on_train,_ = evaluate(model_w2v, train_iterator, criterion, print_v=False)\n",
        "    f1s.append(f1_on_train)\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model_w2v, val_iterator, criterion, print_v=False)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)\n",
        "fp, fn, tp, tn, accuracy_new, precision_new, recall_new = predict(model_w2v, val_iterator)\n",
        "print('Значения после подбора гипепараметров:')\n",
        "print(f'accuracy: {accuracy_new}, improved by {accuracy_new-accuracy_old}')\n",
        "print(f'precision: {precision_new}, improved by {precision_new-precision_old}')\n",
        "print(f'recall: {recall_new}, improved by {recall_new-recall_old}')"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "Значения после подбора гипепараметров:\n",
            "accuracy: 0.8153, improved by 0.019500000000000073\n",
            "precision: 0.8279438682123246, improved by 0.03648561503802672\n",
            "recall: 0.8026419558359621, improved by -0.008477917981072558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47D-VC0WvtFv"
      },
      "source": [
        "# Готовим датасет для второй модельки (пока что с учетом пунктуации)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M5OHRFsv6Db"
      },
      "source": [
        " У нас уже есть все необходимые словари (и слов, и символов), надо только правильно их подать."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYNKIcTUI8Gv"
      },
      "source": [
        "class TweetsDataset_2(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, word2id, symbol2id, DEVICE):\n",
        "        self.dataset = dataset['text'].values\n",
        "        self.word2id = word2id\n",
        "        self.symbol2id = symbol2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = dataset['tone'].values\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index): \n",
        "        words = self.dataset[index].split()\n",
        "        id_words = torch.LongTensor([self.word2id[word] for word in words if word in self.word2id])\n",
        "        symbols = list(self.dataset[index])\n",
        "        id_symbols= torch.LongTensor([self.symbol2id[symbol] for symbol in symbols if symbol in self.symbol2id])\n",
        "        y = [self.target[index]]\n",
        "        return id_words, id_symbols, y\n",
        "\n",
        "    def collate_fn(self, batch): \n",
        "      id_words, id_symbols, y = list(zip(*batch))\n",
        "      padded_id_words = pad_sequence(id_words, batch_first=True).to(self.device)\n",
        "      padded_id_symbols = pad_sequence(id_symbols, batch_first=True).to(self.device)\n",
        "      y = torch.Tensor(y).to(self.device)\n",
        "      return padded_id_words, padded_id_symbols, y"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEHbzz1UeTLf"
      },
      "source": [
        "val_dataset = TweetsDataset_2(val_sentences, word2id, symbol2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ03qiQLjVWJ"
      },
      "source": [
        "train_dataset = TweetsDataset_2(train_sentences, word2id, symbol2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpMEws2jwQ4J"
      },
      "source": [
        "# Готовим саму вторую модельку"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcKixAFgwV1N"
      },
      "source": [
        "Я реализую вариант с готовыми w2v эмбеддингами (обученными мной), тк он лучше работает. Усложненная модель дольше учится, поэтому тут я ограничусь 10 эпохами, и так все будет отлично."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsUfTf-imDDJ"
      },
      "source": [
        "class CNN_2(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size_word, vocab_size_symbol, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding_word = nn.Embedding(vocab_size_word, embedding_dim)\n",
        "        self.embedding_word.from_pretrained(torch.tensor(weights), freeze=True)\n",
        "        self.embedding_symbol = nn.Embedding(vocab_size_symbol, embedding_dim)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.hidden = nn.Linear(in_features=180, out_features=1)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, text, word):\n",
        "        embedded_words = self.embedding_word(text)\n",
        "        embedded_symbols = self.embedding_symbol(word)\n",
        "        embedded_words = embedded_words.transpose(1,2)\n",
        "        embedded_symbols = embedded_symbols.transpose(1,2)\n",
        "        feature_map_bigrams = self.pooling(self.bigrams(embedded_symbols)).max(2)[0] \n",
        "        feature_map_trigrams = self.pooling(self.trigrams(embedded_symbols)).max(2)[0] \n",
        "        concat = torch.cat((feature_map_bigrams, feature_map_trigrams), 1)\n",
        "        logits = self.hidden(concat) \n",
        "        logits = self.out(logits)      \n",
        "        return logits"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Npm11-uMzz5r"
      },
      "source": [
        "model_2 = CNN_2(len(word2id), len(symbol2id), 8)\n",
        "optimizer = optim.Adam(model_2.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "model_2 = model_2.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfEuspLz1MZt"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, print_v=True):\n",
        "    epoch_loss = 0 \n",
        "\n",
        "    model.train() \n",
        "    for i, (texts, words, ys) in enumerate(iterator):\n",
        "        optimizer.zero_grad() \n",
        "        preds = model(texts, words) \n",
        "        loss = criterion(preds, ys) \n",
        "        loss.backward() \n",
        "        optimizer.step() \n",
        "        epoch_loss += loss.item() \n",
        "        if print_v==True:\n",
        "          if not (i + 1) % int(len(iterator)/5):\n",
        "              print(f'Train loss: {epoch_loss/(i+1)}')      \n",
        "    return  epoch_loss / len(iterator) "
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfgTwhqg1Pl5"
      },
      "source": [
        "def evaluate(model, iterator, criterion, print_v=True):\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for i, (texts, words, ys) in enumerate(iterator):   \n",
        "            preds = model(texts, words)  \n",
        "            loss = criterion(preds, ys)   \n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = f1(preds.round().long(), ys.long(), ignore_index=0)\n",
        "            epoch_metric += batch_metric\n",
        "            if print_v==True:\n",
        "              if not (i + 1) % int(len(iterator)/5):\n",
        "                print(f'Val loss: {epoch_loss/(i+1)}, Val f1: {epoch_metric/(i+1)}')\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator) "
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OjJqAKw0tec",
        "outputId": "de0c1a9c-bd93-45f9-ae09-9ec39cd6d469"
      },
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(10):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model_2, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model_2, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model_2, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.6866374822223887\n",
            "Train loss: 0.6752418595201829\n",
            "Train loss: 0.6635850831574085\n",
            "Train loss: 0.6506611012360629\n",
            "Train loss: 0.6363153036902932\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.551732294699725, Val f1: 0.8630651831626892\n",
            "Val loss: 0.5508217864176806, Val f1: 0.8626630902290344\n",
            "Val loss: 0.5506519920685712, Val f1: 0.8639872074127197\n",
            "Val loss: 0.5508849980200038, Val f1: 0.8631011843681335\n",
            "Val loss: 0.5513038859647863, Val f1: 0.8628378510475159\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5488004088401794, Val f1: 0.8654316663742065\n",
            "Val loss: 0.5499240159988403, Val f1: 0.8674519658088684\n",
            "Val loss: 0.5497084259986877, Val f1: 0.8666236400604248\n",
            "Val loss: 0.549549050629139, Val f1: 0.8664134740829468\n",
            "Val loss: 0.5508593320846558, Val f1: 0.864578902721405\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.5338061942773706\n",
            "Train loss: 0.5109333001515445\n",
            "Train loss: 0.48968982696533203\n",
            "Train loss: 0.46753228894051385\n",
            "Train loss: 0.44497356134302474\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.32327870411031384, Val f1: 0.9202445149421692\n",
            "Val loss: 0.3253517957294689, Val f1: 0.9191966652870178\n",
            "Val loss: 0.3250541774665608, Val f1: 0.9201520681381226\n",
            "Val loss: 0.32474110932911143, Val f1: 0.9196239709854126\n",
            "Val loss: 0.32488645420354956, Val f1: 0.9197759628295898\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.316948264837265, Val f1: 0.9256526827812195\n",
            "Val loss: 0.3225412964820862, Val f1: 0.924177885055542\n",
            "Val loss: 0.321589137117068, Val f1: 0.9231632947921753\n",
            "Val loss: 0.3215668126940727, Val f1: 0.9254939556121826\n",
            "Val loss: 0.32359758019447327, Val f1: 0.9248014688491821\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.3067803908796871\n",
            "Train loss: 0.2865396882681286\n",
            "Train loss: 0.26865104162225534\n",
            "Train loss: 0.25218769140979824\n",
            "Train loss: 0.23823381434468663\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.16692333536989548, Val f1: 0.9604071378707886\n",
            "Val loss: 0.1678624591406654, Val f1: 0.9599841833114624\n",
            "Val loss: 0.1674264099668054, Val f1: 0.9600715041160583\n",
            "Val loss: 0.165700660470654, Val f1: 0.961046576499939\n",
            "Val loss: 0.1656052620971904, Val f1: 0.9610178470611572\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.15570539236068726, Val f1: 0.9680513143539429\n",
            "Val loss: 0.16440043970942497, Val f1: 0.965248703956604\n",
            "Val loss: 0.16321108986934027, Val f1: 0.9642572402954102\n",
            "Val loss: 0.16365113481879234, Val f1: 0.9636333584785461\n",
            "Val loss: 0.1655983403325081, Val f1: 0.9636204838752747\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.1558095427120433\n",
            "Train loss: 0.1486558173509205\n",
            "Train loss: 0.14047214257366517\n",
            "Train loss: 0.13539902526227868\n",
            "Train loss: 0.12996298057191513\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.10625114291906357, Val f1: 0.97456294298172\n",
            "Val loss: 0.10433975322281613, Val f1: 0.9752545952796936\n",
            "Val loss: 0.1038680300116539, Val f1: 0.9757949113845825\n",
            "Val loss: 0.10307329378145583, Val f1: 0.9762845635414124\n",
            "Val loss: 0.10305827917421566, Val f1: 0.9762697219848633\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.09333708882331848, Val f1: 0.9793331623077393\n",
            "Val loss: 0.10262340493500233, Val f1: 0.9773445725440979\n",
            "Val loss: 0.1010896402100722, Val f1: 0.977878987789154\n",
            "Val loss: 0.10171901993453503, Val f1: 0.977821409702301\n",
            "Val loss: 0.10385332927107811, Val f1: 0.9769964218139648\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.10441727673306185\n",
            "Train loss: 0.1000367286888992\n",
            "Train loss: 0.09705047294789669\n",
            "Train loss: 0.09320249934406842\n",
            "Train loss: 0.09091316400205388\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.08123000579721787, Val f1: 0.9776611328125\n",
            "Val loss: 0.0829260932172046, Val f1: 0.9771543145179749\n",
            "Val loss: 0.0827671768618565, Val f1: 0.977412223815918\n",
            "Val loss: 0.08089519423597, Val f1: 0.9781337976455688\n",
            "Val loss: 0.08099753392093322, Val f1: 0.9780304431915283\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.07211277447640896, Val f1: 0.9812616109848022\n",
            "Val loss: 0.08142824377864599, Val f1: 0.9787542819976807\n",
            "Val loss: 0.07966129047175248, Val f1: 0.9789804220199585\n",
            "Val loss: 0.08037265343591571, Val f1: 0.9790944457054138\n",
            "Val loss: 0.08251954801380634, Val f1: 0.9785097241401672\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.07912372644333278\n",
            "Train loss: 0.07606853150269564\n",
            "Train loss: 0.07555344459765098\n",
            "Train loss: 0.0754317150944296\n",
            "Train loss: 0.0753392830491066\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.075573942021412, Val f1: 0.9757310152053833\n",
            "Val loss: 0.07360203202594728, Val f1: 0.9768194556236267\n",
            "Val loss: 0.07123389730558675, Val f1: 0.9777438044548035\n",
            "Val loss: 0.07057240829967401, Val f1: 0.9780245423316956\n",
            "Val loss: 0.07010618553442113, Val f1: 0.9783619046211243\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.06091246381402016, Val f1: 0.9822087287902832\n",
            "Val loss: 0.06919476948678493, Val f1: 0.9798935651779175\n",
            "Val loss: 0.06757779543598492, Val f1: 0.9798933863639832\n",
            "Val loss: 0.06859837286174297, Val f1: 0.9800038933753967\n",
            "Val loss: 0.07044415473937989, Val f1: 0.9793543815612793\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.0694956205347005\n",
            "Train loss: 0.06838118504075442\n",
            "Train loss: 0.06744219143601025\n",
            "Train loss: 0.06576808876193621\n",
            "Train loss: 0.06513264174847042\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.06093265883186284, Val f1: 0.9800726771354675\n",
            "Val loss: 0.05963865021134124, Val f1: 0.9805684685707092\n",
            "Val loss: 0.06002602185688767, Val f1: 0.9804254174232483\n",
            "Val loss: 0.060651784762740135, Val f1: 0.9800964593887329\n",
            "Val loss: 0.06047055226038484, Val f1: 0.9800865650177002\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.05281122215092182, Val f1: 0.9841084480285645\n",
            "Val loss: 0.05991645809262991, Val f1: 0.9812910556793213\n",
            "Val loss: 0.058314516519506775, Val f1: 0.9809761047363281\n",
            "Val loss: 0.05949237197637558, Val f1: 0.9810395240783691\n",
            "Val loss: 0.06098711118102074, Val f1: 0.9806053042411804\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.059116107576033645\n",
            "Train loss: 0.058452549226143784\n",
            "Train loss: 0.05804006974486744\n",
            "Train loss: 0.05724113083937589\n",
            "Train loss: 0.05629455263123793\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.05287154566715745, Val f1: 0.9808636903762817\n",
            "Val loss: 0.052277658463400954, Val f1: 0.9814522862434387\n",
            "Val loss: 0.05233737212770125, Val f1: 0.9814531207084656\n",
            "Val loss: 0.05219809097402236, Val f1: 0.9815033078193665\n",
            "Val loss: 0.05153398417374667, Val f1: 0.9817291498184204\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.04537147842347622, Val f1: 0.9845415353775024\n",
            "Val loss: 0.051650796085596085, Val f1: 0.9826430678367615\n",
            "Val loss: 0.049850778033336006, Val f1: 0.9826457500457764\n",
            "Val loss: 0.05107414349913597, Val f1: 0.9826294183731079\n",
            "Val loss: 0.052337250858545306, Val f1: 0.9819710850715637\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.04879913106560707\n",
            "Train loss: 0.051914104972692096\n",
            "Train loss: 0.05003432089499399\n",
            "Train loss: 0.04822107212727561\n",
            "Train loss: 0.04742226894287502\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.04203859519432573, Val f1: 0.983397901058197\n",
            "Val loss: 0.04205841483438716, Val f1: 0.9837435483932495\n",
            "Val loss: 0.041817807975937336, Val f1: 0.9839178919792175\n",
            "Val loss: 0.0414082344950122, Val f1: 0.9842917323112488\n",
            "Val loss: 0.042555857625077756, Val f1: 0.9839581847190857\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.03788380138576031, Val f1: 0.9869619607925415\n",
            "Val loss: 0.043236445635557175, Val f1: 0.9843046069145203\n",
            "Val loss: 0.04116811354955038, Val f1: 0.9849903583526611\n",
            "Val loss: 0.042284263763576746, Val f1: 0.9846131801605225\n",
            "Val loss: 0.043336280062794684, Val f1: 0.9841259121894836\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.04054930565111777\n",
            "Train loss: 0.04088373893104932\n",
            "Train loss: 0.03963813886922948\n",
            "Train loss: 0.03879372408503995\n",
            "Train loss: 0.038038277867085794\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0331597391735105, Val f1: 0.9884399175643921\n",
            "Val loss: 0.034257764544557125, Val f1: 0.9883678555488586\n",
            "Val loss: 0.0334150572398714, Val f1: 0.9889328479766846\n",
            "Val loss: 0.03321618863436229, Val f1: 0.9891136288642883\n",
            "Val loss: 0.03302859576309428, Val f1: 0.9891823530197144\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.030669577419757843, Val f1: 0.9898234605789185\n",
            "Val loss: 0.03460619039833546, Val f1: 0.9879634380340576\n",
            "Val loss: 0.032341938776274524, Val f1: 0.9891471266746521\n",
            "Val loss: 0.03314138879068196, Val f1: 0.9888613224029541\n",
            "Val loss: 0.0339410500600934, Val f1: 0.9883773922920227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFhL7obm5QXd"
      },
      "source": [
        "def predict(model, iterator):\n",
        "    model.eval()\n",
        "    fp = []\n",
        "    fn = []\n",
        "    tp = [] \n",
        "    tn = []\n",
        "    with torch.no_grad():\n",
        "        for i, (texts, words, ys) in enumerate(iterator):   \n",
        "            preds = model(texts, words)  # делаем предсказания на тесте \n",
        "            for pred, gold, text in zip(preds, ys, texts):\n",
        "              text = ' '.join([id2word[int(symbol)] for symbol in text if symbol !=0])\n",
        "              if round(pred.item()) > gold:\n",
        "                fp.append(text)\n",
        "              elif round(pred.item()) < gold:\n",
        "                fn.append(text)\n",
        "              elif round(pred.item()) == gold == 1:\n",
        "                tp.append(text)\n",
        "              elif round(pred.item()) == gold == 0:\n",
        "                tn.append(text)\n",
        "    accuracy = (len(tp)+len(tn))/(len(tp)+len(fp)+len(fn)+len(tn))\n",
        "    precision = len(tp)/(len(tp)+len(fp))\n",
        "    recall = len(tp)/(len(tp)+len(fn))\n",
        "    return fp, fn, tp, tn, accuracy, precision, recall"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WliUYGvPxphR"
      },
      "source": [
        "На всякий случай смотрю, адекватно ли все обучалось."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "S71Iuc1N1ANh",
        "outputId": "96171f7b-26db-4c97-cdbb-715194eff063"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(losses)\n",
        "plt.plot(losses_eval)\n",
        "plt.title('BCE loss value')\n",
        "plt.ylabel('BCE loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "YWyExFIh5Fy3",
        "outputId": "a86c8aaf-3cad-4385-fc83-a9620e9c5020"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(f1s)\n",
        "plt.plot(f1s_eval)\n",
        "plt.title('f1 value')\n",
        "plt.ylabel('f1 value')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4YBPxc85HF6"
      },
      "source": [
        "fp_best, fn_best, tp_best, tn_best, accuracy_old, precision_old, recall_old = predict(model_2, val_iterator)"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nb89RjHu5Lhx",
        "outputId": "ceae306e-f98a-472d-c5d2-36a9f625e0f9"
      },
      "source": [
        "print('accuracy:', accuracy_old)\n",
        "print('precision:', precision_old)\n",
        "print('recall:', recall_old)"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.9879\n",
            "precision: 0.9780408630895551\n",
            "recall: 0.998829953198128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ugFg50Fxs59"
      },
      "source": [
        "Предлагаю считать этот вариант улучшенным, потому что, по-моему, дальше улучшать некуда :) Красота! Сейчас покажу ухудшенный (в беседе договорились, что для получения полного балла нужен и вариант без предобработки, и плохо работающий вариант с предобработкой)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtmwjv6zyHYf"
      },
      "source": [
        "# Ухудшенный вариант (без пунктуации, жесткая предобработка)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK9YMtuI5fe8"
      },
      "source": [
        "import re\n",
        "def preprocess_text(text):\n",
        "    text = text.lower().replace(\"ё\", \"е\")\n",
        "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', text)\n",
        "    text = re.sub('@[^\\s]+', 'USER', text)\n",
        "    text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    return text.strip()"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXw0S0uNyQSm"
      },
      "source": [
        "vocab = Counter()\n",
        "for text in tweets_data['text']:\n",
        "    vocab.update(preprocess(preprocess_text(text)))\n",
        "\n",
        "vocab_sym = Counter()\n",
        "for text in tweets_data['text']:\n",
        "    vocab_sym.update(preprocess1(preprocess_text(text)))\n",
        "\n",
        "filtered_vocab = set()\n",
        "for word in vocab:\n",
        "    if vocab[word] > 2:\n",
        "        filtered_vocab.add(word)\n",
        "\n",
        "filtered_vocab_sym = set()\n",
        "for symbol in vocab_sym:\n",
        "    if vocab_sym[symbol] > 5:\n",
        "        filtered_vocab_sym.add(symbol)\n",
        "\n",
        "word2id = {'PAD':0}\n",
        "for word in filtered_vocab:\n",
        "    word2id[word] = len(word2id)\n",
        "\n",
        "symbol2id = {'PAD':0}\n",
        "for symbol in filtered_vocab_sym:\n",
        "    symbol2id[symbol] = len(symbol2id)\n",
        "\n",
        "id2word = {i:word for word, i in word2id.items()}\n",
        "id2symbol = {i:symbol for symbol, i in symbol2id.items()}"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YjXTzdpmoEe"
      },
      "source": [
        "val_dataset = TweetsDataset_2(val_sentences, word2id, symbol2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)\n",
        "\n",
        "train_dataset = TweetsDataset_2(train_sentences, word2id, symbol2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)\n",
        "batch = next(iter(train_iterator))"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7-oXz_FnBO2",
        "outputId": "e7ebb2da-48a0-4f2a-8fd1-9a54182b86ac"
      },
      "source": [
        "model_2 = CNN_2(len(word2id), len(symbol2id), 100)\n",
        "optimizer = optim.Adam(model_2.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "model_2 = model_2.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)\n",
        "\n",
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(10):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model_2, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model_2, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model_2, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.693515956401825\n",
            "Train loss: 0.6743733479696161\n",
            "Train loss: 0.6655929158715641\n",
            "Train loss: 0.6583113495041343\n",
            "Train loss: 0.6523407971157748\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.62093725975822, Val f1: 0.673568606376648\n",
            "Val loss: 0.6226497976218953, Val f1: 0.6719118356704712\n",
            "Val loss: 0.62451545864928, Val f1: 0.6691737174987793\n",
            "Val loss: 0.6244076937437057, Val f1: 0.6687766909599304\n",
            "Val loss: 0.6238111453897813, Val f1: 0.669788122177124\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6290678679943085, Val f1: 0.6645082235336304\n",
            "Val loss: 0.6279489398002625, Val f1: 0.6730499267578125\n",
            "Val loss: 0.6263652940591177, Val f1: 0.6740919947624207\n",
            "Val loss: 0.624264158308506, Val f1: 0.6789233684539795\n",
            "Val loss: 0.6251124858856201, Val f1: 0.6765679717063904\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.6240392712985768\n",
            "Train loss: 0.6221384037943447\n",
            "Train loss: 0.6203319687469333\n",
            "Train loss: 0.6175680011510849\n",
            "Train loss: 0.6152715051875395\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6050040967324201, Val f1: 0.6743848323822021\n",
            "Val loss: 0.6026738969718709, Val f1: 0.6770777106285095\n",
            "Val loss: 0.6028463676864025, Val f1: 0.6760702729225159\n",
            "Val loss: 0.6035370476105634, Val f1: 0.6740384697914124\n",
            "Val loss: 0.6032728889409233, Val f1: 0.6756023168563843\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6108507812023163, Val f1: 0.6721757054328918\n",
            "Val loss: 0.6106293648481369, Val f1: 0.6718287467956543\n",
            "Val loss: 0.6088098486264547, Val f1: 0.6739012598991394\n",
            "Val loss: 0.6067754179239273, Val f1: 0.6757651567459106\n",
            "Val loss: 0.607715904712677, Val f1: 0.6719957590103149\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.603685109054341\n",
            "Train loss: 0.6026983716908623\n",
            "Train loss: 0.602185351007125\n",
            "Train loss: 0.6002974957227707\n",
            "Train loss: 0.5990989131086013\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5966760410982019, Val f1: 0.7143852710723877\n",
            "Val loss: 0.5957381865557503, Val f1: 0.713307797908783\n",
            "Val loss: 0.5943383609547335, Val f1: 0.7145305871963501\n",
            "Val loss: 0.5939448791391709, Val f1: 0.7150706052780151\n",
            "Val loss: 0.593858699237599, Val f1: 0.7156845331192017\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6073069274425507, Val f1: 0.7122972011566162\n",
            "Val loss: 0.6036383211612701, Val f1: 0.7135912775993347\n",
            "Val loss: 0.6009839077790579, Val f1: 0.7156475782394409\n",
            "Val loss: 0.5979416742920876, Val f1: 0.7179338335990906\n",
            "Val loss: 0.5989781796932221, Val f1: 0.716576337814331\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.5888857280506807\n",
            "Train loss: 0.5856345120598289\n",
            "Train loss: 0.5865248243014017\n",
            "Train loss: 0.5868636772913092\n",
            "Train loss: 0.5861855927635642\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5782239647472606, Val f1: 0.7065219283103943\n",
            "Val loss: 0.5769103151910445, Val f1: 0.7080554366111755\n",
            "Val loss: 0.5764576208357718, Val f1: 0.707920253276825\n",
            "Val loss: 0.5772022604942322, Val f1: 0.7076602578163147\n",
            "Val loss: 0.5772399986491484, Val f1: 0.7073929905891418\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5916736721992493, Val f1: 0.699897050857544\n",
            "Val loss: 0.5899973809719086, Val f1: 0.6943522691726685\n",
            "Val loss: 0.5875677863756815, Val f1: 0.6989737749099731\n",
            "Val loss: 0.5851260349154472, Val f1: 0.7012063264846802\n",
            "Val loss: 0.5865227937698364, Val f1: 0.6999725699424744\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.576652235844556\n",
            "Train loss: 0.5768055232132182\n",
            "Train loss: 0.5769954651009803\n",
            "Train loss: 0.575957270229564\n",
            "Train loss: 0.5763400849174051\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5693394015817081, Val f1: 0.6907842755317688\n",
            "Val loss: 0.5701041011249318, Val f1: 0.6939846873283386\n",
            "Val loss: 0.5682421724001566, Val f1: 0.6965293884277344\n",
            "Val loss: 0.5682815690250957, Val f1: 0.6949175596237183\n",
            "Val loss: 0.568977653279024, Val f1: 0.6949111819267273\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5860514342784882, Val f1: 0.6882871389389038\n",
            "Val loss: 0.5855540633201599, Val f1: 0.6823421120643616\n",
            "Val loss: 0.5827206273873647, Val f1: 0.6894625425338745\n",
            "Val loss: 0.5810089632868767, Val f1: 0.6912107467651367\n",
            "Val loss: 0.5828772187232971, Val f1: 0.6876510381698608\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.5706320334883297\n",
            "Train loss: 0.5692515057675979\n",
            "Train loss: 0.5690969240431692\n",
            "Train loss: 0.568377294084605\n",
            "Train loss: 0.568593375823077\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5616980230107027, Val f1: 0.6943222880363464\n",
            "Val loss: 0.5638517439365387, Val f1: 0.6922630071640015\n",
            "Val loss: 0.5638604292682573, Val f1: 0.6921173930168152\n",
            "Val loss: 0.563748770776917, Val f1: 0.6911966800689697\n",
            "Val loss: 0.5631854639333838, Val f1: 0.6922751069068909\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5849682986736298, Val f1: 0.6819453835487366\n",
            "Val loss: 0.5842020809650421, Val f1: 0.6805821657180786\n",
            "Val loss: 0.5803474386533102, Val f1: 0.6874245405197144\n",
            "Val loss: 0.5788355022668839, Val f1: 0.6893823146820068\n",
            "Val loss: 0.5807514488697052, Val f1: 0.6836812496185303\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.5631581089075874\n",
            "Train loss: 0.5621833345469307\n",
            "Train loss: 0.5609298523734597\n",
            "Train loss: 0.5601037267376395\n",
            "Train loss: 0.5609137675341438\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5545896817656124, Val f1: 0.7358469367027283\n",
            "Val loss: 0.5548432188875535, Val f1: 0.7340425848960876\n",
            "Val loss: 0.5546396316266528, Val f1: 0.7335773706436157\n",
            "Val loss: 0.5551859473480898, Val f1: 0.7327146530151367\n",
            "Val loss: 0.555471036714666, Val f1: 0.7328818440437317\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5855462849140167, Val f1: 0.7222499847412109\n",
            "Val loss: 0.5796510577201843, Val f1: 0.7219476103782654\n",
            "Val loss: 0.5745567679405212, Val f1: 0.7251319885253906\n",
            "Val loss: 0.5716572999954224, Val f1: 0.7271525859832764\n",
            "Val loss: 0.573406434059143, Val f1: 0.7238993048667908\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.5498816826764275\n",
            "Train loss: 0.5533728827448452\n",
            "Train loss: 0.555497971235537\n",
            "Train loss: 0.554818496984594\n",
            "Train loss: 0.5544423944809858\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5471603975576513, Val f1: 0.7328822016716003\n",
            "Val loss: 0.5487436774898978, Val f1: 0.7293106913566589\n",
            "Val loss: 0.5476447928185556, Val f1: 0.7292813658714294\n",
            "Val loss: 0.5475554361062891, Val f1: 0.7292293310165405\n",
            "Val loss: 0.5478154953788309, Val f1: 0.7292385697364807\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5803264379501343, Val f1: 0.71251380443573\n",
            "Val loss: 0.5752727687358856, Val f1: 0.7088294625282288\n",
            "Val loss: 0.5695019364356995, Val f1: 0.7150728702545166\n",
            "Val loss: 0.567332535982132, Val f1: 0.7178390622138977\n",
            "Val loss: 0.56944260597229, Val f1: 0.714358925819397\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.5474237168536467\n",
            "Train loss: 0.5525966900236466\n",
            "Train loss: 0.5515615612852807\n",
            "Train loss: 0.5514298449544346\n",
            "Train loss: 0.5517407894134522\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5493178928599638, Val f1: 0.7473313808441162\n",
            "Val loss: 0.5489477585343754, Val f1: 0.7462627291679382\n",
            "Val loss: 0.5480559211151272, Val f1: 0.7473107576370239\n",
            "Val loss: 0.5489593884524178, Val f1: 0.7461096048355103\n",
            "Val loss: 0.5498946603606729, Val f1: 0.745826244354248\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5893281698226929, Val f1: 0.7244948148727417\n",
            "Val loss: 0.5810775309801102, Val f1: 0.7293581962585449\n",
            "Val loss: 0.5736186405022939, Val f1: 0.7327927350997925\n",
            "Val loss: 0.5701470300555229, Val f1: 0.735345184803009\n",
            "Val loss: 0.572299200296402, Val f1: 0.7335613965988159\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.5451933461077073\n",
            "Train loss: 0.5463904422872207\n",
            "Train loss: 0.544307158273809\n",
            "Train loss: 0.5450031441800735\n",
            "Train loss: 0.5456877918804393\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.543123764150283, Val f1: 0.743069052696228\n",
            "Val loss: 0.5418159944169662, Val f1: 0.7437165975570679\n",
            "Val loss: 0.5396100218389549, Val f1: 0.7441627383232117\n",
            "Val loss: 0.5406376232995707, Val f1: 0.7420949339866638\n",
            "Val loss: 0.5398342886391808, Val f1: 0.7425131797790527\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5801273882389069, Val f1: 0.7207984924316406\n",
            "Val loss: 0.5739180147647858, Val f1: 0.7218360900878906\n",
            "Val loss: 0.5669217308362325, Val f1: 0.7255443334579468\n",
            "Val loss: 0.564253106713295, Val f1: 0.728223443031311\n",
            "Val loss: 0.5662921369075775, Val f1: 0.7254394292831421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DiJzDkVnFGe",
        "outputId": "4b1a2d9f-b6f0-4986-c558-ebc7a885ac18"
      },
      "source": [
        "fp, fn, tp, tn, accuracy_new, precision_new, recall_new = predict(model_2, val_iterator)\n",
        "print(f'accuracy: {accuracy_new}')\n",
        "print(f'precision: {precision_new}')\n",
        "print(f'recall: {recall_new}')"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.7041\n",
            "precision: 0.6917093866006717\n",
            "recall: 0.7630655226209049\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_f0bwuH1_Xq"
      },
      "source": [
        "Мда, печально... Модель, которая вообще не учитывает пунктуацию, работает совсем плохо. Я объясню, почему так, в части с анализом работы лучших моделей."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEALo5mA2Ycr"
      },
      "source": [
        "# Немножко улучшаем наш ухудшенный вариант"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEyfRfTWn2W7",
        "outputId": "135febcd-be54-4220-daca-065e8207c93e"
      },
      "source": [
        "best_acc = 0\n",
        "lr_val = ''\n",
        "\n",
        "for lr in [0.0005, 0.005, 0.01]:\n",
        "    model_2 = CNN_2(len(word2id), len(symbol2id), 100)\n",
        "    optimizer = optim.Adam(model_2.parameters(), lr=lr)\n",
        "    criterion = nn.BCELoss()  \n",
        "\n",
        "    # веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "    model_2 = model_2.to(DEVICE)\n",
        "    criterion = criterion.to(DEVICE)\n",
        "\n",
        "    losses = []\n",
        "    losses_eval = []\n",
        "    f1s = []\n",
        "    f1s_eval = []\n",
        "\n",
        "    for i in range(10):\n",
        "        print(f'starting Epoch {i}')\n",
        "        epoch_loss = train(model_2, train_iterator, optimizer, criterion, print_v=False)\n",
        "        losses.append(epoch_loss)\n",
        "        f1_on_train,_ = evaluate(model_2, train_iterator, criterion, print_v=False)\n",
        "        f1s.append(f1_on_train)\n",
        "        f1_on_test, epoch_loss_on_test = evaluate(model_2, val_iterator, criterion, print_v=False)\n",
        "        losses_eval.append(epoch_loss_on_test)\n",
        "        f1s_eval.append(f1_on_test)\n",
        "\n",
        "    fp, fn, tp, tn, accuracy, precision, recall = predict(model_2, val_iterator)\n",
        "    print(f'results for lr {lr}')\n",
        "    print('accuracy:', accuracy)\n",
        "    print('precision:', precision)\n",
        "    print('recall:', recall)\n",
        "    if accuracy > best_acc:\n",
        "      best_acc = accuracy\n",
        "      lr_val = lr\n",
        "print(f'____________________________________________')\n",
        "print(f'best accuracy is {best_acc} for lr {lr_val}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting Epoch 0\n",
            "starting Epoch 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dMpObq3pqMz",
        "outputId": "7e404d15-3bf0-4078-b181-5d4cee203f40"
      },
      "source": [
        "model_2 = CNN_2(len(word2id), len(symbol2id), 100)\n",
        "optimizer = optim.Adam(model_2.parameters(), lr=lr_val)\n",
        "criterion = nn.BCELoss()  \n",
        "model_2 = model_2.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)\n",
        "\n",
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(10):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model_2, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model_2, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model_2, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.8400774142321419\n",
            "Train loss: 0.7451080557178048\n",
            "Train loss: 0.707933131386252\n",
            "Train loss: 0.6856661000672508\n",
            "Train loss: 0.6719668570686789\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6022049924906563, Val f1: 0.7063120007514954\n",
            "Val loss: 0.6020027591901667, Val f1: 0.7104583978652954\n",
            "Val loss: 0.6029147423949897, Val f1: 0.7098850607872009\n",
            "Val loss: 0.6023731634897345, Val f1: 0.7103009819984436\n",
            "Val loss: 0.6023539466016433, Val f1: 0.7092711329460144\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6018490493297577, Val f1: 0.7004812955856323\n",
            "Val loss: 0.6062739193439484, Val f1: 0.7016798257827759\n",
            "Val loss: 0.6097299853960673, Val f1: 0.6977676749229431\n",
            "Val loss: 0.6062004715204239, Val f1: 0.7030916810035706\n",
            "Val loss: 0.6065591931343078, Val f1: 0.7010650634765625\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.6078647164737477\n",
            "Train loss: 0.6037487177287831\n",
            "Train loss: 0.6020280964234296\n",
            "Train loss: 0.6002708445577061\n",
            "Train loss: 0.5980649737750783\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5698220975258771, Val f1: 0.7186318635940552\n",
            "Val loss: 0.5703903506783878, Val f1: 0.7218441963195801\n",
            "Val loss: 0.5719838563133689, Val f1: 0.7203623056411743\n",
            "Val loss: 0.5718820752466426, Val f1: 0.7201800346374512\n",
            "Val loss: 0.5727343236698824, Val f1: 0.7192660570144653\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5846429765224457, Val f1: 0.7096946835517883\n",
            "Val loss: 0.5880894362926483, Val f1: 0.7116163969039917\n",
            "Val loss: 0.5916999777158102, Val f1: 0.7023278474807739\n",
            "Val loss: 0.5865422040224075, Val f1: 0.7091107368469238\n",
            "Val loss: 0.5863025724887848, Val f1: 0.7047597169876099\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.5774425478542552\n",
            "Train loss: 0.5768276751041412\n",
            "Train loss: 0.5757923441774705\n",
            "Train loss: 0.5749583735185511\n",
            "Train loss: 0.5744038658983567\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5751261956551496, Val f1: 0.6312025785446167\n",
            "Val loss: 0.5764746052377364, Val f1: 0.6280549168586731\n",
            "Val loss: 0.5776611262676763, Val f1: 0.6271630525588989\n",
            "Val loss: 0.5781397232237984, Val f1: 0.6287857890129089\n",
            "Val loss: 0.5791566126486835, Val f1: 0.6295855045318604\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6055354475975037, Val f1: 0.600239634513855\n",
            "Val loss: 0.6032911241054535, Val f1: 0.6027930974960327\n",
            "Val loss: 0.6088460683822632, Val f1: 0.5943260192871094\n",
            "Val loss: 0.6068034023046494, Val f1: 0.5974342226982117\n",
            "Val loss: 0.6064366400241852, Val f1: 0.5919561982154846\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.570641980451696\n",
            "Train loss: 0.5645822269075057\n",
            "Train loss: 0.5748993544017568\n",
            "Train loss: 0.5765837492311702\n",
            "Train loss: 0.5782836654606988\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5476000659606036, Val f1: 0.721878170967102\n",
            "Val loss: 0.5442807078361511, Val f1: 0.726493775844574\n",
            "Val loss: 0.5428751999256658, Val f1: 0.7256211638450623\n",
            "Val loss: 0.5428949243882123, Val f1: 0.7251461148262024\n",
            "Val loss: 0.5413611545282252, Val f1: 0.7259273529052734\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.564963698387146, Val f1: 0.7090147733688354\n",
            "Val loss: 0.5655857175588608, Val f1: 0.7079567909240723\n",
            "Val loss: 0.57156041264534, Val f1: 0.699124813079834\n",
            "Val loss: 0.5699798539280891, Val f1: 0.7033712863922119\n",
            "Val loss: 0.571598905324936, Val f1: 0.6971226930618286\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.5512511975624982\n",
            "Train loss: 0.5481475872151992\n",
            "Train loss: 0.5493144486464706\n",
            "Train loss: 0.5516034715315875\n",
            "Train loss: 0.5538645709262174\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5440730873276206, Val f1: 0.6919933557510376\n",
            "Val loss: 0.5424977278008181, Val f1: 0.6937059164047241\n",
            "Val loss: 0.5387286181543388, Val f1: 0.6975418329238892\n",
            "Val loss: 0.5390893992255715, Val f1: 0.6975452303886414\n",
            "Val loss: 0.5399490854319404, Val f1: 0.6978700757026672\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5765148401260376, Val f1: 0.6614842414855957\n",
            "Val loss: 0.5759905278682709, Val f1: 0.6625327467918396\n",
            "Val loss: 0.5835820436477661, Val f1: 0.6569910049438477\n",
            "Val loss: 0.5812800154089928, Val f1: 0.6641141176223755\n",
            "Val loss: 0.5816837251186371, Val f1: 0.6594001054763794\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.5507297691176919\n",
            "Train loss: 0.5429914330734926\n",
            "Train loss: 0.5427761673927307\n",
            "Train loss: 0.5450433711795246\n",
            "Train loss: 0.5477882083724527\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5223897976033828, Val f1: 0.7300011515617371\n",
            "Val loss: 0.523491233587265, Val f1: 0.729183554649353\n",
            "Val loss: 0.5240829212992799, Val f1: 0.729213535785675\n",
            "Val loss: 0.5230815173948512, Val f1: 0.7307692766189575\n",
            "Val loss: 0.5234435691553003, Val f1: 0.730218231678009\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5632997453212738, Val f1: 0.6926461458206177\n",
            "Val loss: 0.5645161420106888, Val f1: 0.6919123530387878\n",
            "Val loss: 0.5715332726637522, Val f1: 0.6852196455001831\n",
            "Val loss: 0.5689016506075859, Val f1: 0.6906323432922363\n",
            "Val loss: 0.5690777957439422, Val f1: 0.6868593096733093\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.5240755256484536\n",
            "Train loss: 0.5284120703444761\n",
            "Train loss: 0.5287128885587057\n",
            "Train loss: 0.5300739482045174\n",
            "Train loss: 0.5317261601195616\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5290601919679081, Val f1: 0.6971997618675232\n",
            "Val loss: 0.5325517110964831, Val f1: 0.6927907466888428\n",
            "Val loss: 0.5314034918944041, Val f1: 0.6954497694969177\n",
            "Val loss: 0.5309459764291259, Val f1: 0.6957456469535828\n",
            "Val loss: 0.5307962957550497, Val f1: 0.6959333419799805\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5910078883171082, Val f1: 0.6479324698448181\n",
            "Val loss: 0.5840430110692978, Val f1: 0.6538065075874329\n",
            "Val loss: 0.5924829840660095, Val f1: 0.644698977470398\n",
            "Val loss: 0.5913833379745483, Val f1: 0.6505347490310669\n",
            "Val loss: 0.5901651978492737, Val f1: 0.6450293064117432\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.5175818485372207\n",
            "Train loss: 0.5207617913975435\n",
            "Train loss: 0.5263812822454116\n",
            "Train loss: 0.5306204636307323\n",
            "Train loss: 0.5336096307810615\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5089681516675388, Val f1: 0.7595115900039673\n",
            "Val loss: 0.5057616111110238, Val f1: 0.7623041272163391\n",
            "Val loss: 0.5051950128639445, Val f1: 0.76207435131073\n",
            "Val loss: 0.5048792686532525, Val f1: 0.7615557909011841\n",
            "Val loss: 0.5040246048394371, Val f1: 0.7616301774978638\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5602573752403259, Val f1: 0.7233619689941406\n",
            "Val loss: 0.5589845180511475, Val f1: 0.7240490317344666\n",
            "Val loss: 0.5685840547084808, Val f1: 0.7153927683830261\n",
            "Val loss: 0.5657923147082329, Val f1: 0.7197643518447876\n",
            "Val loss: 0.5653626322746277, Val f1: 0.7166082262992859\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.5098622458822587\n",
            "Train loss: 0.5158323373864678\n",
            "Train loss: 0.5184148655218237\n",
            "Train loss: 0.5198266883106792\n",
            "Train loss: 0.5222367938827066\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5539549238541547, Val f1: 0.7625483870506287\n",
            "Val loss: 0.5544424635522506, Val f1: 0.7647297978401184\n",
            "Val loss: 0.5538168549537659, Val f1: 0.7647225260734558\n",
            "Val loss: 0.553313663777183, Val f1: 0.7641969323158264\n",
            "Val loss: 0.5540438806309419, Val f1: 0.7640604376792908\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6108672320842743, Val f1: 0.7437561750411987\n",
            "Val loss: 0.6192923188209534, Val f1: 0.7413251996040344\n",
            "Val loss: 0.62931028008461, Val f1: 0.7306622266769409\n",
            "Val loss: 0.6210858002305031, Val f1: 0.7364662289619446\n",
            "Val loss: 0.6232413291931153, Val f1: 0.7374846935272217\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.5431429252905005\n",
            "Train loss: 0.5351142436265945\n",
            "Train loss: 0.5275296942860472\n",
            "Train loss: 0.5297188154038261\n",
            "Train loss: 0.5311835885047913\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5080029981977799, Val f1: 0.7759088277816772\n",
            "Val loss: 0.508337049799807, Val f1: 0.7766696214675903\n",
            "Val loss: 0.5097869658002666, Val f1: 0.7751707434654236\n",
            "Val loss: 0.5086544239345718, Val f1: 0.775402843952179\n",
            "Val loss: 0.5097616760169759, Val f1: 0.7746493220329285\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.5768603086471558, Val f1: 0.7415326833724976\n",
            "Val loss: 0.5799980908632278, Val f1: 0.7412875890731812\n",
            "Val loss: 0.589459757010142, Val f1: 0.7305547595024109\n",
            "Val loss: 0.5826384946703911, Val f1: 0.7369612455368042\n",
            "Val loss: 0.583467710018158, Val f1: 0.7345003485679626\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIijX2zmvaTI",
        "outputId": "c23aa189-7bc2-424d-db62-880794e84be5"
      },
      "source": [
        "fp, fn, tp, tn, accuracy_newer, precision_newer, recall_newer = predict(model_2, val_iterator)\n",
        "print('Значения после подбора гипепараметров:')\n",
        "print(f'accuracy: {accuracy_new}, improved by {accuracy_newer-accuracy_new}')\n",
        "print(f'precision: {precision_new}, improved by {precision_newer-precision_new}')\n",
        "print(f'recall: {recall_new}, improved by {recall_newer-recall_new}')"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Значения после подбора гипепараметров:\n",
            "accuracy: 0.7005, improved by 0.00019999999999997797\n",
            "precision: 0.7043084792445407, improved by -0.03753485498655895\n",
            "recall: 0.7058359621451105, improved by 0.11356466876971605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fPEcP-08y9I"
      },
      "source": [
        "# Анализ ошибок лучших моделей"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7cEy5zJvd7Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdf2d97c-64c3-4627-e06b-189c2e403cbc"
      },
      "source": [
        "print('\\n\\nчто правильно предсказываем как позитивное:\\n')\n",
        "for item in tp_best[:10]:\n",
        "  print(item)\n",
        "\n",
        "print('\\n\\nчто правильно предсказываем как негативное:\\n')\n",
        "for item in tn_best[:10]:\n",
        "  print(item)\n",
        "\n",
        "print('\\n\\nошибочно не относим к позитивному:\\n')\n",
        "for item in fn_best[:10]:\n",
        "  print(item)\n",
        "\n",
        "print('\\n\\nошибочно не относим к негативному:\\n')\n",
        "for item in fp_best[:10]:\n",
        "  print(item)"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "что правильно предсказываем как позитивное:\n",
            "\n",
            "Ааа, новенький уже пришел. Поздравляю с\n",
            "Я купил на ДР :) Я горжусь собой :) Вы ведь для меня время? :)\n",
            "Сегодня был очень насыщенный день... Всегда бы так))) спасибо\n",
            "ну и чо вы тут\n",
            "когда то ты была такой, но сегодня тебе Любимая\n",
            "ну что такое, девушку, она тут теперь и не появится :)\n",
            "бабушка к нам жить через неделю =) убейте меня, плз =)\n",
            "Реклама для мотивация для других :)\n",
            "Тимур заебал, мой дорогой, Реклама твоя ебаная в Иди\n",
            "ахахахахах когда ехали в автобусе в мармелад мое слово было\n",
            "\n",
            "\n",
            "что правильно предсказываем как негативное:\n",
            "\n",
            "#TeamFollowBack здарова, коты и вот этот день настал ( #новости\n",
            "Но этот водитель не Он наверняка на будет долго\n",
            "@_Saturana_ тебе((( я на работе, кушаю :3\n",
            "уу где ну в и так в мелких не помню названий ((\n",
            "Только из за одного дебила я в выходные сижу дома и никаких клубов :((\n",
            "Очень сильно хочу на концерт @L_One_Mars денег нет((( Что мне делать? Нужно больше\n",
            "Да, я жирная !! Ты вообще 47 я тоже так хочу :(\n",
            "ничего не случилось точнее случилось ничего(\n",
            "RT скоро :(\n",
            "ноо, 33 раза все сто раз адреса написать, еще мата услышать в свой адрес и стоять часа 2 в лучшем\n",
            "\n",
            "\n",
            "ошибочно не относим к позитивному:\n",
            "\n",
            "привезти ^_~\n",
            "У кого-то сегодня будут ^_^\n",
            "RT @AlYoNa_GlAmBeRt\n",
            "нифига ты тоже хочу так\n",
            "RT поток с компа с музыкой на телефон, а оттуда через наушники О_О ^_^\n",
            "RT не умею говорить происходит полёт от погулять со\n",
            "\n",
            "\n",
            "ошибочно не относим к негативному:\n",
            "\n",
            "RT мне кажется, или ты только\n",
            "@to_over_kill насчет реформы согласна. Она блин каждый год меняется! И с каждым годом все \"лучше и лучше\"\n",
            "Мне звонят два мужика с двух разных номеров и просят еще трех человек на завтра о_О боюсь представить, за кого они меня принимают\n",
            "RT @DO_OR_PIE_: фанатики прочтите. Специально для вас.\n",
            "RT @prisonero_O: Тут должна быть шутка,про \"не все потеряно\". Но это лишнее.Просто знайте,что никто не рожден звездой http://t.co/YmZt6evxKu\n",
            "RT @Horanso_on @KINGWANTYOU с\n",
            "@kota_Oo_oO спасибо и тебя с:\n",
            "RT месяца\n",
            "@prisonero_O как мне как потом как я мы не что то вообще\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2Rwng6F7Kce"
      },
      "source": [
        "В целом, нетрудно заметить, что в подавляющем большинстве анализируемых твитов присутствуют смайлики. Они и стали нашей золотой чертой, благодаря которой модель, использующая посимвольное представлени без очистки от пунктуации сработала почти идеально. Ошибалась модель либо в случаях, когда смайликов не было (и вообще не было никаких эмоционально окрашенных слов -- предполагаю, что они выпали, когда мы вычищали редко упоминаемые слова; в таком случае сделать предсказание и правда почти невозможно), либо в случаях, когда смайлики были нетипичными (^_^) и, как я предполагаю, встречались в корпусе слишком редко, чтоб модель поняла, позитивная это или негативная черта. Еще важно сказать, что разметка в нашем датасете тоже неидеальна: почему \"спасибо и тебя с:\" должен быть негативным твитом, я так и не смогла понять, увы. Хотя, может быть, мы негативное из него тоже отсеяли, когда срезали редко употребляющиеся слова ))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCRf8Sdb-nLo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}