{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled13.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zonlbzzslvAF",
        "outputId": "a76bf578-c69a-457e-8db9-d60df8cdb462"
      },
      "source": [
        "!pip install torchmetrics"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.6.0-py3-none-any.whl (329 kB)\n",
            "\u001b[K     |████████████████████████████████| 329 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.6)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 869
        },
        "id": "yA9IJdGzl_Ai",
        "outputId": "d1939410-0dc6-442a-8809-6666e6fe4620"
      },
      "source": [
        "!pip install ipdb"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipdb\n",
            "  Downloading ipdb-0.13.9.tar.gz (16 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from ipdb) (57.4.0)\n",
            "Collecting ipython>=7.17.0\n",
            "  Downloading ipython-7.29.0-py3-none-any.whl (790 kB)\n",
            "\u001b[K     |████████████████████████████████| 790 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from ipdb) (0.10.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipdb) (4.4.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.18.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (5.1.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.2.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (2.6.1)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.1.3)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.22-py3-none-any.whl (374 kB)\n",
            "\u001b[K     |████████████████████████████████| 374 kB 64.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (4.8.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.17.0->ipdb) (0.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.17.0->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.17.0->ipdb) (0.2.5)\n",
            "Building wheels for collected packages: ipdb\n",
            "  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipdb: filename=ipdb-0.13.9-py3-none-any.whl size=11648 sha256=cd060159147c05f6368167228cafd97f841cd8ccfd800f95ec0c7175f6f21196\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/cd/cc/aaf92acae337a28fdd2aa4d632196a59745c8c39f76eaeed01\n",
            "Successfully built ipdb\n",
            "Installing collected packages: prompt-toolkit, ipython, ipdb\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.22 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.29.0 which is incompatible.\u001b[0m\n",
            "Successfully installed ipdb-0.13.9 ipython-7.29.0 prompt-toolkit-3.0.22\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqbNUZ2al_6p"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "from torchmetrics import F1\n",
        "from torchmetrics.functional import f1, recall\n",
        "import ipdb"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj7pzdcUmBVV",
        "outputId": "2dec772a-a0ec-4df2-9be2-13246b5dd52c"
      },
      "source": [
        "!wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv\n",
        "!wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-26 13:45:50--  https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/r6u59ljhhjdg6j0/negative.csv [following]\n",
            "--2021-11-26 13:45:50--  https://www.dropbox.com/s/raw/r6u59ljhhjdg6j0/negative.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc674b0198d52d1b612f46dd018c.dl.dropboxusercontent.com/cd/0/inline/BasNKwzetHEzQIHQ744o3xrO8nZcbO39TdvP6bo05p_sdAqi6kQksW8k50BSuH_S049eHcF6GhuciwjOFrkmaFywDn9ZpiVL_McgUJS9-yaEvFOfjbU3mu_DJGe1pqI3siniHbgmBPt0LfhTtBzfX-tz/file# [following]\n",
            "--2021-11-26 13:45:50--  https://uc674b0198d52d1b612f46dd018c.dl.dropboxusercontent.com/cd/0/inline/BasNKwzetHEzQIHQ744o3xrO8nZcbO39TdvP6bo05p_sdAqi6kQksW8k50BSuH_S049eHcF6GhuciwjOFrkmaFywDn9ZpiVL_McgUJS9-yaEvFOfjbU3mu_DJGe1pqI3siniHbgmBPt0LfhTtBzfX-tz/file\n",
            "Resolving uc674b0198d52d1b612f46dd018c.dl.dropboxusercontent.com (uc674b0198d52d1b612f46dd018c.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uc674b0198d52d1b612f46dd018c.dl.dropboxusercontent.com (uc674b0198d52d1b612f46dd018c.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24450101 (23M) [text/plain]\n",
            "Saving to: ‘negative.csv’\n",
            "\n",
            "negative.csv        100%[===================>]  23.32M  47.1MB/s    in 0.5s    \n",
            "\n",
            "2021-11-26 13:45:51 (47.1 MB/s) - ‘negative.csv’ saved [24450101/24450101]\n",
            "\n",
            "--2021-11-26 13:45:51--  https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/fnpq3z4bcnoktiv/positive.csv [following]\n",
            "--2021-11-26 13:45:51--  https://www.dropbox.com/s/raw/fnpq3z4bcnoktiv/positive.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc7634611f3b09e6cb94fe92f1e7.dl.dropboxusercontent.com/cd/0/inline/BauVS9VHcmT79-fUTNtiUaXfvfXzLeaHDqIUzjEhY2F7t8B2KytIPkiZ6-KBL8AugN_Sos9nlZVwxv2S3u2Y0FYiCTEV8JF9t7HeHRyCteREE-tm1jOWgUaLHupwSfsl5TqG24vdqI-vXfaW_BPVzzpL/file# [following]\n",
            "--2021-11-26 13:45:51--  https://uc7634611f3b09e6cb94fe92f1e7.dl.dropboxusercontent.com/cd/0/inline/BauVS9VHcmT79-fUTNtiUaXfvfXzLeaHDqIUzjEhY2F7t8B2KytIPkiZ6-KBL8AugN_Sos9nlZVwxv2S3u2Y0FYiCTEV8JF9t7HeHRyCteREE-tm1jOWgUaLHupwSfsl5TqG24vdqI-vXfaW_BPVzzpL/file\n",
            "Resolving uc7634611f3b09e6cb94fe92f1e7.dl.dropboxusercontent.com (uc7634611f3b09e6cb94fe92f1e7.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uc7634611f3b09e6cb94fe92f1e7.dl.dropboxusercontent.com (uc7634611f3b09e6cb94fe92f1e7.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26233379 (25M) [text/plain]\n",
            "Saving to: ‘positive.csv’\n",
            "\n",
            "positive.csv        100%[===================>]  25.02M  55.0MB/s    in 0.5s    \n",
            "\n",
            "2021-11-26 13:45:52 (55.0 MB/s) - ‘positive.csv’ saved [26233379/26233379]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcXFWXlgmHOA"
      },
      "source": [
        "pos_tweets = pd.read_csv('positive.csv', encoding='utf-8', sep=';', header=None,  names=[0,1,2,'text','tone',5,6,7,8,9,10,11])\n",
        "neg_tweets = pd.read_csv('negative.csv', encoding='utf-8', sep=';', header=None, names=[0,1,2,'text','tone',5,6,7,8,9,10,11] )\n",
        "neg_tweets['tone'] = 0"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDSC-8IWmcAc"
      },
      "source": [
        "all_tweets_data = pos_tweets.append(neg_tweets)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LluMFRJJmqvj"
      },
      "source": [
        "tweets_data = shuffle(all_tweets_data[['text','tone']])[:100000]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_97SXgHOoEA5"
      },
      "source": [
        "train_sentences, val_sentences = train_test_split(tweets_data, test_size=0.1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "rHptbOatoFqN",
        "outputId": "317e6bb7-9f65-42f4-afc1-49693f64ffd9"
      },
      "source": [
        "val_sentences[:10]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>tone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>87918</th>\n",
              "      <td>@solnisko_123 ахах) я не прочь иметь схожие че...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77423</th>\n",
              "      <td>RT @creature_sleepy: я не чеми\\nчеми тож тут \\...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102215</th>\n",
              "      <td>@arianagranqe *проснулся после Гранде. Просто ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54939</th>\n",
              "      <td>Доброе деревенское утро)): Посмотреть на Яндек...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7164</th>\n",
              "      <td>аааааааа сегодня в школе сломала 3 ногтя :(</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83130</th>\n",
              "      <td>@abforever98 нит. Я просто обижена. И обидела ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47873</th>\n",
              "      <td>Что-то у меня совсем нету новогоднего настроен...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19125</th>\n",
              "      <td>@love_tommy_joe КОНЕЧНО, БЕССПОРНО, БЕЗУСЛОВНО...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74139</th>\n",
              "      <td>\"@cat_dormidont: @patya3661 доброе утро)) http...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105179</th>\n",
              "      <td>“@queen_viktoriy: @JonVlasov @valgor79 @rubin_...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text  tone\n",
              "87918   @solnisko_123 ахах) я не прочь иметь схожие че...     1\n",
              "77423   RT @creature_sleepy: я не чеми\\nчеми тож тут \\...     0\n",
              "102215  @arianagranqe *проснулся после Гранде. Просто ...     1\n",
              "54939   Доброе деревенское утро)): Посмотреть на Яндек...     1\n",
              "7164          аааааааа сегодня в школе сломала 3 ногтя :(     0\n",
              "83130   @abforever98 нит. Я просто обижена. И обидела ...     0\n",
              "47873   Что-то у меня совсем нету новогоднего настроен...     0\n",
              "19125   @love_tommy_joe КОНЕЧНО, БЕССПОРНО, БЕЗУСЛОВНО...     1\n",
              "74139   \"@cat_dormidont: @patya3661 доброе утро)) http...     1\n",
              "105179  “@queen_viktoriy: @JonVlasov @valgor79 @rubin_...     1"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xeu0bKxoHBY"
      },
      "source": [
        "def preprocess(text):\n",
        "    tokens = text.split()\n",
        "    #tokens = [token.strip(punctuation) for token in tokens]\n",
        "    return tokens"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx2SWW-_f_pR"
      },
      "source": [
        "def preprocess1(text):\n",
        "    tokens = list(text)\n",
        "    #tokens = [token.strip(punctuation) for token in tokens]\n",
        "    return tokens"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQHI5q40plB7",
        "outputId": "f128d0b0-55c8-43c6-9626-20969ae06c59"
      },
      "source": [
        "vocab = Counter()\n",
        "for text in tweets_data['text']:\n",
        "    vocab.update(preprocess(text))\n",
        "print('всего уникальных токенов:', len(vocab))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "всего уникальных токенов: 305195\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXUyTNg9fb2j",
        "outputId": "758f5d35-2a64-43d0-c3a3-c830c342aa73"
      },
      "source": [
        "vocab_sym = Counter()\n",
        "for text in tweets_data['text']:\n",
        "    vocab_sym.update(preprocess1(text))\n",
        "print('всего уникальных токенов:', len(vocab_sym))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "всего уникальных токенов: 348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LPWyS7bpmp1",
        "outputId": "d10e2b88-d844-4cc1-d5f2-61a7f0b983e3"
      },
      "source": [
        "filtered_vocab = set()\n",
        "\n",
        "for word in vocab:\n",
        "    if vocab[word] > 2:\n",
        "        filtered_vocab.add(word)\n",
        "print('уникальных токенов, вcтретившихся больше 2 раз:', len(filtered_vocab))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "уникальных токенов, вcтретившихся больше 2 раз: 37786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqLGzzdcY6DB",
        "outputId": "5c60ad5b-b2d8-4ff1-b970-be659da20cdb"
      },
      "source": [
        "filtered_vocab_sym = set()\n",
        "\n",
        "for symbol in vocab_sym:\n",
        "    if vocab_sym[symbol] > 5:\n",
        "        filtered_vocab_sym.add(symbol)\n",
        "print('уникальных символов, втретившихся больше 5 раз:', len(filtered_vocab_sym))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "уникальных символов, втретившихся больше 5 раз: 208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi2CmQaYprrB"
      },
      "source": [
        "#создаем словарь с индексами word2id, для спецсимвола паддинга дефолтный индекс - 0\n",
        "word2id = {'PAD':0}\n",
        "\n",
        "for word in filtered_vocab:\n",
        "    word2id[word] = len(word2id)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpLucfZEV7lT"
      },
      "source": [
        "symbol2id = {'PAD':0}\n",
        "\n",
        "for symbol in filtered_vocab_sym:\n",
        "    symbol2id[symbol] = len(symbol2id)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4HGCuSrpuHj"
      },
      "source": [
        "#обратный словарь для того, чтобы раскодировать последовательность\n",
        "id2word = {i:word for word, i in word2id.items()}"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkQzIqEsYbwo"
      },
      "source": [
        "id2symbol = {i:symbol for symbol, i in symbol2id.items()}"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXZo19n9pvTu",
        "outputId": "63eb765c-22df-41f2-99c8-7387382a4055"
      },
      "source": [
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "DEVICE"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRvOHM6ftozJ"
      },
      "source": [
        "class TweetsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, word2id, DEVICE):\n",
        "        self.dataset = dataset['text'].values\n",
        "        self.word2id = word2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = dataset['tone'].values\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index): \n",
        "        symbols = self.dataset[index].split()\n",
        "        ids = torch.LongTensor([self.word2id[symbol] for symbol in symbols if symbol in self.word2id])\n",
        "        y = [self.target[index]]\n",
        "        return ids, y\n",
        "\n",
        "    def collate_fn(self, batch): #этот метод можно реализовывать и отдельно,\n",
        "    # он понадобится для DataLoader во время итерации по батчам\n",
        "      ids, y = list(zip(*batch))\n",
        "      padded_ids = pad_sequence(ids, batch_first=True).to(self.device)\n",
        "      y = torch.Tensor(y).to(self.device)\n",
        "      return padded_ids, y"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "_U_Dv5WZumBH",
        "outputId": "efb013b0-4a88-4bd5-8b8f-7bd78411c80f"
      },
      "source": [
        "train_sentences"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>tone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>97457</th>\n",
              "      <td>@add__death только проснулась,хорошо но как вс...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101972</th>\n",
              "      <td>Бывшим надо желать только удачи. Счастье свое ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48992</th>\n",
              "      <td>На мероприятии, посвященном дню борьбы с корру...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74464</th>\n",
              "      <td>Я на фотках такая классная, а в жизни такое чмо(</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64804</th>\n",
              "      <td>Приятно когда стиль моего артиста Анны Павлово...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40610</th>\n",
              "      <td>Всё-таки кофе в метро - это здорово. Славьтесь...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32515</th>\n",
              "      <td>восхищаешься человеком ровно до того момента,п...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24438</th>\n",
              "      <td>Во-вторых, мы ничего пока не сделали. &amp;gt;:-D ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95078</th>\n",
              "      <td>Круто в первый день учебы я получила 2 по ин.я...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58704</th>\n",
              "      <td>@MegaFonHelp Перезагрузил устройство. Подёргал...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>90000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text  tone\n",
              "97457   @add__death только проснулась,хорошо но как вс...     1\n",
              "101972  Бывшим надо желать только удачи. Счастье свое ...     1\n",
              "48992   На мероприятии, посвященном дню борьбы с корру...     1\n",
              "74464    Я на фотках такая классная, а в жизни такое чмо(     0\n",
              "64804   Приятно когда стиль моего артиста Анны Павлово...     1\n",
              "...                                                   ...   ...\n",
              "40610   Всё-таки кофе в метро - это здорово. Славьтесь...     1\n",
              "32515   восхищаешься человеком ровно до того момента,п...     1\n",
              "24438   Во-вторых, мы ничего пока не сделали. &gt;:-D ...     1\n",
              "95078   Круто в первый день учебы я получила 2 по ин.я...     0\n",
              "58704   @MegaFonHelp Перезагрузил устройство. Подёргал...     0\n",
              "\n",
              "[90000 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUcgEN7lto8n"
      },
      "source": [
        "train_dataset = TweetsDataset(train_sentences, word2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n48EBNCYvAQI"
      },
      "source": [
        "val_dataset = TweetsDataset(val_sentences, word2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NrAnHgxvASY"
      },
      "source": [
        "test_batch = next(iter(val_iterator))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnssgFoYvAbN"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.conv =  nn.Conv1d(in_channels=180, out_channels=50, kernel_size=2, padding='same')\n",
        "        self.hidden = nn.Linear(in_features=50, out_features=1)\n",
        "        #self.dropout = nn.Dropout(p=0.5)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word):\n",
        "        embedded = self.embedding(word)\n",
        "        embedded = embedded.transpose(1,2)\n",
        "        feature_map_bigrams = self.bigrams(embedded)\n",
        "        feature_map_trigrams = self.trigrams(embedded)\n",
        "        concat = self.conv(torch.cat((feature_map_bigrams, feature_map_trigrams), 1))\n",
        "        pooling = self.pooling(concat).max(2)[0]\n",
        "        logits = self.hidden(pooling) \n",
        "        logits = self.out(logits)      \n",
        "        return logits"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3MpKf4vw9P5"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, print_v=True):\n",
        "    epoch_loss = 0 # для подсчета среднего лосса на всех батчах\n",
        "\n",
        "    model.train()  # ставим модель в обучение, явно указываем, что сейчас надо будет хранить градиенты у всех весов\n",
        "\n",
        "    for i, (texts, ys) in enumerate(iterator): #итерируемся по батчам\n",
        "        optimizer.zero_grad()  #обнуляем градиенты\n",
        "        preds = model(texts)  #прогоняем данные через модель\n",
        "        loss = criterion(preds, ys) #считаем значение функции потерь  \n",
        "        loss.backward() #считаем градиенты  \n",
        "        optimizer.step() #обновляем веса \n",
        "        epoch_loss += loss.item() #сохраняем значение функции потерь\n",
        "        if print_v == True:\n",
        "          if not (i + 1) % int(len(iterator)/5):\n",
        "              print(f'Train loss: {epoch_loss/i}')      \n",
        "    return  epoch_loss / len(iterator) # возвращаем среднее значение лосса по всей выборке"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq3J2NVew_u7"
      },
      "source": [
        "def evaluate(model, iterator, criterion, print_v=True):\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for i, (texts, ys) in enumerate(iterator):   \n",
        "            preds = model(texts)  # делаем предсказания на тесте\n",
        "            loss = criterion(preds, ys)   # считаем значения функции ошибки для статистики  \n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = f1(preds.round().long(), ys.long(), ignore_index=0)\n",
        "            epoch_metric += batch_metric\n",
        "            if print_v == True:\n",
        "              if not (i + 1) % int(len(iterator)/5):\n",
        "                print(f'Val loss: {epoch_loss/i}, Val f1: {epoch_metric/i}')\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator) # возвращаем среднее значение по всей выборке"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSoFQpdSxEUY"
      },
      "source": [
        "batch, y = next(iter(train_iterator))\n",
        "batch, y = batch.to(device='cpu'), y.to(device='cpu')\n",
        "loss = nn.BCELoss()\n",
        "\n",
        "model = CNN(len(word2id), 8)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "# веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNB4C4DTxEdU",
        "outputId": "e3745435-ce2e-4a90-b499-95119c993088"
      },
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(20):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.7307682745158672\n",
            "Train loss: 0.6998978275241274\n",
            "Train loss: 0.6850063049793244\n",
            "Train loss: 0.6737979196790439\n",
            "Train loss: 0.6653144891772952\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6643787249922752, Val f1: 0.7103244066238403\n",
            "Val loss: 0.6429896119869116, Val f1: 0.6897682547569275\n",
            "Val loss: 0.6370397651195526, Val f1: 0.6826779246330261\n",
            "Val loss: 0.6331581331011075, Val f1: 0.679706871509552\n",
            "Val loss: 0.631295444000335, Val f1: 0.6779122352600098\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2437514066696167, Val f1: 1.3434028625488281\n",
            "Val loss: 0.8290635347366333, Val f1: 0.8930319547653198\n",
            "Val loss: 0.7467913508415223, Val f1: 0.8073295950889587\n",
            "Val loss: 0.7122343523161752, Val f1: 0.7690405249595642\n",
            "Val loss: 0.6950439545843337, Val f1: 0.7453333735466003\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.6597053036093712\n",
            "Train loss: 0.6365224068815057\n",
            "Train loss: 0.6258238697052002\n",
            "Train loss: 0.6211127638816833\n",
            "Train loss: 0.6161700898692721\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6321487836539745, Val f1: 0.6823388934135437\n",
            "Val loss: 0.612254509420106, Val f1: 0.6629547476768494\n",
            "Val loss: 0.6052240908145905, Val f1: 0.6560408473014832\n",
            "Val loss: 0.6020484370971794, Val f1: 0.6536921858787537\n",
            "Val loss: 0.6005735936618987, Val f1: 0.650610089302063\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.192443609237671, Val f1: 1.2736129760742188\n",
            "Val loss: 0.7932039300600687, Val f1: 0.8418804407119751\n",
            "Val loss: 0.7152190804481506, Val f1: 0.7604737281799316\n",
            "Val loss: 0.6832142642566136, Val f1: 0.7259799838066101\n",
            "Val loss: 0.6666103932592604, Val f1: 0.7044739127159119\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.6245975866913795\n",
            "Train loss: 0.6039005337339459\n",
            "Train loss: 0.5952148532867432\n",
            "Train loss: 0.5912136079660103\n",
            "Train loss: 0.5884412186486381\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6056877672672272, Val f1: 0.7027802467346191\n",
            "Val loss: 0.5874240217786847, Val f1: 0.6832736134529114\n",
            "Val loss: 0.5815872716903686, Val f1: 0.6756105422973633\n",
            "Val loss: 0.5775559651317881, Val f1: 0.6721011996269226\n",
            "Val loss: 0.5757984866698583, Val f1: 0.6705103516578674\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.148255169391632, Val f1: 1.288647174835205\n",
            "Val loss: 0.7656428813934326, Val f1: 0.8605655431747437\n",
            "Val loss: 0.6905395269393921, Val f1: 0.7769421935081482\n",
            "Val loss: 0.6589052336556571, Val f1: 0.7446951270103455\n",
            "Val loss: 0.643323560555776, Val f1: 0.7201964855194092\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.6025517880916595\n",
            "Train loss: 0.5840927651434233\n",
            "Train loss: 0.5760420477390289\n",
            "Train loss: 0.5706006021641973\n",
            "Train loss: 0.5673131112541471\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5844834074378014, Val f1: 0.7149066925048828\n",
            "Val loss: 0.5676685806476709, Val f1: 0.6955016851425171\n",
            "Val loss: 0.5613548016548157, Val f1: 0.6879312992095947\n",
            "Val loss: 0.5575661552486135, Val f1: 0.6851822137832642\n",
            "Val loss: 0.5555009515512557, Val f1: 0.6837345361709595\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1126605868339539, Val f1: 1.3107020854949951\n",
            "Val loss: 0.7442824045817057, Val f1: 0.8744841814041138\n",
            "Val loss: 0.67130606174469, Val f1: 0.791480541229248\n",
            "Val loss: 0.6399800777435303, Val f1: 0.7569454908370972\n",
            "Val loss: 0.6246612800492181, Val f1: 0.7325688004493713\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.5813812837004662\n",
            "Train loss: 0.5653712424364957\n",
            "Train loss: 0.5578926062583923\n",
            "Train loss: 0.5528711383022479\n",
            "Train loss: 0.5487031574760165\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5720678120851517, Val f1: 0.6967413425445557\n",
            "Val loss: 0.5527198350790775, Val f1: 0.6801706552505493\n",
            "Val loss: 0.5481020736694336, Val f1: 0.6732543110847473\n",
            "Val loss: 0.5455056375532008, Val f1: 0.6696489453315735\n",
            "Val loss: 0.5442609148366111, Val f1: 0.6673040390014648\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0978621244430542, Val f1: 1.280427098274231\n",
            "Val loss: 0.7326771219571432, Val f1: 0.8559750914573669\n",
            "Val loss: 0.6616628646850586, Val f1: 0.7737048864364624\n",
            "Val loss: 0.6311812656266349, Val f1: 0.7383836507797241\n",
            "Val loss: 0.6159500413470798, Val f1: 0.7143166661262512\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.5662876404821873\n",
            "Train loss: 0.5466432174046835\n",
            "Train loss: 0.540318831205368\n",
            "Train loss: 0.5358346224720798\n",
            "Train loss: 0.5319697232473464\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5428359899669886, Val f1: 0.7474114298820496\n",
            "Val loss: 0.5289452753283761, Val f1: 0.7224634885787964\n",
            "Val loss: 0.5256419318914414, Val f1: 0.7143324613571167\n",
            "Val loss: 0.5232330280453411, Val f1: 0.7105193138122559\n",
            "Val loss: 0.5226981366674105, Val f1: 0.7078187465667725\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0587865114212036, Val f1: 1.3854998350143433\n",
            "Val loss: 0.7095139225323995, Val f1: 0.9132468104362488\n",
            "Val loss: 0.6405260920524597, Val f1: 0.818446934223175\n",
            "Val loss: 0.6101629904338292, Val f1: 0.7810544371604919\n",
            "Val loss: 0.5957135226991441, Val f1: 0.7551325559616089\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.5449542999267578\n",
            "Train loss: 0.5313849376909661\n",
            "Train loss: 0.5239243704080582\n",
            "Train loss: 0.5192914298221246\n",
            "Train loss: 0.5160294391569638\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5235107373446226, Val f1: 0.7696498036384583\n",
            "Val loss: 0.5114187200864156, Val f1: 0.7447492480278015\n",
            "Val loss: 0.5083993363380432, Val f1: 0.7342900037765503\n",
            "Val loss: 0.5071857513776467, Val f1: 0.7296836972236633\n",
            "Val loss: 0.50631402078129, Val f1: 0.7270851731300354\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0313315391540527, Val f1: 1.411360740661621\n",
            "Val loss: 0.6921450694402059, Val f1: 0.9307136535644531\n",
            "Val loss: 0.6250214338302612, Val f1: 0.8367647528648376\n",
            "Val loss: 0.5957279460770744, Val f1: 0.7969697713851929\n",
            "Val loss: 0.5817564593421088, Val f1: 0.7728227972984314\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.5346694886684418\n",
            "Train loss: 0.513264536857605\n",
            "Train loss: 0.5071379816532136\n",
            "Train loss: 0.5057103954144379\n",
            "Train loss: 0.5036301868302482\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5195969566702843, Val f1: 0.7636814713478088\n",
            "Val loss: 0.5043173098202908, Val f1: 0.7433880567550659\n",
            "Val loss: 0.4988867485523224, Val f1: 0.7366191744804382\n",
            "Val loss: 0.49688019547889484, Val f1: 0.7319725751876831\n",
            "Val loss: 0.49491474706502186, Val f1: 0.7299349904060364\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0162183344364166, Val f1: 1.398718237876892\n",
            "Val loss: 0.6803180078665415, Val f1: 0.9264386892318726\n",
            "Val loss: 0.6146251261234283, Val f1: 0.8319612741470337\n",
            "Val loss: 0.5856958329677582, Val f1: 0.7949153184890747\n",
            "Val loss: 0.5718427134884728, Val f1: 0.7717365026473999\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.5174240451306105\n",
            "Train loss: 0.5015991492704912\n",
            "Train loss: 0.4957948303222656\n",
            "Train loss: 0.49242453165908356\n",
            "Train loss: 0.48982375150635127\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.49610021710395813, Val f1: 0.8035681843757629\n",
            "Val loss: 0.4878570049098044, Val f1: 0.7711266279220581\n",
            "Val loss: 0.4820957726240158, Val f1: 0.7637732625007629\n",
            "Val loss: 0.48051101191719964, Val f1: 0.7583491206169128\n",
            "Val loss: 0.47998194573890596, Val f1: 0.7562063932418823\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9886284470558167, Val f1: 1.4472219944000244\n",
            "Val loss: 0.6659003098805746, Val f1: 0.958321750164032\n",
            "Val loss: 0.6019526958465576, Val f1: 0.8600198030471802\n",
            "Val loss: 0.573574355670384, Val f1: 0.8219667077064514\n",
            "Val loss: 0.5599378479851617, Val f1: 0.7975977659225464\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.507849795743823\n",
            "Train loss: 0.4902715132091985\n",
            "Train loss: 0.48623519718647\n",
            "Train loss: 0.4827189943683681\n",
            "Train loss: 0.4788587238816988\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.49873269349336624, Val f1: 0.8101799488067627\n",
            "Val loss: 0.4808558758461114, Val f1: 0.7887484431266785\n",
            "Val loss: 0.4782544392347336, Val f1: 0.7785742878913879\n",
            "Val loss: 0.47473547262931937, Val f1: 0.7767782211303711\n",
            "Val loss: 0.4725771152547428, Val f1: 0.7749267220497131\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9779086709022522, Val f1: 1.4800995588302612\n",
            "Val loss: 0.6609468857447306, Val f1: 0.977475643157959\n",
            "Val loss: 0.5969772279262543, Val f1: 0.877264142036438\n",
            "Val loss: 0.5687061761106763, Val f1: 0.839921236038208\n",
            "Val loss: 0.5548928810490502, Val f1: 0.8177317380905151\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.4929433558136225\n",
            "Train loss: 0.47821562398563733\n",
            "Train loss: 0.47279142558574677\n",
            "Train loss: 0.46938000641652006\n",
            "Train loss: 0.4664797704844248\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4813764449208975, Val f1: 0.8030792474746704\n",
            "Val loss: 0.4677104986075199, Val f1: 0.781988799571991\n",
            "Val loss: 0.4625955766439438, Val f1: 0.772807776927948\n",
            "Val loss: 0.4600102189761489, Val f1: 0.767957329750061\n",
            "Val loss: 0.45779057166406084, Val f1: 0.765961229801178\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9578913748264313, Val f1: 1.4579603672027588\n",
            "Val loss: 0.645393302043279, Val f1: 0.9662520885467529\n",
            "Val loss: 0.5837516188621521, Val f1: 0.862432599067688\n",
            "Val loss: 0.5567402924810138, Val f1: 0.825372040271759\n",
            "Val loss: 0.5429960820409987, Val f1: 0.8010744452476501\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.47808629274368286\n",
            "Train loss: 0.46307719656915375\n",
            "Train loss: 0.46119937002658845\n",
            "Train loss: 0.4584600422809373\n",
            "Train loss: 0.45755973138979505\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4722207058221102, Val f1: 0.8010355234146118\n",
            "Val loss: 0.4582702941966779, Val f1: 0.776484489440918\n",
            "Val loss: 0.4535950541496277, Val f1: 0.7699424028396606\n",
            "Val loss: 0.4509711772648256, Val f1: 0.7669996023178101\n",
            "Val loss: 0.4492840887535186, Val f1: 0.7643907070159912\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9477200210094452, Val f1: 1.4513702392578125\n",
            "Val loss: 0.6360809504985809, Val f1: 0.9669684171676636\n",
            "Val loss: 0.5777331531047821, Val f1: 0.861359715461731\n",
            "Val loss: 0.551262732063021, Val f1: 0.8219969272613525\n",
            "Val loss: 0.5379437373744117, Val f1: 0.7982262372970581\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.4724597316235304\n",
            "Train loss: 0.45345493428634875\n",
            "Train loss: 0.44850558280944824\n",
            "Train loss: 0.44644091600802405\n",
            "Train loss: 0.44641136271612986\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.452261246740818, Val f1: 0.8201389312744141\n",
            "Val loss: 0.4461052824150432, Val f1: 0.7900640368461609\n",
            "Val loss: 0.4415901136398315, Val f1: 0.7816595435142517\n",
            "Val loss: 0.4392791208046586, Val f1: 0.7777519822120667\n",
            "Val loss: 0.4381399303674698, Val f1: 0.7761551141738892\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9338676333427429, Val f1: 1.4617500305175781\n",
            "Val loss: 0.6270404656728109, Val f1: 0.9744890928268433\n",
            "Val loss: 0.5686075329780579, Val f1: 0.8689857721328735\n",
            "Val loss: 0.542227817433221, Val f1: 0.8323118090629578\n",
            "Val loss: 0.529375884268019, Val f1: 0.8080238699913025\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.45746174454689026\n",
            "Train loss: 0.4434396200107806\n",
            "Train loss: 0.4397940647602081\n",
            "Train loss: 0.43782059069889695\n",
            "Train loss: 0.43635171616361257\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.45258145965635777, Val f1: 0.8173488974571228\n",
            "Val loss: 0.4370217594233426, Val f1: 0.792610228061676\n",
            "Val loss: 0.4331734448671341, Val f1: 0.7840300798416138\n",
            "Val loss: 0.4320224803775104, Val f1: 0.7783696055412292\n",
            "Val loss: 0.42997938642899197, Val f1: 0.7768771648406982\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9279183745384216, Val f1: 1.4605401754379272\n",
            "Val loss: 0.6218713521957397, Val f1: 0.9710497856140137\n",
            "Val loss: 0.5650340795516968, Val f1: 0.8656362891197205\n",
            "Val loss: 0.5394116044044495, Val f1: 0.8277738094329834\n",
            "Val loss: 0.5263773500919342, Val f1: 0.8039752244949341\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.4484171513468027\n",
            "Train loss: 0.4358671479152911\n",
            "Train loss: 0.43206227481365206\n",
            "Train loss: 0.4287883101114586\n",
            "Train loss: 0.42719087643282755\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4395094085484743, Val f1: 0.8315697312355042\n",
            "Val loss: 0.42847474145166803, Val f1: 0.806756317615509\n",
            "Val loss: 0.424469496011734, Val f1: 0.7980538010597229\n",
            "Val loss: 0.4209920683903481, Val f1: 0.7954676747322083\n",
            "Val loss: 0.41957812578905196, Val f1: 0.793903112411499\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9136588871479034, Val f1: 1.497799038887024\n",
            "Val loss: 0.6136557658513387, Val f1: 0.9915121793746948\n",
            "Val loss: 0.5577835440635681, Val f1: 0.8843968510627747\n",
            "Val loss: 0.532757933650698, Val f1: 0.84432053565979\n",
            "Val loss: 0.5194214085737864, Val f1: 0.8205683827400208\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.44140675477683544\n",
            "Train loss: 0.4247654468724222\n",
            "Train loss: 0.42169746875762937\n",
            "Train loss: 0.4196070966435902\n",
            "Train loss: 0.4177223529134478\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.42896050214767456, Val f1: 0.8427809476852417\n",
            "Val loss: 0.4158357291510611, Val f1: 0.8166650533676147\n",
            "Val loss: 0.4112348186969757, Val f1: 0.8073557019233704\n",
            "Val loss: 0.4113401032205838, Val f1: 0.801758348941803\n",
            "Val loss: 0.4102662049588703, Val f1: 0.7987918257713318\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.902958333492279, Val f1: 1.5008891820907593\n",
            "Val loss: 0.6083358526229858, Val f1: 0.9938287734985352\n",
            "Val loss: 0.5518715560436249, Val f1: 0.8884060978889465\n",
            "Val loss: 0.5273359205041613, Val f1: 0.8485094308853149\n",
            "Val loss: 0.5140377548005846, Val f1: 0.8231236934661865\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.42721088975667953\n",
            "Train loss: 0.41329765861684625\n",
            "Train loss: 0.4100100237131119\n",
            "Train loss: 0.40754131728143833\n",
            "Train loss: 0.4082522392272949\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.42308449000120163, Val f1: 0.8415045738220215\n",
            "Val loss: 0.4102531536058946, Val f1: 0.8177435398101807\n",
            "Val loss: 0.4043643689155579, Val f1: 0.810765266418457\n",
            "Val loss: 0.4024197121164692, Val f1: 0.8066587448120117\n",
            "Val loss: 0.4008106120995113, Val f1: 0.804622232913971\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8916079699993134, Val f1: 1.5027656555175781\n",
            "Val loss: 0.5998736123243967, Val f1: 0.9939985275268555\n",
            "Val loss: 0.5450034737586975, Val f1: 0.8875333666801453\n",
            "Val loss: 0.5206403349127088, Val f1: 0.8488661050796509\n",
            "Val loss: 0.507775389485889, Val f1: 0.8236739039421082\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.41628117859363556\n",
            "Train loss: 0.40672674775123596\n",
            "Train loss: 0.40328514397144316\n",
            "Train loss: 0.4017308643504755\n",
            "Train loss: 0.4008479725037302\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.41599647141993046, Val f1: 0.8539993762969971\n",
            "Val loss: 0.4014563199245568, Val f1: 0.826906681060791\n",
            "Val loss: 0.3966514587402344, Val f1: 0.8207981586456299\n",
            "Val loss: 0.39377172358000456, Val f1: 0.8172083497047424\n",
            "Val loss: 0.3920777241388957, Val f1: 0.8147638440132141\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8826054632663727, Val f1: 1.5262386798858643\n",
            "Val loss: 0.5941272179285685, Val f1: 1.0094687938690186\n",
            "Val loss: 0.5397478461265564, Val f1: 0.9002407193183899\n",
            "Val loss: 0.5157374143600464, Val f1: 0.8587495684623718\n",
            "Val loss: 0.5028155148029327, Val f1: 0.835374653339386\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.41321094892919064\n",
            "Train loss: 0.40038891181801306\n",
            "Train loss: 0.3930857479572296\n",
            "Train loss: 0.39323588583006786\n",
            "Train loss: 0.3916218273696445\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4053003955632448, Val f1: 0.8531181812286377\n",
            "Val loss: 0.39217216589234094, Val f1: 0.8275570869445801\n",
            "Val loss: 0.3871665185689926, Val f1: 0.8214027285575867\n",
            "Val loss: 0.386445817217898, Val f1: 0.8153272271156311\n",
            "Val loss: 0.3854975345588866, Val f1: 0.812692403793335\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8816070854663849, Val f1: 1.5113580226898193\n",
            "Val loss: 0.5912755330403646, Val f1: 0.9999388456344604\n",
            "Val loss: 0.5380428194999695, Val f1: 0.8925527930259705\n",
            "Val loss: 0.5143760825906482, Val f1: 0.8525535464286804\n",
            "Val loss: 0.5014209349950155, Val f1: 0.8274414539337158\n",
            "\n",
            "starting Epoch 19\n",
            "Training...\n",
            "Train loss: 0.39665823243558407\n",
            "Train loss: 0.3898703157901764\n",
            "Train loss: 0.3902659237384796\n",
            "Train loss: 0.38758144850161536\n",
            "Train loss: 0.38623405815589995\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.39283902384340763, Val f1: 0.8690842390060425\n",
            "Val loss: 0.38466883337858954, Val f1: 0.8392333984375\n",
            "Val loss: 0.38055640637874605, Val f1: 0.8303806185722351\n",
            "Val loss: 0.37776626980126793, Val f1: 0.8266565799713135\n",
            "Val loss: 0.37550880511601764, Val f1: 0.8257548213005066\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8690172731876373, Val f1: 1.5393974781036377\n",
            "Val loss: 0.5851379036903381, Val f1: 1.017296314239502\n",
            "Val loss: 0.531946474313736, Val f1: 0.9093109369277954\n",
            "Val loss: 0.5089334888117654, Val f1: 0.8677073121070862\n",
            "Val loss: 0.49567973282602096, Val f1: 0.843471884727478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUui5ZU1hPEm"
      },
      "source": [
        "def predict(model, iterator):\n",
        "    model.eval()\n",
        "    fp = []\n",
        "    fn = []\n",
        "    tp = [] \n",
        "    tn = []\n",
        "    with torch.no_grad():\n",
        "        for i, (texts, ys) in enumerate(iterator):   \n",
        "            preds = model(texts)  # делаем предсказания на тесте \n",
        "            for pred, gold, text in zip(preds, ys, texts):\n",
        "              text = ' '.join([id2word[int(symbol)] for symbol in text if symbol !=0])\n",
        "              if round(pred.item()) > gold:\n",
        "                fp.append(text)\n",
        "              elif round(pred.item()) < gold:\n",
        "                fn.append(text)\n",
        "              elif round(pred.item()) == gold == 1:\n",
        "                tp.append(text)\n",
        "              elif round(pred.item()) == gold == 0:\n",
        "                tn.append(text)\n",
        "    accuracy = (len(tp)+len(tn))/(len(tp)+len(fp)+len(fn)+len(tn))\n",
        "    precision = len(tp)/(len(tp)+len(fp))\n",
        "    recall = len(tp)/(len(tp)+len(fn))\n",
        "    return fp, fn, tp, tn, accuracy, precision, recall"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "nsJPvP7iCMl6",
        "outputId": "a694b5c1-0238-4305-8718-2b3ebe802719"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(losses)\n",
        "plt.plot(losses_eval)\n",
        "plt.title('BCE loss value')\n",
        "plt.ylabel('BCE loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "fNCKDMtNCTHI",
        "outputId": "0fa31d5a-a454-4daf-e694-9abd17eb8277"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(f1s)\n",
        "plt.plot(f1s_eval)\n",
        "plt.title('f1 value')\n",
        "plt.ylabel('f1 value')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P8OYwMRhVg5"
      },
      "source": [
        "fp, fn, tp, tn, accuracy, precision, recall = predict(model, val_iterator)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG6qpKF7hbUM",
        "outputId": "2eb73add-5162-4036-8163-7beb6215e92d"
      },
      "source": [
        "print('accuracy:', accuracy)\n",
        "print('precision:', precision)\n",
        "print('recall:', recall)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.7567\n",
            "precision: 0.762782401902497\n",
            "recall: 0.7569321533923303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T31rEutti6P1",
        "outputId": "6441072d-656f-43d5-d365-8d9dad520e3a"
      },
      "source": [
        "best_acc = 0\n",
        "emb_val = ''\n",
        "\n",
        "for emb in [5, 10, 20, 50, 100]:\n",
        "#  for lr in [0.0005, 0.001, 0.01, 0.1]:\n",
        "    model = CNN(len(word2id), emb)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "    criterion = nn.BCELoss()  \n",
        "\n",
        "    # веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "    model = model.to(DEVICE)\n",
        "    criterion = criterion.to(DEVICE)\n",
        "\n",
        "    losses = []\n",
        "    losses_eval = []\n",
        "    f1s = []\n",
        "    f1s_eval = []\n",
        "\n",
        "    for i in range(10):\n",
        "        print(f'starting Epoch {i}')\n",
        "        epoch_loss = train(model, train_iterator, optimizer, criterion, print_v=False)\n",
        "        losses.append(epoch_loss)\n",
        "        f1_on_train,_ = evaluate(model, train_iterator, criterion, print_v=False)\n",
        "        f1s.append(f1_on_train)\n",
        "        f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion, print_v=False)\n",
        "        losses_eval.append(epoch_loss_on_test)\n",
        "        f1s_eval.append(f1_on_test)\n",
        "    fp, fn, tp, tn, accuracy, precision, recall = predict(model, val_iterator)\n",
        "    print(f'results for embedding_size={emb}')\n",
        "    print('accuracy:', accuracy)\n",
        "    print('precision:', precision)\n",
        "    print('recall:', recall)\n",
        "    if accuracy > best_acc:\n",
        "      best_acc = accuracy\n",
        "      emb_val = emb\n",
        "print(f'____________________________________________')\n",
        "print(f'best accuracy is {best_acc} for emb_size {emb_val}')"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for embedding_size=5\n",
            "accuracy: 0.6986\n",
            "precision: 0.7113446279219386\n",
            "recall: 0.665396188565697\n",
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for embedding_size=10\n",
            "accuracy: 0.7302\n",
            "precision: 0.7198615650836377\n",
            "recall: 0.7510531594784353\n",
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for embedding_size=20\n",
            "accuracy: 0.7537\n",
            "precision: 0.7573469387755102\n",
            "recall: 0.7444332998996991\n",
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for embedding_size=50\n",
            "accuracy: 0.7786\n",
            "precision: 0.7725752508361204\n",
            "recall: 0.7877632898696089\n",
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for embedding_size=100\n",
            "accuracy: 0.7945\n",
            "precision: 0.7869173521347435\n",
            "recall: 0.8060180541624875\n",
            "best accuracy is 0.7945 for emb_size 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMLeJ5zklIih",
        "outputId": "90e9e560-f02c-4ea5-eeb2-12544152febc"
      },
      "source": [
        "best_acc = 0\n",
        "lr_val = ''\n",
        "\n",
        "for lr in [0.0005, 0.001, 0.005, 0.01, 0.05]:\n",
        "    model = CNN(len(word2id), emb_val)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.BCELoss()  \n",
        "\n",
        "    # веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "    model = model.to(DEVICE)\n",
        "    criterion = criterion.to(DEVICE)\n",
        "\n",
        "    losses = []\n",
        "    losses_eval = []\n",
        "    f1s = []\n",
        "    f1s_eval = []\n",
        "\n",
        "    for i in range(10):\n",
        "        print(f'starting Epoch {i}')\n",
        "        epoch_loss = train(model, train_iterator, optimizer, criterion, print_v=False)\n",
        "        losses.append(epoch_loss)\n",
        "        f1_on_train,_ = evaluate(model, train_iterator, criterion, print_v=False)\n",
        "        f1s.append(f1_on_train)\n",
        "        f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion, print_v=False)\n",
        "        losses_eval.append(epoch_loss_on_test)\n",
        "        f1s_eval.append(f1_on_test)\n",
        "    fp, fn, tp, tn, accuracy, precision, recall = predict(model, val_iterator)\n",
        "    print(f'results for lr={lr}')\n",
        "    print('accuracy:', accuracy)\n",
        "    print('precision:', precision)\n",
        "    print('recall:', recall)\n",
        "    if accuracy > best_acc:\n",
        "      best_acc = accuracy\n",
        "      lr_val = lr\n",
        "print(f'____________________________________________')\n",
        "print(f'best accuracy is {best_acc} for lr {lr_val}')"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for lr=0.0005\n",
            "accuracy: 0.7858\n",
            "precision: 0.7672494829855236\n",
            "recall: 0.8186559679037111\n",
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for lr=0.001\n",
            "accuracy: 0.7999\n",
            "precision: 0.7857142857142857\n",
            "recall: 0.8230692076228686\n",
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for lr=0.005\n",
            "accuracy: 0.8163\n",
            "precision: 0.799125807677689\n",
            "recall: 0.843530591775326\n",
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for lr=0.01\n",
            "accuracy: 0.8168\n",
            "precision: 0.8014916810097533\n",
            "recall: 0.8407221664994985\n",
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for lr=0.05\n",
            "accuracy: 0.4985\n",
            "precision: 0.4985\n",
            "recall: 1.0\n",
            "best accuracy is 0.8168 for lr 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3cnJYIZnX71",
        "outputId": "3e61e453-ff6b-4b9f-fc88-26516d16d7f5"
      },
      "source": [
        "model = CNN(len(word2id), emb_val)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr_val)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)\n",
        "\n",
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(20):\n",
        "    print(f'starting Epoch {i}')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion, print_v=False)\n",
        "    losses.append(epoch_loss)\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion, print_v=False)\n",
        "    f1s.append(f1_on_train)\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion, print_v=False)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)\n",
        "fp, fn, tp, tn, accuracy, precision, recall = predict(model, val_iterator)\n",
        "print('Значения после подбора гипепараметров:')\n",
        "print('accuracy:', accuracy)\n",
        "print('precision:', precision)\n",
        "print('recall:', recall)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "starting Epoch 10\n",
            "starting Epoch 11\n",
            "starting Epoch 12\n",
            "starting Epoch 13\n",
            "starting Epoch 14\n",
            "starting Epoch 15\n",
            "starting Epoch 16\n",
            "starting Epoch 17\n",
            "starting Epoch 18\n",
            "starting Epoch 19\n",
            "Значения после подбора гипепараметров:\n",
            "accuracy: 0.8195\n",
            "precision: 0.8011363636363636\n",
            "recall: 0.8485456369107321\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDapzsKb3MVs",
        "outputId": "41605de8-2281-4106-947d-e1b63b87c99d"
      },
      "source": [
        "print('что правильно предсказываем:', tp[:100])"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "что правильно предсказываем: ['Еще не совсем, но у меня это в планах :) :) :)', 'наш по скорее', 'RT Кировский район в твиттере только и делает, что желает доброго утра)))', 'с моим чувством юмора я ничего комментировать не ))', 'у миня тут одна ток шлюшка :D', 'RT Браво! растёт на одном только острове - Редкий и себе привезла и друзьям', 'Я после', '@CakeeDean Добрый День МИЛАЯ :*', 'Просыпаться в понедельник в 9:30 просто прекрасно)))', '@angel_kimki да:) ахахах)) а вообще можно', 'ну если что пиши', 'зато видно, что ты своего дела! :DDDD', 'меня меня меня', 'реклама была и сказали что 12 числа будет', 'Добра тебе)', 'С девочками покупали мандаринки и прочие За одно и погуляли, меня подруга в весело*', 'На работе рекламная акция Взяли себе пять )))', '#HappyVIrthday #np - Вот что делает меня счастливой :)))))', 'сахар тоже. И И И Наверное, мысль ясна :)', 'Такая красивая девушка в с на', 'вчера слушал тоже в машине, старый рок )', 'Хрень какая-то, но идея интересная :)', 'как будто в детстве :D катались с на горках в площади Ленина :D', 'Что я делаю в 5 утра? же мышь от кошки :D', 'RT на следующей неделе будут каждый день забирать с', 'Я сегодня не пошла в школу. У меня сегодня концерт, а все бабушки приехали ко мне и я после вчерашнего заболела :D', 'Надя тааак меня что я так :)', 'RT @GruzdevVladimir Не там ничего нет, дайте прямую ссылку. Если вы в', 'RT хахаха) ТЫ як свет у', 'ахах, у меня эта песня давным давно в знала', 'А еще у меня новый крутой )))', '@MoselenaV @nessi44 Надо значит сделать, чтобы было все так)))', '- ну вот утро только а мы уже во всей квартире, благодаря @_Vladaa :D', 'теперь я с тобой и правда классная) Хоть раньше я ее считала чересчур', 'RT В ГОЛОС НА ВСЮ ЭТО :DD ТЫ ЭТО :DD', 'общаться в живую и хоть есть о чём то по тел. сразу все а', 'меня немного радует то, что я наизусть знаю песни из перед Рождеством х)', 'не знаю на счет а вот Даша та еще', '@loric_pozzni на', 'Мечта сбылась да :D —', 'главное, ни кого вместо меня в Питер не Владиком', 'RT Кстати говоря , чуть не забыла) Настю , когда мы сидели на истории :]', 'больше ждут что это будет что-то с', 'но каждую пятницу практически убегаю домой с работы :D', 'Сегодня 11.12.13 Что бы сбудется, надо', 'Ладно, немного а теперь собираюсь ко надеюсь завтра тоже будет хороший, а может и продуктивный день! ;-)', 'RT прочитал сборник \"Последнее желание\", это просто прекрасно! буду читать дальше)', 'а как же!!! должна быть в -)', 'RT меня и я вас)', '@VA_DavydoFF @RuFoteev она и в будь мы на ней продукты', 'RT @GulnaraKarimova: в просто пить Моя единственная люблю тебя http://t.co…', 'RT я вообще то не про это твит сделала,', '#ЛУЧИРАДОСТИОТРАДОСТИ ТЕБЕ, МОЙ НОВЫЙ ЧИТАТЕЛЬ:)', 'хочю побыстрее свою я', 'в нашей жизни и так иногда приходит время что-то менять :-) @', 'RT @katepolyakova96 всё правильно ты сказал ;)', 'обожаю, когда люди Ну и фраза :D', '@followyoutuber о, ты надо будет потом поискать х)', 'оказывается работает в', '-Дочь, ты что влюбилась? -не, мам, с чего ты взяла? -Да ты уже 3 часа суп вилкой ешь!)))', 'вы в этих планах на вечер:)', 'RT @jjangum: @gbaterdene гэж байсан хүн байгаа шүү :-))', 'Кот погоду на #улыбнуло ;) #ВзаимныйФолловинг', 'у меня там или еще бегает', 'Наконец-то фотки с', 'дружишь с — нет! он мой лучший друг))', 'НА МЫ С В С АЗАЗА Я ЖДУ ЭТУ НАДЕЮСЬ ВСЕ', 'что бы ни говорили про то, что одна голова -это хорошо, но всё тело по-любому лучше :D', 'правильно, Новый Год - семейный праздник)', 'RT Я от них кайфую :)', '– отличный фильм! энергии', 'А я нашла это))', 'Здравствуйте, Здравствуйте и будьте', 'Когда я начинаю подряд писать сообщения без и то всё, это не к добру :)', 'RT утро, жуйк :) а вот вам милый на песенку одной такой команды :)', 'RT сегодня я самая хоть горы свернуть с', '@Ex_Freund вон что зимой в Наполи продавать будем. У есть шанс обновить парк еще раз))', '@interfax_news и в но сцука', 'попросил сделать ну не отказывать же ему)))', 'Линта, а можно как нибудь посмотреть сколько твоему', '@Kantiha очень долго Сначала затем но потом решили :-)', 'по поводу я лишь могу ей показать путь, остальное зависит от нее)', 'чем ближе новый год, тем больше желания узнать что', 'RT спасибо за очень весёлый день и очередной не и за ве…', 'Папа с читают звучит очень)', 'Всё ровно мы самые лучшие , люблю свой класс ))))', 'У этой сучки сегодня др) С цветами сегодня Вообщем поздравляю тебя, моя, всего тебе)', 'всё таки секс не очень ... видишь что твориться начинает с мозгом ))))', 'RT наш сайт до человеческого состояния =) пока можно и оставить на время т…', 'RT сладких снов :*', 'Давно пора разделить лайки на \"мне нравится нажимать или о моем', 'Что не сделаешь, ради фото для #снег #утро #зима', 'мама тебе одной !я с ней с утра так ,это', 'Ты хоть раз смотрела порно :D — я только порно и', 'Интересные вещи', 'я только вот от врача пришёл :-) ты ихихи с: не', 'RT вот это Даша знает про что это', 'я непросто я его мужское причем чисто случайно, :)', 'Надо уроки Ведь скоро конец Надо исправлять оценки, но хотя в моём случае их Удачи всем:3', 'зверь Очень хочу, чтобы он победил на']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JV-mjHZ93ZUi",
        "outputId": "aa609249-5cc1-4dfc-e5e2-603675aff6c5"
      },
      "source": [
        "print('ошибочно не относим к фамилиям:', fn[:100])"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ошибочно не относим к фамилиям: ['RT друзья, летит из во в ближайшее время? за', 'кто войну и мир\" теперь не боится', 'седня была на я на права и пздц я не', 'ты чего так рано?', 'я даже боюсь представить что у тебя там... Наверное в прячешь целое :DD', 'Всё-таки засирать лс очень круто) 101 сообщение, не никакого смысла, конечно, немного, но для начала норм) Как-то', 'сперва я думала,что они очень моя подруга сразу подвох после случая в вк, так тем', 'Я КАК Я ДАЖЕ НЕ БУДУ ЭТО [2]', 'сначала я думал сейчас начал фишку', 'такая вот', 'не хочется но предложила продолжить традицию московских только,что пока ничего не предвидится', 'Надо делать видео для А да Смотрю', 'День не очень удался сегодня, но сейчас я круто а всё потому что меня главный спасибо от', 'RT @zackforex: уже и Киев скоро людьми. не', 'я видела трейлер просто', 'RT Получил винил на почте и иду улыбаюсь как', 'RT @_Nikita_Horan_ не за что, своих не', 'Желание уголок почти теперь можно по делам идти)', 'Спасибо,что меня от прошлой ты не', 'О как я во время включили через 15 мин каникулы в 2 сезон)', 'так после нг и лучше даже. Сейчас и так денег дохуища надо)', 'привет... я тут пытаюсь с Вашей помощью уже поняла как', 'там же даже не а для особо есть даже с общим', 'благодаря не 4 гимназии на игру, у нам в кармане 3 и весело поиграли между собой)', 'Когда у тебя по 5, жизнь начинает иметь', 'Бедняжка почти весь день. 8-) #TeamFollowback', 'RT Мы не сигареты лет. Это закон. так, девочка. Это', '4 часа назад флешку скинуть что я не', 'что-то я мне мама пела ту бездый ту бездый бездый ту когда с работы', 'Ахах, сам с я веду ну, а почему бы и нет?', 'Зато я теперь знаю что я не плохой учитель по танцам !)', 'сегодня этот репортаж он Дедом подрабатывает - просто воет от', 'и сок сегодня мой заказ был', 'меня дома была отличная школа', 'теперь мне опять стало', 'после школы поиграли в не', 'с сейчас и в Москве найти не', 'сегодня в школе подходят и -Почему плачешь? боюсь 2', '- Хорошие девушки по не а сидят дома - Пацаны, в P.S. старый, но', 'пришла и в школу не', 'то что пойдем наконец-то пойдем ***', 'папа просто мой мужик обиделась за то что он на он мне кушать', 'капец как я мои руки ужасно', 'Бабуля сказала когда в тебя увидела что ты плохой, некрасивый и будущий Да она обосрала его как только можно)))', 'а мое в в воскресенье', '@diananessi больше не буду) А я ходу с рукавами и никто не', 'Сегодня на тренировке мне разбили сразу две Разговаривать немного не удобно)', 'У меня в телефоне из музыки только', 'там много', 'а кто через полгода буду даже)', 'там конечно про живот но', 'К вечеру пошел мелкий снег! Не большой мороз', 'моя ради тебя на нг там делать не чего по', 'в смысле? оО откуда он тогда у нас?)', 'RT Подскажите где сейчас можно создать дневник. Раньше все были на Где все сидят', 'Сижу жду улице наконец-то валит снег)', 'уже пятница 13', 'Спасибо, Хороший день, много Выходной не', 'до а то нужно продолжать работать Меня ждет как обычно', 'Удачно сходила в интернет посылки из хочу на', 'RT ночи тебе, О/', 'нормально меня в магазин за 2 минуты до того,как я собралась идти', 'RT пошли с на Мисс 4 шк) болеть за наших но жюри не я не рада что так дали места', 'девушке на вокзале в : Какой Как тебя \" Мне нечего сказать. Я', 'хмм Новые посмотри хд если не смотрела конечно:) 2, я 2', '\"Я тебе, напишу, а тебя папа об х)', 'Хахаха пфф конечно же нет, это даже не', 'Мои руки пропахли но я счастлива, как никогда)', '@e_sivanova меня как то никто с этим праздником не поздравлял', 'У кого нет первого Кто победитель по', 'наверное там нет,но я не ты всегда можешь взять книгу в кафе с собой)', 'нас пол школы стало рисовать себе ужасно только делать нечего аха', 'А мне приснилось, что был старый новый год и предложила мне и проблема в том, что я', '#RT тут, не то во дворе тебя а то и :*) #FOLLOW', 'RT Мои мечты не сможет даже', 'привет ну как ты там? а знаешь, я скучаю, по твоим а мы обещали друг другу', 'Вот, что делает на работе, с @', 'да, согласна с тобой)) но вот например из нового альбома мне больше всего нравится My What You In The Dark :з', 'У кого что... А вот у меня, меня жена Чтоб не', 'Не всегда. Моя сестра едва ли не лучше, чем папа, водитель с Хотя куриц за рулем', 'я вчера только 1 раз поговорила и как то похуй вообще)', '=) вот уже и у вас то же самое .. что у нас 5 лет назад было', 'и когда он увидел меня не в джинсах и', 'в какой школе учишься и в каком классе, если — отвечаю последний раз! в 3 школе в 10 классе', '@Love_YourEnemy теперь буду предупреждать', 'Только состояние когда стараться двигаться всегда', 'RT @X_Alja: меня тож не Без', 'сегодня друга менты думал что то не оказалось проверка зато', 'слова из песни, отношения ко мне', 'Она милая только тогда Когда спит)', 'RT ко мне сегодня на потом неожиданно дома', 'Никогда не делал на как они теперь', 'Жалко её. Но мы все морально готовы к этому)', '@Omg_trololo БЛИН, МНЕ ТОЖЕ НАДО ТИПО НУ', 'А за свой подарок она будет чистить мандаринки мне на НГ)) ооо мой любимый', 'не, просто я очень плохой и не смотрел еще даже О, удачи тебе:3', 'ленту, в глубине души, где-то очень-очень мне даже самую чуточку жаль, что я не смотря этот', 'Не будем о знаю ты всегда со за меня и совсем уже скоро сейчас', '@DIRICFOREVER Я КАЖДЫЙ ДЕНЬ БУДУ ДЕЛАТЬ БЛЯ Я НЕ МОГУ ТЕБЕ НА ТЕЛЕФОН У МЕНЯ НЕТ НА', 'Сегодня Даша']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqHDrbLR3c9Z",
        "outputId": "c2a71e20-21b8-4fa3-edb3-7f72aff9e52a"
      },
      "source": [
        "print('ошибочно считаем фамилиями:', fp[:100])"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ошибочно считаем фамилиями: ['Последние дни я на', 'В 8:00 моя всьо буде нормально!', 'никакого настроения. хочу просто целый день лежать на полу и Хотелось чего-то но, ты не умеешь так', 'RT Говорят если у человека к нуждается в любви. А я вот думал, что он просто жрать сладкое', 'Его имя и он,навсегда останутся в моем сердце!(((', 'не могу его не даст мне спать(((', 'С в одной машине сегодня :33', 'действительно, тебе лучше пить только на нг', 'почему те люди которые мне нужны и редко заходят в соц', 'даааа мне совмещать и алкоголь', 'RT ну так не интересно! Я на игру(', '@batzorig2012 Одоо тэгвэл л гаргах дээ ((-: @HaliunaaTs', 'звезда', 'каблуки 20 см куплю и сразу(', 'Я хочу к брату не более его проблемы не дают нам всем', 'буду( я всегда буду скучать пока ты не будешь рядом со мной', 'Даже пирожки вкусно пахнут тогда как я на #боль', 'Там как раз ВЫ ТАКИЕ ОТ ЭТОЙ !!!!! И тот неловкий момент, когда', 'Наверно я скоро выучу все даты на которых мне не суждено', 'RT Последний магазинчик в и тот', 'в с началом сейчас', 'Неймар мой вечер:( Только бы побыстрее и', 'Надо чтобы и на следующих выходных я тут была. Хочу полную зарплату,', 'потому, что есть такие которые очень любят валить студентов на противные люди', 'зачем дали эту Она совершенно ей не подходит(', 'RT не в учебе(', 'потеряла новый номер напишу в', 'только хотела у тебя спросить ей я не могу, бо', 'проект моих друзей и в мире 10 минут времени для полезного дела! RT!', 'учитель у меня не было таких Был лишь гей, педофил и', '', 'Не нравится все же, когда девушки пытаются Девушки остаются девушками, даже когда косят под', 'У нас тока и и', 'да( может придумать', 'боль и слезы, даже когда ужасно плохо.... когда Дамблдор', 'настроение в у тебя хорошее его точно кто нибудь', 'RT #Снаступающимтвиттерский @1Dmagicc', 'в и делать', 'ахуенно соседи не оценили только:( да и музыку попросили Не ценят', 'подарите мне одеяло с или скиньте хотя бы ссылку, где его можно купить', 'Их мое Они предлагают участие в с с элементами и', 'этого составляет 30 миллионов', '@JamesBond_O_O ну это же Влад, всегда себе что-нибудь', 'ей ещё экзамен сдавать(', '@_casualaffair мою', 'Нет чтобы кофе пить или чай, я тут', 'мама постоянно не к добру это, не к', 'RT @anna_miley_anna :(:(:( если что счет сериала. буду рада тебе&lt;3', 'Шел человек весной, видит сел и', 'Я конченная Плакать и смеяться', '@kuroglieva нее, меня из', 'просто на красивые', 'RT @_OOO_OOO__: стоп мы же на третьем месте #mtvstars 30 seconds to Mars', 'я тоже на эту херню в Мало того, что просто бутерброд, так еще и есть невозможно было', '@TVOY_KUMIR_ он какой-то не он как будто', '@MAXIMOV69 Вот так и живем 20 лет с', 'если у тебя плохое настроение, то зачем его другим', 'не помню( года четыре может. в которую ей такая', 'я умру от', 'да не, все нормально, морозы, вот опухла и', 'кто вчера на документы и оставил дома это', 'около 10 машин было! А вы тут тьфу на вас!', 'Но потом вспомнила, что учусь в', 'это первый на моей памяти год, когда весь пиздец и сразу', 'почему-то у меня твитты других не появляются на', '@iiiwant1D Надо проверить почки и', 'RT Совсем нет предновогоднего настроения:(', 'RT', '@GaGa_oo_lala так все же за инглиш чтож ты так? а 2 часть над которой ты сидела все то время тоже', 'я останусь как и ты', 'У моей подруге столько неё было 10 парней и у неё 10 двоек в типо такая', 'Всё же 2 симки это бы на деньги', 'Мы ж все уже у тебя', 'тебя ненавижу! Знай это! Из за тебя мы мое', 'ещё и логик не пришёл до сих пор(', '@mon_liz1 Кошмар какой:( И ещё,', 'RT НЕ НА МЕНЯ', 'читаю историю в аське с всё было хорошо =(.', 'силы, давай мои седины!', '@migel_dost1986 да! А вот будущее моих детей, а я считаю своим долгом защищать их. Жаль,', 'Становится гугл на запрос информации по теме диплома одной из первых ссылок выдает твою же', 'устроили перед самым НГ(( RT @Alex7236 @58Vovan @ue72 ппц какой-то...', '@Lubalubochka2 щас еще в командировку', 'сейчас хожу с телефоном пока он на турнире свой', 'купили не то платье, что я хочу очень, но тоже Но тоо платье вообще но оно маловато :(((((((', 'И как бы нибыло мне больно,но я забуду о тебе=(((((', 'голос болею за Наргиз и самые', 'бл кто нибудь меня', 'RT #Снаступающимтвиттерский @dellins_NaNaNa @juliabartsler', 'RT Очень скучаю по своим хочу побыстрее домой.', 'Первый рабочее утро после 10 дней', 'у меня открывается, но иногда нет:(', '150 параграфов на', 'по утрам в магаз ходят бабушки со скидками и им впаривают все', 'RT @kota_Oo_oO: собираюсь в шк :с', 'завтра Ужас, дз еще делать,', 'ну она типа моя жена, а она даже не', 'Какая-то радует только то, что почти половина зимы позади. Хотя судя по погоде, зима будет в', 'Зато дал конфету ... из своего рта ._. мне в руку(', 'до НГ 16 или там дней,а настроение не']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Jdem-6DsVes"
      },
      "source": [
        "import gensim\n",
        "texts = all_tweets_data.text.apply(preprocess).tolist()\n",
        "w2v = gensim.models.Word2Vec(texts, size=100, window=5, min_count=1)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCoxdbQ7FhKY"
      },
      "source": [
        "weights = np.zeros((len(word2id), 100))\n",
        "count = 0\n",
        "for word, i in word2id.items():\n",
        "    if word == 'PAD':\n",
        "        continue   \n",
        "    try:\n",
        "        weights[i] = w2v.wv[word]    \n",
        "    except KeyError:\n",
        "      count += 1\n",
        "      # oov словам сопоставляем случайный вектор\n",
        "      weights[i] = np.random.normal(0,0.1,100)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVjmQw1tFxjr"
      },
      "source": [
        "class CNN_w2v(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.from_pretrained(torch.tensor(weights), freeze=True)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.conv =  nn.Conv1d(in_channels=180, out_channels=50, kernel_size=2, padding='same')\n",
        "        self.hidden = nn.Linear(in_features=50, out_features=1)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word):\n",
        "        embedded = self.embedding(word)\n",
        "        embedded = embedded.transpose(1,2)\n",
        "        feature_map_bigrams = self.bigrams(embedded)\n",
        "        feature_map_trigrams = self.trigrams(embedded)\n",
        "        concat = self.conv(torch.cat((feature_map_bigrams, feature_map_trigrams), 1))\n",
        "        pooling = concat.max(2)[0] \n",
        "        logits = self.hidden(pooling) \n",
        "        logits = self.out(logits)      \n",
        "        return logits"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iceORPe_GCsy"
      },
      "source": [
        "model_w2v = CNN_w2v(len(word2id), 8)\n",
        "optimizer = optim.Adam(model_w2v.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "# веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "model_w2v = model_w2v.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRtW4Io0GSiK",
        "outputId": "dc36ac13-7e10-4d58-820c-47572fadbd5a"
      },
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(20):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model_w2v, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model_w2v, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model_w2v, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.7262892462313175\n",
            "Train loss: 0.6944870840419423\n",
            "Train loss: 0.6816103887557984\n",
            "Train loss: 0.6714405550885556\n",
            "Train loss: 0.6644368150404522\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6701085120439529, Val f1: 0.6710220575332642\n",
            "Val loss: 0.6491085868893247, Val f1: 0.6545130610466003\n",
            "Val loss: 0.6423030161857605, Val f1: 0.6481454372406006\n",
            "Val loss: 0.6397659787491187, Val f1: 0.6439507603645325\n",
            "Val loss: 0.6372197589703968, Val f1: 0.6433106064796448\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2680591940879822, Val f1: 1.2662785053253174\n",
            "Val loss: 0.8451773126920065, Val f1: 0.8381778597831726\n",
            "Val loss: 0.7566016793251038, Val f1: 0.7652910351753235\n",
            "Val loss: 0.7205219779695783, Val f1: 0.7321568131446838\n",
            "Val loss: 0.7015093631214566, Val f1: 0.7106819152832031\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.6677677929401398\n",
            "Train loss: 0.6451104789069204\n",
            "Train loss: 0.6347089183330535\n",
            "Train loss: 0.6289643022551465\n",
            "Train loss: 0.6235407236076537\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.638067901134491, Val f1: 0.7230409979820251\n",
            "Val loss: 0.6171305233781988, Val f1: 0.700742781162262\n",
            "Val loss: 0.6106254136562348, Val f1: 0.692875325679779\n",
            "Val loss: 0.607845413151072, Val f1: 0.6880612969398499\n",
            "Val loss: 0.6055310134376798, Val f1: 0.6873687505722046\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2125478386878967, Val f1: 1.362634539604187\n",
            "Val loss: 0.8078488111495972, Val f1: 0.8939882516860962\n",
            "Val loss: 0.7207059025764465, Val f1: 0.811259925365448\n",
            "Val loss: 0.68642543894904, Val f1: 0.7735780477523804\n",
            "Val loss: 0.6690160499678718, Val f1: 0.751533567905426\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.6355651095509529\n",
            "Train loss: 0.6107595021074469\n",
            "Train loss: 0.6015624773502349\n",
            "Train loss: 0.5969797070346662\n",
            "Train loss: 0.5942753425666264\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6061893068253994, Val f1: 0.7461525797843933\n",
            "Val loss: 0.5894914811307733, Val f1: 0.7209315299987793\n",
            "Val loss: 0.5853540897369385, Val f1: 0.7125234603881836\n",
            "Val loss: 0.58234660305194, Val f1: 0.7077835202217102\n",
            "Val loss: 0.5803358384541103, Val f1: 0.706159770488739\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1708428859710693, Val f1: 1.4062941074371338\n",
            "Val loss: 0.77985946337382, Val f1: 0.9243274927139282\n",
            "Val loss: 0.6940188050270081, Val f1: 0.8378661274909973\n",
            "Val loss: 0.6611031549317496, Val f1: 0.8003022074699402\n",
            "Val loss: 0.6448230412271287, Val f1: 0.775424063205719\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.609694354236126\n",
            "Train loss: 0.586911495887872\n",
            "Train loss: 0.579314523935318\n",
            "Train loss: 0.5749178190729511\n",
            "Train loss: 0.572587373710814\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.588589072227478, Val f1: 0.7546182870864868\n",
            "Val loss: 0.5706355734304949, Val f1: 0.7350015640258789\n",
            "Val loss: 0.5640107357501983, Val f1: 0.7280492186546326\n",
            "Val loss: 0.561586661125297, Val f1: 0.7234405279159546\n",
            "Val loss: 0.5598173893633343, Val f1: 0.7215679883956909\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.135413944721222, Val f1: 1.4310228824615479\n",
            "Val loss: 0.7567333976427714, Val f1: 0.9458762407302856\n",
            "Val loss: 0.6726045966148376, Val f1: 0.8530742526054382\n",
            "Val loss: 0.6414293902260917, Val f1: 0.8156643509864807\n",
            "Val loss: 0.6261036064889696, Val f1: 0.7912980318069458\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.5875633358955383\n",
            "Train loss: 0.5683157498186285\n",
            "Train loss: 0.55923210978508\n",
            "Train loss: 0.5552683561595518\n",
            "Train loss: 0.5518232101485843\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5669189840555191, Val f1: 0.7591985464096069\n",
            "Val loss: 0.5479073398041002, Val f1: 0.7390702962875366\n",
            "Val loss: 0.5431145310401917, Val f1: 0.7304551005363464\n",
            "Val loss: 0.5408388064868415, Val f1: 0.7274389863014221\n",
            "Val loss: 0.5389844725529352, Val f1: 0.7250036001205444\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1024152040481567, Val f1: 1.4286932945251465\n",
            "Val loss: 0.732817014058431, Val f1: 0.9396786689758301\n",
            "Val loss: 0.6503818154335022, Val f1: 0.8510898947715759\n",
            "Val loss: 0.621517368725368, Val f1: 0.8119311928749084\n",
            "Val loss: 0.6064899762471517, Val f1: 0.7887276411056519\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.5662223175168037\n",
            "Train loss: 0.5486916520378806\n",
            "Train loss: 0.5419741201400757\n",
            "Train loss: 0.537571544077859\n",
            "Train loss: 0.5343827764902797\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5427811592817307, Val f1: 0.7822031378746033\n",
            "Val loss: 0.5290135723171812, Val f1: 0.755878746509552\n",
            "Val loss: 0.5245167124271393, Val f1: 0.7473134994506836\n",
            "Val loss: 0.52359207041228, Val f1: 0.7428321838378906\n",
            "Val loss: 0.522129892948128, Val f1: 0.7404393553733826\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0772418975830078, Val f1: 1.4458611011505127\n",
            "Val loss: 0.7165450851122538, Val f1: 0.9532517194747925\n",
            "Val loss: 0.6353725671768189, Val f1: 0.8627893328666687\n",
            "Val loss: 0.6068603566714695, Val f1: 0.82508385181427\n",
            "Val loss: 0.5923830999268426, Val f1: 0.8017555475234985\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.5459360666573048\n",
            "Train loss: 0.5305331204876755\n",
            "Train loss: 0.5243905079364777\n",
            "Train loss: 0.5203477117552686\n",
            "Train loss: 0.5183393611084848\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5298587810248137, Val f1: 0.7891138792037964\n",
            "Val loss: 0.5177426699436072, Val f1: 0.7617895007133484\n",
            "Val loss: 0.5110956376791, Val f1: 0.754973828792572\n",
            "Val loss: 0.5081602686376714, Val f1: 0.7510448694229126\n",
            "Val loss: 0.5056561458678472, Val f1: 0.7502448558807373\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0503364205360413, Val f1: 1.4671149253845215\n",
            "Val loss: 0.7006575067838033, Val f1: 0.9630166888237\n",
            "Val loss: 0.621449065208435, Val f1: 0.8708528876304626\n",
            "Val loss: 0.5934917415891375, Val f1: 0.8339807391166687\n",
            "Val loss: 0.5792067382070754, Val f1: 0.8088535666465759\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.5328811127692461\n",
            "Train loss: 0.5127910240129991\n",
            "Train loss: 0.5088555318117142\n",
            "Train loss: 0.5041729518726691\n",
            "Train loss: 0.5026090570858547\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5114774368703365, Val f1: 0.7897279858589172\n",
            "Val loss: 0.4996008863954833, Val f1: 0.7631630301475525\n",
            "Val loss: 0.49432314217090606, Val f1: 0.7551746964454651\n",
            "Val loss: 0.49153728849852263, Val f1: 0.7520599961280823\n",
            "Val loss: 0.49077481740996953, Val f1: 0.7494370937347412\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.027212381362915, Val f1: 1.4452509880065918\n",
            "Val loss: 0.6848676204681396, Val f1: 0.9555538892745972\n",
            "Val loss: 0.6068539679050445, Val f1: 0.8653419613838196\n",
            "Val loss: 0.5802918033940452, Val f1: 0.8275720477104187\n",
            "Val loss: 0.565893871916665, Val f1: 0.8043627738952637\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.5138305183500051\n",
            "Train loss: 0.4975441874879779\n",
            "Train loss: 0.4912050956487656\n",
            "Train loss: 0.48905423283576965\n",
            "Train loss: 0.4868190036643119\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4959039781242609, Val f1: 0.7976346015930176\n",
            "Val loss: 0.48432529062935803, Val f1: 0.7751337289810181\n",
            "Val loss: 0.4798771530389786, Val f1: 0.7679075598716736\n",
            "Val loss: 0.47794991286832894, Val f1: 0.7642176151275635\n",
            "Val loss: 0.4764892803061576, Val f1: 0.76220703125\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.01115882396698, Val f1: 1.479195237159729\n",
            "Val loss: 0.6732254425684611, Val f1: 0.9703923463821411\n",
            "Val loss: 0.5956789970397949, Val f1: 0.8770594000816345\n",
            "Val loss: 0.5694780775478908, Val f1: 0.8393145203590393\n",
            "Val loss: 0.554934905634986, Val f1: 0.8160155415534973\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.5013477317988873\n",
            "Train loss: 0.4855583608150482\n",
            "Train loss: 0.47975426495075224\n",
            "Train loss: 0.4759958132879058\n",
            "Train loss: 0.4745345896198636\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4871095959097147, Val f1: 0.8280404806137085\n",
            "Val loss: 0.47536149169459485, Val f1: 0.7994526624679565\n",
            "Val loss: 0.4722663152217865, Val f1: 0.7899665832519531\n",
            "Val loss: 0.4700887567961394, Val f1: 0.7852085828781128\n",
            "Val loss: 0.4683265654104097, Val f1: 0.7833385467529297\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0005305409431458, Val f1: 1.5050902366638184\n",
            "Val loss: 0.6696202953656515, Val f1: 0.9880775213241577\n",
            "Val loss: 0.5924128293991089, Val f1: 0.8963930010795593\n",
            "Val loss: 0.5659934793199811, Val f1: 0.8584853410720825\n",
            "Val loss: 0.5524012280835046, Val f1: 0.8354499936103821\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.48678333312273026\n",
            "Train loss: 0.4722976404609102\n",
            "Train loss: 0.46700640439987184\n",
            "Train loss: 0.4623747766017914\n",
            "Train loss: 0.4604516951810746\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.47079151682555676, Val f1: 0.8227553963661194\n",
            "Val loss: 0.45555097406560724, Val f1: 0.8004425168037415\n",
            "Val loss: 0.45445357739925385, Val f1: 0.7918505072593689\n",
            "Val loss: 0.4523481117255652, Val f1: 0.787214457988739\n",
            "Val loss: 0.4506071067991711, Val f1: 0.7848605513572693\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9775999188423157, Val f1: 1.4932806491851807\n",
            "Val loss: 0.6535159150759379, Val f1: 0.9826878309249878\n",
            "Val loss: 0.5779662966728211, Val f1: 0.8902890086174011\n",
            "Val loss: 0.5530665516853333, Val f1: 0.852819561958313\n",
            "Val loss: 0.5388485226366255, Val f1: 0.8292658925056458\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.4769942872226238\n",
            "Train loss: 0.45867036779721576\n",
            "Train loss: 0.4531147062778473\n",
            "Train loss: 0.450368471110045\n",
            "Train loss: 0.44881620683840345\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4615402799099684, Val f1: 0.8394683003425598\n",
            "Val loss: 0.4491666952768962, Val f1: 0.8106961846351624\n",
            "Val loss: 0.44524812161922456, Val f1: 0.8032194375991821\n",
            "Val loss: 0.4424295968084193, Val f1: 0.7988304495811462\n",
            "Val loss: 0.442017745758806, Val f1: 0.7960222363471985\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9672037661075592, Val f1: 1.5127118825912476\n",
            "Val loss: 0.6479773819446564, Val f1: 0.9952248930931091\n",
            "Val loss: 0.5729806602001191, Val f1: 0.9028531312942505\n",
            "Val loss: 0.5480757568563733, Val f1: 0.862968921661377\n",
            "Val loss: 0.5339902804957496, Val f1: 0.8400464653968811\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.45928404852747917\n",
            "Train loss: 0.4464223366795164\n",
            "Train loss: 0.4405238342285156\n",
            "Train loss: 0.43833811692337493\n",
            "Train loss: 0.437309035942668\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4491398837417364, Val f1: 0.8266380429267883\n",
            "Val loss: 0.4343467147061319, Val f1: 0.8053289651870728\n",
            "Val loss: 0.42919663310050965, Val f1: 0.7991993427276611\n",
            "Val loss: 0.4283376496229599, Val f1: 0.7951816916465759\n",
            "Val loss: 0.4273438382716406, Val f1: 0.7928075194358826\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9462182521820068, Val f1: 1.4988608360290527\n",
            "Val loss: 0.6336414714654287, Val f1: 0.9890115261077881\n",
            "Val loss: 0.5603941798210144, Val f1: 0.894109845161438\n",
            "Val loss: 0.5363702859197345, Val f1: 0.853911817073822\n",
            "Val loss: 0.5225131577915616, Val f1: 0.8301410675048828\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.45534250885248184\n",
            "Train loss: 0.4399278904452468\n",
            "Train loss: 0.43327695190906523\n",
            "Train loss: 0.42934733807151\n",
            "Train loss: 0.4275728467674482\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4361568056046963, Val f1: 0.8393844366073608\n",
            "Val loss: 0.4239975602337808, Val f1: 0.8140198588371277\n",
            "Val loss: 0.42131237864494325, Val f1: 0.8040151000022888\n",
            "Val loss: 0.41923223991892233, Val f1: 0.7995238900184631\n",
            "Val loss: 0.41714654791922795, Val f1: 0.7981268763542175\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9356111884117126, Val f1: 1.4932825565338135\n",
            "Val loss: 0.6252087354660034, Val f1: 0.9887031316757202\n",
            "Val loss: 0.5530670642852783, Val f1: 0.8974119424819946\n",
            "Val loss: 0.5297698548861912, Val f1: 0.8572657704353333\n",
            "Val loss: 0.5158608886930678, Val f1: 0.8342178463935852\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.43937989324331284\n",
            "Train loss: 0.4227079279495008\n",
            "Train loss: 0.4183591431379318\n",
            "Train loss: 0.4170183001169518\n",
            "Train loss: 0.41602607248794465\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4308586046099663, Val f1: 0.8523423075675964\n",
            "Val loss: 0.4193215234713121, Val f1: 0.8264300227165222\n",
            "Val loss: 0.4134957545995712, Val f1: 0.8191354274749756\n",
            "Val loss: 0.41080927137118667, Val f1: 0.8152850866317749\n",
            "Val loss: 0.4087009160291581, Val f1: 0.8126636147499084\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9258688390254974, Val f1: 1.5270941257476807\n",
            "Val loss: 0.6203753550847372, Val f1: 1.006325125694275\n",
            "Val loss: 0.5493736505508423, Val f1: 0.910843551158905\n",
            "Val loss: 0.5255642022405352, Val f1: 0.8706439733505249\n",
            "Val loss: 0.5118205447991689, Val f1: 0.8470684885978699\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.42617687582969666\n",
            "Train loss: 0.4138115933447173\n",
            "Train loss: 0.41036578714847566\n",
            "Train loss: 0.40893332815881983\n",
            "Train loss: 0.40758984216621946\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.41993483901023865, Val f1: 0.8492177128791809\n",
            "Val loss: 0.4080031956687118, Val f1: 0.8227443099021912\n",
            "Val loss: 0.40226090788841246, Val f1: 0.8178150653839111\n",
            "Val loss: 0.4002858369208094, Val f1: 0.8146209120750427\n",
            "Val loss: 0.39815102198294233, Val f1: 0.8129869103431702\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9122858941555023, Val f1: 1.5151958465576172\n",
            "Val loss: 0.6104540228843689, Val f1: 1.003479242324829\n",
            "Val loss: 0.5407334804534912, Val f1: 0.909964382648468\n",
            "Val loss: 0.5174304161752973, Val f1: 0.8702293634414673\n",
            "Val loss: 0.503513428899977, Val f1: 0.8465439081192017\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.4223492257297039\n",
            "Train loss: 0.40678921883756464\n",
            "Train loss: 0.4033156645298004\n",
            "Train loss: 0.398829941429309\n",
            "Train loss: 0.3979111725375766\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.40349024534225464, Val f1: 0.8618490695953369\n",
            "Val loss: 0.39471129215124884, Val f1: 0.8323507905006409\n",
            "Val loss: 0.39119155824184415, Val f1: 0.822975754737854\n",
            "Val loss: 0.389426240280493, Val f1: 0.8194161057472229\n",
            "Val loss: 0.3889593354293278, Val f1: 0.8164219856262207\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9067835509777069, Val f1: 1.5119112730026245\n",
            "Val loss: 0.6065638065338135, Val f1: 1.001447319984436\n",
            "Val loss: 0.536937153339386, Val f1: 0.9093391299247742\n",
            "Val loss: 0.5140789747238159, Val f1: 0.8688923716545105\n",
            "Val loss: 0.5002806848949857, Val f1: 0.845670759677887\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.408459248021245\n",
            "Train loss: 0.39760380351182184\n",
            "Train loss: 0.3942497819662094\n",
            "Train loss: 0.3917973775472214\n",
            "Train loss: 0.3887651136943272\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3943030033260584, Val f1: 0.8673489093780518\n",
            "Val loss: 0.3807743561990333, Val f1: 0.8426408171653748\n",
            "Val loss: 0.3806215703487396, Val f1: 0.8317106366157532\n",
            "Val loss: 0.3793189925933952, Val f1: 0.8270828723907471\n",
            "Val loss: 0.38008115759917666, Val f1: 0.8233615756034851\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8964838981628418, Val f1: 1.533752679824829\n",
            "Val loss: 0.6011791527271271, Val f1: 1.0113236904144287\n",
            "Val loss: 0.5322044670581818, Val f1: 0.9180039763450623\n",
            "Val loss: 0.5096614403384072, Val f1: 0.8767806887626648\n",
            "Val loss: 0.49589727653397453, Val f1: 0.8519619703292847\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.39973936043679714\n",
            "Train loss: 0.3888192998640465\n",
            "Train loss: 0.38305640816688535\n",
            "Train loss: 0.3798636500515155\n",
            "Train loss: 0.3803063513977187\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3977469354867935, Val f1: 0.8655593395233154\n",
            "Val loss: 0.38045528982624865, Val f1: 0.8423672318458557\n",
            "Val loss: 0.376228803396225, Val f1: 0.8320685625076294\n",
            "Val loss: 0.37297031461302915, Val f1: 0.8276882171630859\n",
            "Val loss: 0.37268121256714776, Val f1: 0.8249356746673584\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8887610137462616, Val f1: 1.5280380249023438\n",
            "Val loss: 0.5965776443481445, Val f1: 1.010333776473999\n",
            "Val loss: 0.5286925077438355, Val f1: 0.9164850115776062\n",
            "Val loss: 0.5067256305898938, Val f1: 0.874884307384491\n",
            "Val loss: 0.49269218577278984, Val f1: 0.8512359261512756\n",
            "\n",
            "starting Epoch 19\n",
            "Training...\n",
            "Train loss: 0.3934387303888798\n",
            "Train loss: 0.3815077416824572\n",
            "Train loss: 0.375495445728302\n",
            "Train loss: 0.37325426980630677\n",
            "Train loss: 0.372136055004029\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.38417876698076725, Val f1: 0.874218225479126\n",
            "Val loss: 0.3692006568113963, Val f1: 0.8487111926078796\n",
            "Val loss: 0.36744830310344695, Val f1: 0.8386936783790588\n",
            "Val loss: 0.36513408395781444, Val f1: 0.8353772163391113\n",
            "Val loss: 0.36412851476953145, Val f1: 0.833220362663269\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8837321698665619, Val f1: 1.5415208339691162\n",
            "Val loss: 0.5928227603435516, Val f1: 1.0144150257110596\n",
            "Val loss: 0.5255183100700378, Val f1: 0.92030268907547\n",
            "Val loss: 0.5034965361867633, Val f1: 0.8794419765472412\n",
            "Val loss: 0.48913540111647713, Val f1: 0.8553900718688965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8rq22npGc59"
      },
      "source": [
        "fp, fn, tp, tn, accuracy, precision, recall = predict(model_w2v, val_iterator)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJCW6vv2GsJo",
        "outputId": "25713362-74a7-4703-80b2-e39bfb8b0630"
      },
      "source": [
        "print('accuracy:', accuracy)\n",
        "print('precision:', precision)\n",
        "print('recall:', recall)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.7652\n",
            "precision: 0.7675464320625611\n",
            "recall: 0.7720747295968535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYNKIcTUI8Gv"
      },
      "source": [
        "class TweetsDataset_2(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, word2id, symbol2id, DEVICE):\n",
        "        self.dataset = dataset['text'].values\n",
        "        self.word2id = word2id\n",
        "        self.symbol2id = symbol2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = dataset['tone'].values\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index): \n",
        "        words = self.dataset[index].split()\n",
        "        id_words = torch.LongTensor([self.word2id[word] for word in words if word in self.word2id])\n",
        "        symbols = list(self.dataset[index])\n",
        "        id_symbols= torch.LongTensor([self.symbol2id[symbol] for symbol in symbols if symbol in self.symbol2id])\n",
        "        y = [self.target[index]]\n",
        "        return id_words, id_symbols, y\n",
        "\n",
        "    def collate_fn(self, batch): #этот метод можно реализовывать и отдельно,\n",
        "    # он понадобится для DataLoader во время итерации по батчам\n",
        "      id_words, id_symbols, y = list(zip(*batch))\n",
        "      padded_id_words = pad_sequence(id_words, batch_first=True).to(self.device)\n",
        "      padded_id_symbols = pad_sequence(id_symbols, batch_first=True).to(self.device)\n",
        "      y = torch.Tensor(y).to(self.device)\n",
        "      return padded_id_words, padded_id_symbols, y"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEHbzz1UeTLf"
      },
      "source": [
        "val_dataset = TweetsDataset_2(val_sentences, word2id, symbol2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ03qiQLjVWJ"
      },
      "source": [
        "train_dataset = TweetsDataset_2(train_sentences, word2id, symbol2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tULbOLqOjlUv",
        "outputId": "4510c00e-26cf-44ef-b37f-b50f11fc5546"
      },
      "source": [
        "train_iterator"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7fc67e23f990>"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yhk_wAqieTdn",
        "outputId": "ef1b1b62-338d-4b2e-99a2-b7137c502bbe"
      },
      "source": [
        "batch = next(iter(train_iterator))\n",
        "batch[1].shape"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1024, 140])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsUfTf-imDDJ"
      },
      "source": [
        "class CNN_2(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size_word, vocab_size_symbol, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding_word = nn.Embedding(vocab_size_word, embedding_dim)\n",
        "        self.embedding_word.from_pretrained(torch.tensor(weights), freeze=True)\n",
        "        self.embedding_symbol = nn.Embedding(vocab_size_symbol, embedding_dim)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.hidden = nn.Linear(in_features=180, out_features=1)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, text, word):\n",
        "        embedded_words = self.embedding_word(text)\n",
        "        embedded_symbols = self.embedding_symbol(word)\n",
        "        embedded_words = embedded_words.transpose(1,2)\n",
        "        embedded_symbols = embedded_symbols.transpose(1,2)\n",
        "        feature_map_bigrams = self.pooling(self.bigrams(embedded_symbols)).max(2)[0] \n",
        "        feature_map_trigrams = self.pooling(self.trigrams(embedded_symbols)).max(2)[0] \n",
        "        concat = torch.cat((feature_map_bigrams, feature_map_trigrams), 1)\n",
        "        logits = self.hidden(concat) \n",
        "        logits = self.out(logits)      \n",
        "        return logits"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Npm11-uMzz5r"
      },
      "source": [
        "model_2 = CNN_2(len(word2id), len(symbol2id), 8)\n",
        "optimizer = optim.Adam(model_2.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "model_2 = model_2.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfEuspLz1MZt"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, print_v=True):\n",
        "    epoch_loss = 0 # для подсчета среднего лосса на всех батчах\n",
        "\n",
        "    model.train()  # ставим модель в обучение, явно указываем, что сейчас надо будет хранить градиенты у всех весов\n",
        "    for i, (texts, words, ys) in enumerate(iterator): #итерируемся по батчам\n",
        "        optimizer.zero_grad()  #обнуляем градиенты\n",
        "        preds = model(texts, words)  #прогоняем данные через модель\n",
        "        loss = criterion(preds, ys) #считаем значение функции потерь  \n",
        "        loss.backward() #считаем градиенты  \n",
        "        optimizer.step() #обновляем веса \n",
        "        epoch_loss += loss.item() #сохраняем значение функции потерь\n",
        "        if print_v==True:\n",
        "          if not (i + 1) % int(len(iterator)/5):\n",
        "              print(f'Train loss: {epoch_loss/i}')      \n",
        "    return  epoch_loss / len(iterator) # возвращаем среднее значение лосса по всей выборке"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfgTwhqg1Pl5"
      },
      "source": [
        "def evaluate(model, iterator, criterion, print_v=True):\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for i, (texts, words, ys) in enumerate(iterator):   \n",
        "            preds = model(texts, words)  # делаем предсказания на тесте\n",
        "            loss = criterion(preds, ys)   # считаем значения функции ошибки для статистики  \n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = f1(preds.round().long(), ys.long(), ignore_index=0)\n",
        "            epoch_metric += batch_metric\n",
        "            if print_v==True:\n",
        "              if not (i + 1) % int(len(iterator)/5):\n",
        "                print(f'Val loss: {epoch_loss/i}, Val f1: {epoch_metric/i}')\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator) # возвращаем среднее значение по всей выборке"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OjJqAKw0tec",
        "outputId": "b7ddaaf7-fb1d-436c-c79d-79e7bc8fbe81"
      },
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(20):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model_2, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model_2, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model_2, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.7295184694230556\n",
            "Train loss: 0.7032984874465249\n",
            "Train loss: 0.6924043822288514\n",
            "Train loss: 0.685340194559809\n",
            "Train loss: 0.6794215532995406\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6904056183993816, Val f1: 0.6504154205322266\n",
            "Val loss: 0.66978919506073, Val f1: 0.631404459476471\n",
            "Val loss: 0.6627586233615875, Val f1: 0.6246640086174011\n",
            "Val loss: 0.6601835907395206, Val f1: 0.6192572712898254\n",
            "Val loss: 0.6582370840367817, Val f1: 0.6172813177108765\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.3064852356910706, Val f1: 1.2085943222045898\n",
            "Val loss: 0.8659918506940206, Val f1: 0.8139581084251404\n",
            "Val loss: 0.7816905617713928, Val f1: 0.7229977250099182\n",
            "Val loss: 0.7452020389693124, Val f1: 0.6895936131477356\n",
            "Val loss: 0.7248063219918145, Val f1: 0.6661089062690735\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.686821598559618\n",
            "Train loss: 0.6605262756347656\n",
            "Train loss: 0.6460525286197663\n",
            "Train loss: 0.6342212086293235\n",
            "Train loss: 0.6221410206386021\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5891841761767864, Val f1: 0.7978318929672241\n",
            "Val loss: 0.5722544825438297, Val f1: 0.7731007933616638\n",
            "Val loss: 0.5671158075332642, Val f1: 0.76516193151474\n",
            "Val loss: 0.5644742632979778, Val f1: 0.7611958980560303\n",
            "Val loss: 0.561973688857896, Val f1: 0.760004997253418\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.113964021205902, Val f1: 1.4886479377746582\n",
            "Val loss: 0.7376829783121744, Val f1: 0.9911934733390808\n",
            "Val loss: 0.6670995831489563, Val f1: 0.888279914855957\n",
            "Val loss: 0.6371851563453674, Val f1: 0.8446819186210632\n",
            "Val loss: 0.6197388238377042, Val f1: 0.8208158016204834\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.5714389979839325\n",
            "Train loss: 0.5329766309622562\n",
            "Train loss: 0.5062585520744324\n",
            "Train loss: 0.4814601656216294\n",
            "Train loss: 0.4582711688819386\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3524587210267782, Val f1: 0.9839145541191101\n",
            "Val loss: 0.3457822302977244, Val f1: 0.951039731502533\n",
            "Val loss: 0.3421400314569473, Val f1: 0.9411799907684326\n",
            "Val loss: 0.3397503103782882, Val f1: 0.9382048845291138\n",
            "Val loss: 0.3382902280205772, Val f1: 0.9358137249946594\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6584469974040985, Val f1: 1.833786964416504\n",
            "Val loss: 0.44207467635472614, Val f1: 1.2234784364700317\n",
            "Val loss: 0.398460727930069, Val f1: 1.1021450757980347\n",
            "Val loss: 0.38237228989601135, Val f1: 1.0499391555786133\n",
            "Val loss: 0.37236060036553276, Val f1: 1.0212820768356323\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.3370618149638176\n",
            "Train loss: 0.3045015344114015\n",
            "Train loss: 0.28302318960428235\n",
            "Train loss: 0.2647611040677597\n",
            "Train loss: 0.24831683153197878\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.1798287322744727, Val f1: 1.028615117073059\n",
            "Val loss: 0.1734901756951303, Val f1: 0.9979684352874756\n",
            "Val loss: 0.17266822159290313, Val f1: 0.9871891736984253\n",
            "Val loss: 0.17179451660433812, Val f1: 0.9821838736534119\n",
            "Val loss: 0.17127436505896704, Val f1: 0.9793027639389038\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.32786640524864197, Val f1: 1.9431205987930298\n",
            "Val loss: 0.22382223109404245, Val f1: 1.2880144119262695\n",
            "Val loss: 0.20060277879238128, Val f1: 1.1583107709884644\n",
            "Val loss: 0.19315703213214874, Val f1: 1.102546215057373\n",
            "Val loss: 0.18938502503765953, Val f1: 1.0720027685165405\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.16814308799803257\n",
            "Train loss: 0.1543061841617931\n",
            "Train loss: 0.14590182438492774\n",
            "Train loss: 0.1396091703825922\n",
            "Train loss: 0.13384750893428213\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.11145940003916621, Val f1: 1.0348870754241943\n",
            "Val loss: 0.1068013994531198, Val f1: 1.003796100616455\n",
            "Val loss: 0.10546008989214897, Val f1: 0.9943196177482605\n",
            "Val loss: 0.10473322445776925, Val f1: 0.9895827770233154\n",
            "Val loss: 0.10442082708080609, Val f1: 0.9867261648178101\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.20131617039442062, Val f1: 1.9533159732818604\n",
            "Val loss: 0.1385019595424334, Val f1: 1.2987827062606812\n",
            "Val loss: 0.12379416972398757, Val f1: 1.169287085533142\n",
            "Val loss: 0.11896022196326937, Val f1: 1.113731861114502\n",
            "Val loss: 0.1173604999979337, Val f1: 1.0818883180618286\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.10729827638715506\n",
            "Train loss: 0.10104914009571075\n",
            "Train loss: 0.0958551949262619\n",
            "Train loss: 0.0920695078684323\n",
            "Train loss: 0.08913422846013591\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0786855025216937, Val f1: 1.0387957096099854\n",
            "Val loss: 0.07652928228631165, Val f1: 1.0068131685256958\n",
            "Val loss: 0.07515998922288418, Val f1: 0.9969480633735657\n",
            "Val loss: 0.0748577764563596, Val f1: 0.9917640089988708\n",
            "Val loss: 0.07407774758480844, Val f1: 0.9890748858451843\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.14667797833681107, Val f1: 1.9550862312316895\n",
            "Val loss: 0.09985820204019547, Val f1: 1.3009752035140991\n",
            "Val loss: 0.08889201283454895, Val f1: 1.1717466115951538\n",
            "Val loss: 0.08539066995893206, Val f1: 1.1161298751831055\n",
            "Val loss: 0.08422987163066864, Val f1: 1.0839909315109253\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.07603810587897897\n",
            "Train loss: 0.07180088634292285\n",
            "Train loss: 0.06859964273869991\n",
            "Train loss: 0.06588059724934066\n",
            "Train loss: 0.06396157604952653\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.05424542841501534, Val f1: 1.045032262802124\n",
            "Val loss: 0.05354256205486529, Val f1: 1.0129022598266602\n",
            "Val loss: 0.054422760158777235, Val f1: 1.0018738508224487\n",
            "Val loss: 0.054286787314201466, Val f1: 0.996955394744873\n",
            "Val loss: 0.053809309821753276, Val f1: 0.9942550659179688\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.11220354586839676, Val f1: 1.9633636474609375\n",
            "Val loss: 0.07381085430582364, Val f1: 1.3081648349761963\n",
            "Val loss: 0.06517234668135644, Val f1: 1.1777251958847046\n",
            "Val loss: 0.06261779527579035, Val f1: 1.1215465068817139\n",
            "Val loss: 0.06122818423642053, Val f1: 1.0894039869308472\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.05534885870292783\n",
            "Train loss: 0.05208178203214298\n",
            "Train loss: 0.04895660504698753\n",
            "Train loss: 0.04740058636265015\n",
            "Train loss: 0.046007161117380575\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0401173880090937, Val f1: 1.0496785640716553\n",
            "Val loss: 0.0394902095537294, Val f1: 1.0173594951629639\n",
            "Val loss: 0.03851334270089865, Val f1: 1.0075081586837769\n",
            "Val loss: 0.038014594521095506, Val f1: 1.0025148391723633\n",
            "Val loss: 0.03795253966624538, Val f1: 0.9995113015174866\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0841577984392643, Val f1: 1.970802903175354\n",
            "Val loss: 0.05361693104108175, Val f1: 1.3147742748260498\n",
            "Val loss: 0.04670181721448898, Val f1: 1.1839449405670166\n",
            "Val loss: 0.04488350397774151, Val f1: 1.127279281616211\n",
            "Val loss: 0.043574515316221446, Val f1: 1.0961602926254272\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.03782043431419879\n",
            "Train loss: 0.03529618116039218\n",
            "Train loss: 0.03441874455660582\n",
            "Train loss: 0.03301549574999667\n",
            "Train loss: 0.03199918060341761\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.028093977365642786, Val f1: 1.056048035621643\n",
            "Val loss: 0.026897891735037167, Val f1: 1.0242365598678589\n",
            "Val loss: 0.02646026946604252, Val f1: 1.0140753984451294\n",
            "Val loss: 0.026227179608905492, Val f1: 1.0089870691299438\n",
            "Val loss: 0.02612213861374628, Val f1: 1.0059734582901\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.060679005458950996, Val f1: 1.982011079788208\n",
            "Val loss: 0.03775089544554552, Val f1: 1.323061227798462\n",
            "Val loss: 0.032456083595752715, Val f1: 1.1919629573822021\n",
            "Val loss: 0.03125705490154879, Val f1: 1.1351147890090942\n",
            "Val loss: 0.030144270923402574, Val f1: 1.1037085056304932\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.026950385770760477\n",
            "Train loss: 0.02505588350874005\n",
            "Train loss: 0.023299574460834263\n",
            "Train loss: 0.022549700033642463\n",
            "Train loss: 0.021660238293753492\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.01805076259188354, Val f1: 1.0592331886291504\n",
            "Val loss: 0.01738973074790203, Val f1: 1.027207851409912\n",
            "Val loss: 0.017267718873918057, Val f1: 1.0168864727020264\n",
            "Val loss: 0.01705568108652065, Val f1: 1.0118962526321411\n",
            "Val loss: 0.017299914311262824, Val f1: 1.0086466073989868\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.041004080325365067, Val f1: 1.990443468093872\n",
            "Val loss: 0.025208634945253532, Val f1: 1.3284927606582642\n",
            "Val loss: 0.02158294040709734, Val f1: 1.1957707405090332\n",
            "Val loss: 0.020949979312717915, Val f1: 1.1386362314224243\n",
            "Val loss: 0.020237649997903243, Val f1: 1.107173204421997\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.017859432962723076\n",
            "Train loss: 0.016565155012137962\n",
            "Train loss: 0.015627019945532082\n",
            "Train loss: 0.014850427035186718\n",
            "Train loss: 0.014435240610813102\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.013149868987966329, Val f1: 1.060771107673645\n",
            "Val loss: 0.012065812973587803, Val f1: 1.0289057493209839\n",
            "Val loss: 0.011852815076708793, Val f1: 1.0186327695846558\n",
            "Val loss: 0.011819052412661154, Val f1: 1.0135204792022705\n",
            "Val loss: 0.011661284636440021, Val f1: 1.0105654001235962\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.028168894350528717, Val f1: 1.996185064315796\n",
            "Val loss: 0.01728506541500489, Val f1: 1.3310667276382446\n",
            "Val loss: 0.01462752539664507, Val f1: 1.1980684995651245\n",
            "Val loss: 0.014312784187495708, Val f1: 1.1408088207244873\n",
            "Val loss: 0.013773169885906909, Val f1: 1.1092427968978882\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.011505218222737312\n",
            "Train loss: 0.010966171369408115\n",
            "Train loss: 0.010605185059830546\n",
            "Train loss: 0.01020213749621119\n",
            "Train loss: 0.009920497508054333\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.008557006920455024, Val f1: 1.0617820024490356\n",
            "Val loss: 0.008195376743308523, Val f1: 1.029628038406372\n",
            "Val loss: 0.008194206319749354, Val f1: 1.0192643404006958\n",
            "Val loss: 0.008201038464903831, Val f1: 1.0142167806625366\n",
            "Val loss: 0.008149050597456239, Val f1: 1.011178970336914\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.019904766231775284, Val f1: 1.9980906248092651\n",
            "Val loss: 0.012242095544934273, Val f1: 1.332365870475769\n",
            "Val loss: 0.010293265059590339, Val f1: 1.198848009109497\n",
            "Val loss: 0.010193781528089727, Val f1: 1.1416314840316772\n",
            "Val loss: 0.009793549103455411, Val f1: 1.1098828315734863\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.008321142086060718\n",
            "Train loss: 0.007737263952466575\n",
            "Train loss: 0.007433483097702265\n",
            "Train loss: 0.007313294044292685\n",
            "Train loss: 0.007122044335119426\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.006518442300148308, Val f1: 1.0617711544036865\n",
            "Val loss: 0.006367469333450903, Val f1: 1.0297430753707886\n",
            "Val loss: 0.00618203766644001, Val f1: 1.0194318294525146\n",
            "Val loss: 0.0060374962666363855, Val f1: 1.01438570022583\n",
            "Val loss: 0.006036427620399211, Val f1: 1.0113948583602905\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.01443576905876398, Val f1: 1.9980906248092651\n",
            "Val loss: 0.008881470498939356, Val f1: 1.332365870475769\n",
            "Val loss: 0.007518929895013571, Val f1: 1.198848009109497\n",
            "Val loss: 0.007612973983798709, Val f1: 1.1416314840316772\n",
            "Val loss: 0.0073314325677023996, Val f1: 1.1100201606750488\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.006349208866595291\n",
            "Train loss: 0.006161281239297806\n",
            "Train loss: 0.005741932531818748\n",
            "Train loss: 0.0055566551650304405\n",
            "Train loss: 0.005344476437173961\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.004751462169224396, Val f1: 1.0620687007904053\n",
            "Val loss: 0.0046355987645008345, Val f1: 1.02991783618927\n",
            "Val loss: 0.004710053154267371, Val f1: 1.0195550918579102\n",
            "Val loss: 0.004688218538301872, Val f1: 1.0145220756530762\n",
            "Val loss: 0.004687542564213453, Val f1: 1.0114903450012207\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.011052209418267012, Val f1: 1.9990243911743164\n",
            "Val loss: 0.006803094409406185, Val f1: 1.3330081701278687\n",
            "Val loss: 0.005784034775570035, Val f1: 1.199418067932129\n",
            "Val loss: 0.0060128245303141216, Val f1: 1.1420387029647827\n",
            "Val loss: 0.0057909613889124655, Val f1: 1.1103368997573853\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.004820810645469464\n",
            "Train loss: 0.004411378628433202\n",
            "Train loss: 0.004255763166584075\n",
            "Train loss: 0.0041973627061207795\n",
            "Train loss: 0.004237629370653026\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.004086059794644825, Val f1: 1.0622031688690186\n",
            "Val loss: 0.003659132409445716, Val f1: 1.030074119567871\n",
            "Val loss: 0.003747797436080873, Val f1: 1.0197330713272095\n",
            "Val loss: 0.00377804301540131, Val f1: 1.0146377086639404\n",
            "Val loss: 0.0037410034786998516, Val f1: 1.0116060972213745\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.008804630488157272, Val f1: 1.9990243911743164\n",
            "Val loss: 0.005434348868827025, Val f1: 1.3330081701278687\n",
            "Val loss: 0.0046160129830241205, Val f1: 1.1996198892593384\n",
            "Val loss: 0.00490624803517546, Val f1: 1.1421828269958496\n",
            "Val loss: 0.004721986802501811, Val f1: 1.110448956489563\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.0032525404822081327\n",
            "Train loss: 0.003614454438458338\n",
            "Train loss: 0.003491766811348498\n",
            "Train loss: 0.0034919150083311903\n",
            "Train loss: 0.003428331992056753\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.003273467445978895, Val f1: 1.0623828172683716\n",
            "Val loss: 0.003419138032547904, Val f1: 1.0300124883651733\n",
            "Val loss: 0.003264041352085769, Val f1: 1.0196906328201294\n",
            "Val loss: 0.003189223851494269, Val f1: 1.0146503448486328\n",
            "Val loss: 0.003132914345423203, Val f1: 1.0116400718688965\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0073399420361965895, Val f1: 2.0\n",
            "Val loss: 0.004537874522308509, Val f1: 1.3330023288726807\n",
            "Val loss: 0.003822920424863696, Val f1: 1.1996164321899414\n",
            "Val loss: 0.0041480296890118295, Val f1: 1.142180323600769\n",
            "Val loss: 0.003973601825742258, Val f1: 1.1104470491409302\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.003533801864250563\n",
            "Train loss: 0.0031168801272570185\n",
            "Train loss: 0.0030185420252382754\n",
            "Train loss: 0.0028833151697667675\n",
            "Train loss: 0.002890546517890124\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0027411392802605405, Val f1: 1.0622613430023193\n",
            "Val loss: 0.0025675931035287, Val f1: 1.0301573276519775\n",
            "Val loss: 0.002598659805953503, Val f1: 1.0197913646697998\n",
            "Val loss: 0.002644993922561963, Val f1: 1.0146671533584595\n",
            "Val loss: 0.002658092788243223, Val f1: 1.011641025543213\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.006217858521267772, Val f1: 2.0\n",
            "Val loss: 0.0038384282185385623, Val f1: 1.3333333730697632\n",
            "Val loss: 0.0032335015945136546, Val f1: 1.199815034866333\n",
            "Val loss: 0.003592949826270342, Val f1: 1.1423221826553345\n",
            "Val loss: 0.003440762714793285, Val f1: 1.1105573177337646\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.002775247201498132\n",
            "Train loss: 0.002654259763375828\n",
            "Train loss: 0.002603432617615908\n",
            "Train loss: 0.00256693094565686\n",
            "Train loss: 0.0024895923700006236\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0024284134415211156, Val f1: 1.0622572898864746\n",
            "Val loss: 0.0022175889250568366, Val f1: 1.0301562547683716\n",
            "Val loss: 0.0022634219494648278, Val f1: 1.0198047161102295\n",
            "Val loss: 0.0022654443346797975, Val f1: 1.014736294746399\n",
            "Val loss: 0.002250002449034669, Val f1: 1.0117087364196777\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0054172552190721035, Val f1: 1.9990243911743164\n",
            "Val loss: 0.0033320123717809715, Val f1: 1.3330081701278687\n",
            "Val loss: 0.002796904440037906, Val f1: 1.1996198892593384\n",
            "Val loss: 0.0031317868768902762, Val f1: 1.1421828269958496\n",
            "Val loss: 0.003007096548875173, Val f1: 1.110448956489563\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.0024159204185707495\n",
            "Train loss: 0.002337688199159774\n",
            "Train loss: 0.0022097828309051693\n",
            "Train loss: 0.002202922367115519\n",
            "Train loss: 0.002159194014633873\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0021104305997141637, Val f1: 1.0623189210891724\n",
            "Val loss: 0.001998000090349127, Val f1: 1.030187726020813\n",
            "Val loss: 0.002003172056283802, Val f1: 1.019827127456665\n",
            "Val loss: 0.002017917174067515, Val f1: 1.0147244930267334\n",
            "Val loss: 0.0020248933911456595, Val f1: 1.0117212533950806\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.00469011371023953, Val f1: 1.9990243911743164\n",
            "Val loss: 0.002864792632559935, Val f1: 1.3330081701278687\n",
            "Val loss: 0.0024329112842679025, Val f1: 1.1996198892593384\n",
            "Val loss: 0.002819107679118003, Val f1: 1.1421828269958496\n",
            "Val loss: 0.002711063729495638, Val f1: 1.110448956489563\n",
            "\n",
            "starting Epoch 19\n",
            "Training...\n",
            "Train loss: 0.0022433944686781615\n",
            "Train loss: 0.0020280040440742264\n",
            "Train loss: 0.0020183761790394782\n",
            "Train loss: 0.0019358156905718038\n",
            "Train loss: 0.0019209340278480557\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0017736499212333001, Val f1: 1.0623176097869873\n",
            "Val loss: 0.0018659543358918393, Val f1: 1.0300978422164917\n",
            "Val loss: 0.001801703665405512, Val f1: 1.0198252201080322\n",
            "Val loss: 0.0017737118456053979, Val f1: 1.0147217512130737\n",
            "Val loss: 0.0017728245120018809, Val f1: 1.0117183923721313\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.00430847448296845, Val f1: 1.9990243911743164\n",
            "Val loss: 0.0026060602782915034, Val f1: 1.3330081701278687\n",
            "Val loss: 0.002183803590014577, Val f1: 1.1996198892593384\n",
            "Val loss: 0.002530842919700912, Val f1: 1.1421828269958496\n",
            "Val loss: 0.002423339795010785, Val f1: 1.110448956489563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFhL7obm5QXd"
      },
      "source": [
        "def predict(model, iterator):\n",
        "    model.eval()\n",
        "    fp = []\n",
        "    fn = []\n",
        "    tp = [] \n",
        "    tn = []\n",
        "    with torch.no_grad():\n",
        "        for i, (texts, words, ys) in enumerate(iterator):   \n",
        "            preds = model(texts, words)  # делаем предсказания на тесте \n",
        "            for pred, gold, text in zip(preds, ys, texts):\n",
        "              text = ' '.join([id2word[int(symbol)] for symbol in text if symbol !=0])\n",
        "              if round(pred.item()) > gold:\n",
        "                fp.append(text)\n",
        "              elif round(pred.item()) < gold:\n",
        "                fn.append(text)\n",
        "              elif round(pred.item()) == gold == 1:\n",
        "                tp.append(text)\n",
        "              elif round(pred.item()) == gold == 0:\n",
        "                tn.append(text)\n",
        "    accuracy = (len(tp)+len(tn))/(len(tp)+len(fp)+len(fn)+len(tn))\n",
        "    precision = len(tp)/(len(tp)+len(fp))\n",
        "    recall = len(tp)/(len(tp)+len(fn))\n",
        "    return fp, fn, tp, tn, accuracy, precision, recall"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "S71Iuc1N1ANh",
        "outputId": "b14a6825-6147-4c19-9d2c-2194b8f31fe8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(losses)\n",
        "plt.plot(losses_eval)\n",
        "plt.title('BCE loss value')\n",
        "plt.ylabel('BCE loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "YWyExFIh5Fy3",
        "outputId": "54b729c5-d308-4987-ef9e-2e998645a4b0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(f1s)\n",
        "plt.plot(f1s_eval)\n",
        "plt.title('f1 value')\n",
        "plt.ylabel('f1 value')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4YBPxc85HF6"
      },
      "source": [
        "fp, fn, tp, tn, accuracy, precision, recall = predict(model_2, val_iterator)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nb89RjHu5Lhx",
        "outputId": "61cfe800-8d0c-47cf-cdda-7fa61ce38167"
      },
      "source": [
        "print('accuracy:', accuracy)\n",
        "print('precision:', precision)\n",
        "print('recall:', recall)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.9994\n",
            "precision: 0.9990174887011201\n",
            "recall: 0.999803343166175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK9YMtuI5fe8"
      },
      "source": [
        "import re\n",
        "def preprocess_text(text):\n",
        "    text = text.lower().replace(\"ё\", \"е\")\n",
        "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', text)\n",
        "    text = re.sub('@[^\\s]+', 'USER', text)\n",
        "    text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    return text.strip()"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OKPSSOdmfzv",
        "outputId": "787a9fb9-0fca-4747-d5d0-880feb17d615"
      },
      "source": [
        "vocab_sym = Counter()\n",
        "for text in tweets_data['text']:\n",
        "    vocab_sym.update(preprocess1(preprocess_text(text)))\n",
        "print('всего уникальных токенов:', len(vocab_sym))\n",
        "\n",
        "filtered_vocab_sym = set()\n",
        "\n",
        "for symbol in vocab_sym:\n",
        "    if vocab_sym[symbol] > 5:\n",
        "        filtered_vocab_sym.add(symbol)\n",
        "print('уникальных символов, втретившихся больше 5 раз:', len(filtered_vocab_sym))\n",
        "\n",
        "symbol2id = {'PAD':0}\n",
        "\n",
        "for symbol in filtered_vocab_sym:\n",
        "    symbol2id[symbol] = len(symbol2id)\n",
        "\n",
        "id2symbol = {i:symbol for symbol, i in symbol2id.items()}"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "всего уникальных токенов: 73\n",
            "уникальных символов, втретившихся больше 5 раз: 73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YjXTzdpmoEe"
      },
      "source": [
        "val_dataset = TweetsDataset_2(val_sentences, word2id, symbol2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)\n",
        "\n",
        "train_dataset = TweetsDataset_2(train_sentences, word2id, symbol2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)\n",
        "\n",
        "batch = next(iter(train_iterator))"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7-oXz_FnBO2",
        "outputId": "9147c428-5eb7-477b-cb57-3258024cd719"
      },
      "source": [
        "model_2 = CNN_2(len(word2id), len(symbol2id), 8)\n",
        "optimizer = optim.Adam(model_2.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "model_2 = model_2.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)\n",
        "\n",
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(20):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model_2, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model_2, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model_2, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.749869029968977\n",
            "Train loss: 0.717858663111022\n",
            "Train loss: 0.705201164484024\n",
            "Train loss: 0.6983190419068978\n",
            "Train loss: 0.6931087275346121\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7120740450918674, Val f1: 0.6139716506004333\n",
            "Val loss: 0.6898128697366426, Val f1: 0.5977404117584229\n",
            "Val loss: 0.683194522857666, Val f1: 0.5898994207382202\n",
            "Val loss: 0.6804384375686077, Val f1: 0.5856269598007202\n",
            "Val loss: 0.678553078855787, Val f1: 0.5837620496749878\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.3426162004470825, Val f1: 1.135582447052002\n",
            "Val loss: 0.8935660918553671, Val f1: 0.7708638906478882\n",
            "Val loss: 0.8060264348983764, Val f1: 0.6840042471885681\n",
            "Val loss: 0.7676125594547817, Val f1: 0.6538643836975098\n",
            "Val loss: 0.7461747858259413, Val f1: 0.6361759901046753\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.712755598127842\n",
            "Train loss: 0.6892282131946448\n",
            "Train loss: 0.6803616893291473\n",
            "Train loss: 0.6753336342413034\n",
            "Train loss: 0.6720490725267501\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6996083632111549, Val f1: 0.6721434593200684\n",
            "Val loss: 0.6793179060473586, Val f1: 0.650371789932251\n",
            "Val loss: 0.6715376317501068, Val f1: 0.6458430886268616\n",
            "Val loss: 0.6677067484428634, Val f1: 0.6429355144500732\n",
            "Val loss: 0.6659825273922512, Val f1: 0.6400488018989563\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.3169223070144653, Val f1: 1.2431623935699463\n",
            "Val loss: 0.8773898681004842, Val f1: 0.839617908000946\n",
            "Val loss: 0.7917157888412476, Val f1: 0.7522546052932739\n",
            "Val loss: 0.7535201225961957, Val f1: 0.7167044878005981\n",
            "Val loss: 0.7326493130789863, Val f1: 0.6977294683456421\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.6979903429746628\n",
            "Train loss: 0.6759079189011545\n",
            "Train loss: 0.6686792755126953\n",
            "Train loss: 0.6645976491828463\n",
            "Train loss: 0.6619167242731366\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6913431882858276, Val f1: 0.6755462884902954\n",
            "Val loss: 0.6683192162802725, Val f1: 0.6554651856422424\n",
            "Val loss: 0.6625697517395019, Val f1: 0.6473199129104614\n",
            "Val loss: 0.6596032247614505, Val f1: 0.6433961987495422\n",
            "Val loss: 0.6575268663111187, Val f1: 0.6427139043807983\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.3003622889518738, Val f1: 1.2601569890975952\n",
            "Val loss: 0.8658181627591451, Val f1: 0.8472164869308472\n",
            "Val loss: 0.7818695425987243, Val f1: 0.7571892142295837\n",
            "Val loss: 0.7443864771298, Val f1: 0.7230280637741089\n",
            "Val loss: 0.7241339749760098, Val f1: 0.702236533164978\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.6885760761797428\n",
            "Train loss: 0.6685220075376106\n",
            "Train loss: 0.6612504661083222\n",
            "Train loss: 0.6574946828742525\n",
            "Train loss: 0.6549140307165328\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6848891973495483, Val f1: 0.690562903881073\n",
            "Val loss: 0.6640858812765642, Val f1: 0.6688174605369568\n",
            "Val loss: 0.6571552217006683, Val f1: 0.662074863910675\n",
            "Val loss: 0.6537781224321964, Val f1: 0.6575946807861328\n",
            "Val loss: 0.6515568650904155, Val f1: 0.6565627455711365\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2876072525978088, Val f1: 1.2905077934265137\n",
            "Val loss: 0.8575782775878906, Val f1: 0.8616757392883301\n",
            "Val loss: 0.7748703718185425, Val f1: 0.7715571522712708\n",
            "Val loss: 0.7378054176058088, Val f1: 0.7364186644554138\n",
            "Val loss: 0.7179294427235922, Val f1: 0.714717447757721\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.6797727718949318\n",
            "Train loss: 0.661580255537322\n",
            "Train loss: 0.6559931683540344\n",
            "Train loss: 0.6521484335856651\n",
            "Train loss: 0.6496111331951051\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6803111843764782, Val f1: 0.6464735865592957\n",
            "Val loss: 0.6597894556594618, Val f1: 0.6287131309509277\n",
            "Val loss: 0.6528251934051513, Val f1: 0.6205701231956482\n",
            "Val loss: 0.6494694939300195, Val f1: 0.6179460287094116\n",
            "Val loss: 0.6476739212161019, Val f1: 0.6156013011932373\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2833863496780396, Val f1: 1.2050013542175293\n",
            "Val loss: 0.8520583311716715, Val f1: 0.8122271299362183\n",
            "Val loss: 0.770345401763916, Val f1: 0.722062885761261\n",
            "Val loss: 0.7342441933495658, Val f1: 0.6885917782783508\n",
            "Val loss: 0.7148952947722541, Val f1: 0.6680718660354614\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.6771685369312763\n",
            "Train loss: 0.6554960283366117\n",
            "Train loss: 0.6498995995521546\n",
            "Train loss: 0.6469617530481139\n",
            "Train loss: 0.6443532655636469\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6781298257410526, Val f1: 0.6671720147132874\n",
            "Val loss: 0.654187565500086, Val f1: 0.6498456001281738\n",
            "Val loss: 0.646522409915924, Val f1: 0.6468152403831482\n",
            "Val loss: 0.6432586819378298, Val f1: 0.6440492272377014\n",
            "Val loss: 0.6416699382520857, Val f1: 0.6424121260643005\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.271215796470642, Val f1: 1.2836827039718628\n",
            "Val loss: 0.8448203603426615, Val f1: 0.8537760972976685\n",
            "Val loss: 0.7639073848724365, Val f1: 0.7584689259529114\n",
            "Val loss: 0.7279326064246041, Val f1: 0.7226049900054932\n",
            "Val loss: 0.7087973687383864, Val f1: 0.7007448673248291\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.6742801889777184\n",
            "Train loss: 0.6518739479960818\n",
            "Train loss: 0.6453839802742004\n",
            "Train loss: 0.6421503122173139\n",
            "Train loss: 0.6404436102935246\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6712723076343536, Val f1: 0.7155382037162781\n",
            "Val loss: 0.651689507744529, Val f1: 0.6953557729721069\n",
            "Val loss: 0.6446156787872315, Val f1: 0.6870800852775574\n",
            "Val loss: 0.6412777491469881, Val f1: 0.6835955381393433\n",
            "Val loss: 0.6381527675049645, Val f1: 0.6825737357139587\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.262198030948639, Val f1: 1.350430965423584\n",
            "Val loss: 0.8409159779548645, Val f1: 0.8988552093505859\n",
            "Val loss: 0.7603916764259339, Val f1: 0.8031836748123169\n",
            "Val loss: 0.7241232820919582, Val f1: 0.7648484706878662\n",
            "Val loss: 0.7049040264553494, Val f1: 0.7433068752288818\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.668982245028019\n",
            "Train loss: 0.6486186493526805\n",
            "Train loss: 0.6426367020606994\n",
            "Train loss: 0.6392261937483034\n",
            "Train loss: 0.6369642601126716\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6667172722518444, Val f1: 0.6776363849639893\n",
            "Val loss: 0.6472850875421003, Val f1: 0.6578296422958374\n",
            "Val loss: 0.6402839267253876, Val f1: 0.6493778228759766\n",
            "Val loss: 0.6370683952943602, Val f1: 0.6470733880996704\n",
            "Val loss: 0.6344942670492899, Val f1: 0.6473788619041443\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2594440579414368, Val f1: 1.2916532754898071\n",
            "Val loss: 0.8360388875007629, Val f1: 0.8584815859794617\n",
            "Val loss: 0.756389570236206, Val f1: 0.7619826197624207\n",
            "Val loss: 0.7208129848752703, Val f1: 0.7261443734169006\n",
            "Val loss: 0.7019105156262716, Val f1: 0.7053406238555908\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.664138924330473\n",
            "Train loss: 0.6448366515564196\n",
            "Train loss: 0.6367591273784637\n",
            "Train loss: 0.6337945977253701\n",
            "Train loss: 0.6323122367972419\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6651679687201977, Val f1: 0.7009519934654236\n",
            "Val loss: 0.6423166340047662, Val f1: 0.684459924697876\n",
            "Val loss: 0.635252389907837, Val f1: 0.6790376901626587\n",
            "Val loss: 0.6319131610998466, Val f1: 0.6753577589988708\n",
            "Val loss: 0.629780017903873, Val f1: 0.6722294092178345\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2501108646392822, Val f1: 1.3360159397125244\n",
            "Val loss: 0.8307794729868571, Val f1: 0.8898918032646179\n",
            "Val loss: 0.7518881320953369, Val f1: 0.7925360202789307\n",
            "Val loss: 0.7162297708647591, Val f1: 0.755021870136261\n",
            "Val loss: 0.6973094940185547, Val f1: 0.7329928874969482\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.6610129214823246\n",
            "Train loss: 0.6409082972642147\n",
            "Train loss: 0.6348107814788818\n",
            "Train loss: 0.6311370365655244\n",
            "Train loss: 0.6289032364175433\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6571343205869198, Val f1: 0.6950691342353821\n",
            "Val loss: 0.6369449676889362, Val f1: 0.6746193766593933\n",
            "Val loss: 0.6319723081588745, Val f1: 0.6679728031158447\n",
            "Val loss: 0.6281576868313462, Val f1: 0.6638988256454468\n",
            "Val loss: 0.6265427207662946, Val f1: 0.6616979241371155\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.246486783027649, Val f1: 1.3072633743286133\n",
            "Val loss: 0.8269776503245035, Val f1: 0.8737998008728027\n",
            "Val loss: 0.7489463448524475, Val f1: 0.775052011013031\n",
            "Val loss: 0.7135338698114667, Val f1: 0.7381841540336609\n",
            "Val loss: 0.6947686208619012, Val f1: 0.7171114683151245\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.6613070964813232\n",
            "Train loss: 0.6394821080294523\n",
            "Train loss: 0.6326508557796479\n",
            "Train loss: 0.6285059469849316\n",
            "Train loss: 0.6253851041907356\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.652562540024519, Val f1: 0.7181959748268127\n",
            "Val loss: 0.6348339012174895, Val f1: 0.6939021348953247\n",
            "Val loss: 0.6277937495708465, Val f1: 0.6873800754547119\n",
            "Val loss: 0.6243707422000259, Val f1: 0.6838111281394958\n",
            "Val loss: 0.6227417595329738, Val f1: 0.6812931299209595\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2378848791122437, Val f1: 1.3456909656524658\n",
            "Val loss: 0.8221990863482157, Val f1: 0.8998684883117676\n",
            "Val loss: 0.7451763868331909, Val f1: 0.8005738258361816\n",
            "Val loss: 0.7094995209148952, Val f1: 0.7620583176612854\n",
            "Val loss: 0.6906176275677152, Val f1: 0.739363968372345\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.6527416408061981\n",
            "Train loss: 0.6342171051285483\n",
            "Train loss: 0.6268594014644623\n",
            "Train loss: 0.623592860663115\n",
            "Train loss: 0.6221076250076294\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6519365273416042, Val f1: 0.7169614434242249\n",
            "Val loss: 0.6309084567156705, Val f1: 0.6970459818840027\n",
            "Val loss: 0.6254295706748962, Val f1: 0.6897499561309814\n",
            "Val loss: 0.6221952153675592, Val f1: 0.6867315769195557\n",
            "Val loss: 0.6197476429598672, Val f1: 0.6852713823318481\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2326531410217285, Val f1: 1.3510828018188477\n",
            "Val loss: 0.8185546398162842, Val f1: 0.9019861221313477\n",
            "Val loss: 0.7421589493751526, Val f1: 0.8037330508232117\n",
            "Val loss: 0.7064852033342633, Val f1: 0.7664156556129456\n",
            "Val loss: 0.6876406139797635, Val f1: 0.7425926327705383\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.6513710804283619\n",
            "Train loss: 0.6305883237809846\n",
            "Train loss: 0.6235477340221405\n",
            "Train loss: 0.6202364359328996\n",
            "Train loss: 0.6191446774062657\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6483006477355957, Val f1: 0.7296373248100281\n",
            "Val loss: 0.6281112902092211, Val f1: 0.7084205150604248\n",
            "Val loss: 0.6208757150173188, Val f1: 0.6995924115180969\n",
            "Val loss: 0.6182719655890963, Val f1: 0.695997416973114\n",
            "Val loss: 0.6168898613679976, Val f1: 0.6941617727279663\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2285059094429016, Val f1: 1.3591020107269287\n",
            "Val loss: 0.8161794145901998, Val f1: 0.9091407656669617\n",
            "Val loss: 0.7402654767036438, Val f1: 0.8094776272773743\n",
            "Val loss: 0.7044480272701809, Val f1: 0.7731529474258423\n",
            "Val loss: 0.6855150130059984, Val f1: 0.7481113076210022\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.6459711119532585\n",
            "Train loss: 0.6283178022413543\n",
            "Train loss: 0.6214637684822083\n",
            "Train loss: 0.6186519033873259\n",
            "Train loss: 0.6170278631505512\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6455278284847736, Val f1: 0.7006434798240662\n",
            "Val loss: 0.6259981755054358, Val f1: 0.6820046305656433\n",
            "Val loss: 0.6183433699607849, Val f1: 0.6774054765701294\n",
            "Val loss: 0.6154238919713604, Val f1: 0.6735659837722778\n",
            "Val loss: 0.6148810301508222, Val f1: 0.6695132255554199\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2290157079696655, Val f1: 1.3056607246398926\n",
            "Val loss: 0.8141534725824991, Val f1: 0.8776237368583679\n",
            "Val loss: 0.7386296272277832, Val f1: 0.7793292999267578\n",
            "Val loss: 0.7034223079681396, Val f1: 0.7419687509536743\n",
            "Val loss: 0.6847312516636319, Val f1: 0.7182741761207581\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.6421132013201714\n",
            "Train loss: 0.624509592851003\n",
            "Train loss: 0.6183915185928345\n",
            "Train loss: 0.6160369648862241\n",
            "Train loss: 0.6138631715661004\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6397935189306736, Val f1: 0.7268377542495728\n",
            "Val loss: 0.6226842421473879, Val f1: 0.7016599178314209\n",
            "Val loss: 0.6175347173213959, Val f1: 0.6940373182296753\n",
            "Val loss: 0.6139373619164994, Val f1: 0.6917786598205566\n",
            "Val loss: 0.6119264789990017, Val f1: 0.6891206502914429\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2213846445083618, Val f1: 1.3394923210144043\n",
            "Val loss: 0.8104624350865682, Val f1: 0.9000889658927917\n",
            "Val loss: 0.7354603290557862, Val f1: 0.8018864989280701\n",
            "Val loss: 0.6999387315341404, Val f1: 0.7660857439041138\n",
            "Val loss: 0.6811280846595764, Val f1: 0.7422600388526917\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.6459253802895546\n",
            "Train loss: 0.6240210659576185\n",
            "Train loss: 0.6164901459217071\n",
            "Train loss: 0.6145216929378794\n",
            "Train loss: 0.612013483331317\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6434774063527584, Val f1: 0.7070943117141724\n",
            "Val loss: 0.6244878696672844, Val f1: 0.6861057281494141\n",
            "Val loss: 0.6169984948635101, Val f1: 0.6805846095085144\n",
            "Val loss: 0.613450684654179, Val f1: 0.6757364869117737\n",
            "Val loss: 0.6112126814467567, Val f1: 0.6750311851501465\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2212356925010681, Val f1: 1.316161870956421\n",
            "Val loss: 0.8093307614326477, Val f1: 0.8833296298980713\n",
            "Val loss: 0.7343834042549133, Val f1: 0.7855430841445923\n",
            "Val loss: 0.6991971390587943, Val f1: 0.750222384929657\n",
            "Val loss: 0.6805135673946805, Val f1: 0.7249835729598999\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.6419542394578457\n",
            "Train loss: 0.6203454841266979\n",
            "Train loss: 0.6144588708877563\n",
            "Train loss: 0.6118486034336375\n",
            "Train loss: 0.6102408064263207\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6398153640329838, Val f1: 0.721035897731781\n",
            "Val loss: 0.6173888372652458, Val f1: 0.7055306434631348\n",
            "Val loss: 0.6107641196250916, Val f1: 0.6994967460632324\n",
            "Val loss: 0.6086426012551607, Val f1: 0.6944992542266846\n",
            "Val loss: 0.6079279532035192, Val f1: 0.6916124224662781\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.215226411819458, Val f1: 1.3503108024597168\n",
            "Val loss: 0.8067147533098856, Val f1: 0.9039530158042908\n",
            "Val loss: 0.7319353103637696, Val f1: 0.8054940104484558\n",
            "Val loss: 0.696520847933633, Val f1: 0.7686565518379211\n",
            "Val loss: 0.6776980691485934, Val f1: 0.7444387078285217\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.6402257531881332\n",
            "Train loss: 0.6196073925856388\n",
            "Train loss: 0.6136332440376282\n",
            "Train loss: 0.6104438180354104\n",
            "Train loss: 0.6084110772325879\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.639656163752079, Val f1: 0.7363569140434265\n",
            "Val loss: 0.6190488482966567, Val f1: 0.7142669558525085\n",
            "Val loss: 0.6133688461780548, Val f1: 0.7073349356651306\n",
            "Val loss: 0.6098529402889422, Val f1: 0.7056936621665955\n",
            "Val loss: 0.6068124998183477, Val f1: 0.7033422589302063\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2123277187347412, Val f1: 1.3714323043823242\n",
            "Val loss: 0.8059680859247843, Val f1: 0.9147106409072876\n",
            "Val loss: 0.7312657117843628, Val f1: 0.8182008862495422\n",
            "Val loss: 0.6954789502280099, Val f1: 0.7814452648162842\n",
            "Val loss: 0.6765269372198317, Val f1: 0.7576292753219604\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.6389421299099922\n",
            "Train loss: 0.6195619521719037\n",
            "Train loss: 0.613145170211792\n",
            "Train loss: 0.6091247031937784\n",
            "Train loss: 0.6067784201531183\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6334179006516933, Val f1: 0.7260082364082336\n",
            "Val loss: 0.6147601767019792, Val f1: 0.706236720085144\n",
            "Val loss: 0.6091895592212677, Val f1: 0.6990176439285278\n",
            "Val loss: 0.6064545449925892, Val f1: 0.6969730257987976\n",
            "Val loss: 0.6044459513255528, Val f1: 0.6944478750228882\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2093660831451416, Val f1: 1.3476803302764893\n",
            "Val loss: 0.803308884302775, Val f1: 0.9008029699325562\n",
            "Val loss: 0.7290277719497681, Val f1: 0.8044922947883606\n",
            "Val loss: 0.6935565301350185, Val f1: 0.7679353952407837\n",
            "Val loss: 0.6748483578364054, Val f1: 0.7441896796226501\n",
            "\n",
            "starting Epoch 19\n",
            "Training...\n",
            "Train loss: 0.6307751163840294\n",
            "Train loss: 0.6111504236857096\n",
            "Train loss: 0.6083826065063477\n",
            "Train loss: 0.6064322315045257\n",
            "Train loss: 0.6050835627885092\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6318282149732113, Val f1: 0.7465651631355286\n",
            "Val loss: 0.6162554513324391, Val f1: 0.7193325161933899\n",
            "Val loss: 0.6081116557121277, Val f1: 0.7142488956451416\n",
            "Val loss: 0.6059407404999235, Val f1: 0.7105787396430969\n",
            "Val loss: 0.6038342707213902, Val f1: 0.7090962529182434\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2073830962181091, Val f1: 1.3847159147262573\n",
            "Val loss: 0.8035336335500082, Val f1: 0.921752393245697\n",
            "Val loss: 0.7291029691696167, Val f1: 0.8256651163101196\n",
            "Val loss: 0.693179726600647, Val f1: 0.7874740958213806\n",
            "Val loss: 0.6742916173405118, Val f1: 0.7637661099433899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DiJzDkVnFGe",
        "outputId": "28027b55-24a5-426a-c26c-54ab2040b3c8"
      },
      "source": [
        "fp, fn, tp, tn, accuracy, precision, recall = predict(model_2, val_iterator)\n",
        "print('accuracy:', accuracy)\n",
        "print('precision:', precision)\n",
        "print('recall:', recall)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.6665\n",
            "precision: 0.6565295169946332\n",
            "recall: 0.7217305801376598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEyfRfTWn2W7",
        "outputId": "176f734c-41bf-427e-b41e-2308918a85f4"
      },
      "source": [
        "best_acc = 0\n",
        "lr_val = ''\n",
        "\n",
        "for lr in [0.0005, 0.001, 0.005, 0.01, 0.05]:\n",
        "    model_2 = CNN_2(len(word2id), len(symbol2id), 8)\n",
        "    optimizer = optim.Adam(model_2.parameters(), lr=lr)\n",
        "    criterion = nn.BCELoss()  \n",
        "\n",
        "    # веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "    model_2 = model_2.to(DEVICE)\n",
        "    criterion = criterion.to(DEVICE)\n",
        "\n",
        "    losses = []\n",
        "    losses_eval = []\n",
        "    f1s = []\n",
        "    f1s_eval = []\n",
        "\n",
        "    for i in range(10):\n",
        "        print(f'starting Epoch {i}')\n",
        "        epoch_loss = train(model_2, train_iterator, optimizer, criterion, print_v=False)\n",
        "        losses.append(epoch_loss)\n",
        "        f1_on_train,_ = evaluate(model_2, train_iterator, criterion, print_v=False)\n",
        "        f1s.append(f1_on_train)\n",
        "        f1_on_test, epoch_loss_on_test = evaluate(model_2, val_iterator, criterion, print_v=False)\n",
        "        losses_eval.append(epoch_loss_on_test)\n",
        "        f1s_eval.append(f1_on_test)\n",
        "\n",
        "    fp, fn, tp, tn, accuracy, precision, recall = predict(model_2, val_iterator)\n",
        "    print(f'results for lr {lr}')\n",
        "    print('accuracy:', accuracy)\n",
        "    print('precision:', precision)\n",
        "    print('recall:', recall)\n",
        "    if accuracy > best_acc:\n",
        "      best_acc = accuracy\n",
        "      lr_val = lr\n",
        "print(f'____________________________________________')\n",
        "print(f'best accuracy is {best_acc} for lr {lr_val}')"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for lr 0.0005\n",
            "accuracy: 0.6483\n",
            "precision: 0.6649137568363483\n",
            "recall: 0.6216322517207473\n",
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for lr 0.001\n",
            "accuracy: 0.6636\n",
            "precision: 0.6787123572170302\n",
            "recall: 0.6426745329400196\n",
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for lr 0.005\n",
            "accuracy: 0.6836\n",
            "precision: 0.6566628608709836\n",
            "recall: 0.791740412979351\n",
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for lr 0.01\n",
            "accuracy: 0.6865\n",
            "precision: 0.7218934911242604\n",
            "recall: 0.623795476892822\n",
            "starting Epoch 0\n",
            "starting Epoch 1\n",
            "starting Epoch 2\n",
            "starting Epoch 3\n",
            "starting Epoch 4\n",
            "starting Epoch 5\n",
            "starting Epoch 6\n",
            "starting Epoch 7\n",
            "starting Epoch 8\n",
            "starting Epoch 9\n",
            "results for lr 0.05\n",
            "accuracy: 0.6757\n",
            "precision: 0.7060402684563758\n",
            "recall: 0.6206489675516225\n",
            "____________________________________________\n",
            "best accuracy is 0.6865 for lr 0.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dMpObq3pqMz",
        "outputId": "ace7c7c4-115b-4e45-ebe6-9dcbc9bd9377"
      },
      "source": [
        "model_2 = CNN_2(len(word2id), len(symbol2id), 8)\n",
        "optimizer = optim.Adam(model_2.parameters(), lr=0.01)\n",
        "criterion = nn.BCELoss()  \n",
        "model_2 = model_2.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)\n",
        "\n",
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(19):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model_2, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model_2, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model_2, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.8855736292898655\n",
            "Train loss: 0.780337778004733\n",
            "Train loss: 0.74134920835495\n",
            "Train loss: 0.7206962037442336\n",
            "Train loss: 0.7067776499759584\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6867129094898701, Val f1: 0.6478328108787537\n",
            "Val loss: 0.6650897372852672, Val f1: 0.6259854435920715\n",
            "Val loss: 0.6582334089279175, Val f1: 0.6201726198196411\n",
            "Val loss: 0.6553518184975012, Val f1: 0.6169167757034302\n",
            "Val loss: 0.6529026443050021, Val f1: 0.6160329580307007\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2900512218475342, Val f1: 1.220392107963562\n",
            "Val loss: 0.8577924569447836, Val f1: 0.8196989893913269\n",
            "Val loss: 0.7776997447013855, Val f1: 0.7232353091239929\n",
            "Val loss: 0.7411121215139117, Val f1: 0.6890925168991089\n",
            "Val loss: 0.7208931512302823, Val f1: 0.6689637303352356\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.6832228563725948\n",
            "Train loss: 0.6627730600761644\n",
            "Train loss: 0.6575409841537475\n",
            "Train loss: 0.652228824238279\n",
            "Train loss: 0.6475767550014314\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6595632806420326, Val f1: 0.6853426098823547\n",
            "Val loss: 0.6382350163026289, Val f1: 0.6672883629798889\n",
            "Val loss: 0.6318282532691956, Val f1: 0.661404013633728\n",
            "Val loss: 0.6295673909471996, Val f1: 0.6582493185997009\n",
            "Val loss: 0.6262240147306806, Val f1: 0.6589090824127197\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2387698888778687, Val f1: 1.330203890800476\n",
            "Val loss: 0.8229229052861532, Val f1: 0.8765920400619507\n",
            "Val loss: 0.748499071598053, Val f1: 0.773710310459137\n",
            "Val loss: 0.7134645581245422, Val f1: 0.7358627915382385\n",
            "Val loss: 0.6942368348439535, Val f1: 0.7158926129341125\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.6650161780416965\n",
            "Train loss: 0.6396613319714864\n",
            "Train loss: 0.6304562592506409\n",
            "Train loss: 0.6249078138550715\n",
            "Train loss: 0.622079148888588\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6560969166457653, Val f1: 0.6172614097595215\n",
            "Val loss: 0.6346097136988784, Val f1: 0.5991788506507874\n",
            "Val loss: 0.6298248898983002, Val f1: 0.5924356579780579\n",
            "Val loss: 0.6258377157040497, Val f1: 0.589464008808136\n",
            "Val loss: 0.6244432741687411, Val f1: 0.5862134695053101\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2375428676605225, Val f1: 1.1369116306304932\n",
            "Val loss: 0.819074551264445, Val f1: 0.7639570236206055\n",
            "Val loss: 0.7479271173477173, Val f1: 0.6801403164863586\n",
            "Val loss: 0.7134725025721959, Val f1: 0.6459102630615234\n",
            "Val loss: 0.6960522267553542, Val f1: 0.6226715445518494\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.6420972160995007\n",
            "Train loss: 0.6173538601759708\n",
            "Train loss: 0.6108663737773895\n",
            "Train loss: 0.6083298192095401\n",
            "Train loss: 0.606122612953186\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6332841664552689, Val f1: 0.7767934799194336\n",
            "Val loss: 0.6177986065546671, Val f1: 0.7456648349761963\n",
            "Val loss: 0.613280177116394, Val f1: 0.7359349727630615\n",
            "Val loss: 0.6086682454863591, Val f1: 0.7337539196014404\n",
            "Val loss: 0.6065319663002378, Val f1: 0.7319840788841248\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2075225710868835, Val f1: 1.4508707523345947\n",
            "Val loss: 0.8086876273155212, Val f1: 0.9562780261039734\n",
            "Val loss: 0.736395788192749, Val f1: 0.8569586873054504\n",
            "Val loss: 0.6989575028419495, Val f1: 0.8210303783416748\n",
            "Val loss: 0.6810444129837884, Val f1: 0.7969209551811218\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.6317029483616352\n",
            "Train loss: 0.6068217356999716\n",
            "Train loss: 0.6064703488349914\n",
            "Train loss: 0.6062345478072095\n",
            "Train loss: 0.6063976543290275\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6174098141491413, Val f1: 0.7652288675308228\n",
            "Val loss: 0.5971985784443942, Val f1: 0.7411889433860779\n",
            "Val loss: 0.5919877827167511, Val f1: 0.7344809770584106\n",
            "Val loss: 0.589513334765363, Val f1: 0.7295552492141724\n",
            "Val loss: 0.5887434078114373, Val f1: 0.7266209721565247\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1771769523620605, Val f1: 1.4285283088684082\n",
            "Val loss: 0.7861486474672953, Val f1: 0.9454336166381836\n",
            "Val loss: 0.7160510897636414, Val f1: 0.8452001810073853\n",
            "Val loss: 0.6799951366015843, Val f1: 0.8080604076385498\n",
            "Val loss: 0.6635467476314969, Val f1: 0.7834240794181824\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.6191873140633106\n",
            "Train loss: 0.5975524114839958\n",
            "Train loss: 0.592069857120514\n",
            "Train loss: 0.5901744427965648\n",
            "Train loss: 0.5877237781172707\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6085972003638744, Val f1: 0.777248740196228\n",
            "Val loss: 0.5924825198722609, Val f1: 0.7519906759262085\n",
            "Val loss: 0.5869301402568817, Val f1: 0.7429471611976624\n",
            "Val loss: 0.5831648609531459, Val f1: 0.7397617101669312\n",
            "Val loss: 0.5823983308814821, Val f1: 0.7367039322853088\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.173989713191986, Val f1: 1.4596118927001953\n",
            "Val loss: 0.7830969293912252, Val f1: 0.9597977995872498\n",
            "Val loss: 0.7126811742782593, Val f1: 0.859891414642334\n",
            "Val loss: 0.6769918288503375, Val f1: 0.8225014209747314\n",
            "Val loss: 0.659443649980757, Val f1: 0.7985792756080627\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.6047790721058846\n",
            "Train loss: 0.5897160840756965\n",
            "Train loss: 0.5863532769680023\n",
            "Train loss: 0.5848796429918773\n",
            "Train loss: 0.5834090028490339\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6005609408020973, Val f1: 0.7628505229949951\n",
            "Val loss: 0.5834279800906326, Val f1: 0.7374341487884521\n",
            "Val loss: 0.577086820602417, Val f1: 0.7304900288581848\n",
            "Val loss: 0.572804890461822, Val f1: 0.7289609313011169\n",
            "Val loss: 0.5705191124053228, Val f1: 0.7260610461235046\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1520469188690186, Val f1: 1.425309181213379\n",
            "Val loss: 0.7673541903495789, Val f1: 0.9371438026428223\n",
            "Val loss: 0.6992625594139099, Val f1: 0.8363467454910278\n",
            "Val loss: 0.6660939284733364, Val f1: 0.7956672310829163\n",
            "Val loss: 0.6494730843438042, Val f1: 0.7736042737960815\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.603100560605526\n",
            "Train loss: 0.5857571995619572\n",
            "Train loss: 0.5828136694431305\n",
            "Train loss: 0.582363633077536\n",
            "Train loss: 0.5825135729142598\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.6314204707741737, Val f1: 0.778997540473938\n",
            "Val loss: 0.6112805279818448, Val f1: 0.7573395371437073\n",
            "Val loss: 0.6037474858760834, Val f1: 0.7512726187705994\n",
            "Val loss: 0.5999493536664479, Val f1: 0.7479141354560852\n",
            "Val loss: 0.5976596850724447, Val f1: 0.7457038760185242\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2089356780052185, Val f1: 1.4495333433151245\n",
            "Val loss: 0.8104494214057922, Val f1: 0.9646133780479431\n",
            "Val loss: 0.7349290132522583, Val f1: 0.8688438534736633\n",
            "Val loss: 0.6967708212988717, Val f1: 0.8332369923591614\n",
            "Val loss: 0.6788134309980605, Val f1: 0.8096833229064941\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.6133047416806221\n",
            "Train loss: 0.604558092175108\n",
            "Train loss: 0.5959084939956665\n",
            "Train loss: 0.5908197484799286\n",
            "Train loss: 0.5864831712983903\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5892151668667793, Val f1: 0.7734174132347107\n",
            "Val loss: 0.5744206598310759, Val f1: 0.7473924160003662\n",
            "Val loss: 0.5682481825351715, Val f1: 0.740547239780426\n",
            "Val loss: 0.5646335215710881, Val f1: 0.7385656833648682\n",
            "Val loss: 0.5629782187087196, Val f1: 0.7353244423866272\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.14614999294281, Val f1: 1.4503107070922852\n",
            "Val loss: 0.7635810573895773, Val f1: 0.9469127058982849\n",
            "Val loss: 0.6941454291343689, Val f1: 0.8482469916343689\n",
            "Val loss: 0.6615893755640302, Val f1: 0.8060702681541443\n",
            "Val loss: 0.6454842355516222, Val f1: 0.783255398273468\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.5938696786761284\n",
            "Train loss: 0.5825786066777778\n",
            "Train loss: 0.5754549086093903\n",
            "Train loss: 0.5754402813626759\n",
            "Train loss: 0.5761930552266893\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5938917547464371, Val f1: 0.7557226419448853\n",
            "Val loss: 0.5750414602684252, Val f1: 0.7325428128242493\n",
            "Val loss: 0.5676322770118714, Val f1: 0.7268050312995911\n",
            "Val loss: 0.5635859806146195, Val f1: 0.7240183353424072\n",
            "Val loss: 0.560853123664856, Val f1: 0.7215225100517273\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1468168497085571, Val f1: 1.4064521789550781\n",
            "Val loss: 0.7621607780456543, Val f1: 0.9291329383850098\n",
            "Val loss: 0.6940693497657776, Val f1: 0.8288716673851013\n",
            "Val loss: 0.6616564563342503, Val f1: 0.789033055305481\n",
            "Val loss: 0.6445916957325406, Val f1: 0.7658742666244507\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.593228030949831\n",
            "Train loss: 0.575551121523886\n",
            "Train loss: 0.5713602817058563\n",
            "Train loss: 0.570025378198766\n",
            "Train loss: 0.5680224051078161\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5860337689518929, Val f1: 0.7654628157615662\n",
            "Val loss: 0.5656692783037821, Val f1: 0.7434156537055969\n",
            "Val loss: 0.560035206079483, Val f1: 0.7355521321296692\n",
            "Val loss: 0.5575296700890384, Val f1: 0.730355441570282\n",
            "Val loss: 0.5560818172636486, Val f1: 0.7269278168678284\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.146490752696991, Val f1: 1.4023864269256592\n",
            "Val loss: 0.7609874606132507, Val f1: 0.9323139190673828\n",
            "Val loss: 0.6909644484519959, Val f1: 0.8325428366661072\n",
            "Val loss: 0.6590472800391061, Val f1: 0.7917430996894836\n",
            "Val loss: 0.642161144150628, Val f1: 0.768818199634552\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.6066082753241062\n",
            "Train loss: 0.5783582004633817\n",
            "Train loss: 0.5725527715682983\n",
            "Train loss: 0.5788716318002388\n",
            "Train loss: 0.5812116442691713\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5825306847691536, Val f1: 0.7721767425537109\n",
            "Val loss: 0.5665297219247529, Val f1: 0.7501203417778015\n",
            "Val loss: 0.5610553252696991, Val f1: 0.7418455481529236\n",
            "Val loss: 0.5585243257124033, Val f1: 0.7399179935455322\n",
            "Val loss: 0.5564978392351241, Val f1: 0.7386457920074463\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1445010900497437, Val f1: 1.4505391120910645\n",
            "Val loss: 0.7625728249549866, Val f1: 0.9541312456130981\n",
            "Val loss: 0.6930070519447327, Val f1: 0.8533709645271301\n",
            "Val loss: 0.6604841692107064, Val f1: 0.8116579651832581\n",
            "Val loss: 0.6437177393171523, Val f1: 0.7876627445220947\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.5905711241066456\n",
            "Train loss: 0.5741665056257537\n",
            "Train loss: 0.5686378824710846\n",
            "Train loss: 0.5699336982485074\n",
            "Train loss: 0.5733890732129415\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5858043655753136, Val f1: 0.7814894914627075\n",
            "Val loss: 0.5647413820931406, Val f1: 0.7579634785652161\n",
            "Val loss: 0.5608650410175323, Val f1: 0.7464249730110168\n",
            "Val loss: 0.5570634434472269, Val f1: 0.7441962361335754\n",
            "Val loss: 0.5547970881064733, Val f1: 0.7421444654464722\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1415683031082153, Val f1: 1.4611896276474\n",
            "Val loss: 0.7596928675969442, Val f1: 0.9615030288696289\n",
            "Val loss: 0.6912024736404419, Val f1: 0.8581000566482544\n",
            "Val loss: 0.6587285143988473, Val f1: 0.8157170414924622\n",
            "Val loss: 0.642449782954322, Val f1: 0.7918961644172668\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.6054015792906284\n",
            "Train loss: 0.581454047651002\n",
            "Train loss: 0.5733248686790466\n",
            "Train loss: 0.5681786777368233\n",
            "Train loss: 0.5669073519252595\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5765748396515846, Val f1: 0.7888389825820923\n",
            "Val loss: 0.56230121309107, Val f1: 0.7597037553787231\n",
            "Val loss: 0.5578923642635345, Val f1: 0.7509299516677856\n",
            "Val loss: 0.5539885136618543, Val f1: 0.7480719089508057\n",
            "Val loss: 0.5530274964514232, Val f1: 0.7460769414901733\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1436195373535156, Val f1: 1.4533092975616455\n",
            "Val loss: 0.7595233718554179, Val f1: 0.9559769034385681\n",
            "Val loss: 0.689491617679596, Val f1: 0.85775226354599\n",
            "Val loss: 0.6582686475345066, Val f1: 0.815723180770874\n",
            "Val loss: 0.6411863035625882, Val f1: 0.7926177382469177\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.5800770744681358\n",
            "Train loss: 0.563226170612104\n",
            "Train loss: 0.5611416530609131\n",
            "Train loss: 0.5613516729269454\n",
            "Train loss: 0.5599306552183061\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5794795118272305, Val f1: 0.7587780356407166\n",
            "Val loss: 0.5615474057920051, Val f1: 0.7365092635154724\n",
            "Val loss: 0.5549363040924072, Val f1: 0.7284350991249084\n",
            "Val loss: 0.5528143306276692, Val f1: 0.7237536311149597\n",
            "Val loss: 0.5506853070997056, Val f1: 0.7231037020683289\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1448756456375122, Val f1: 1.404700517654419\n",
            "Val loss: 0.760831872622172, Val f1: 0.9265908002853394\n",
            "Val loss: 0.6895780205726624, Val f1: 0.8274006843566895\n",
            "Val loss: 0.6585637501307896, Val f1: 0.7880625128746033\n",
            "Val loss: 0.6419018308321635, Val f1: 0.7622553706169128\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.5842121802270412\n",
            "Train loss: 0.5718282099926111\n",
            "Train loss: 0.5650117516517639\n",
            "Train loss: 0.5657697533493611\n",
            "Train loss: 0.5680227300950459\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5776945985853672, Val f1: 0.7637525796890259\n",
            "Val loss: 0.5573559674349698, Val f1: 0.7413727641105652\n",
            "Val loss: 0.5539320266246796, Val f1: 0.7324808835983276\n",
            "Val loss: 0.5518143862041075, Val f1: 0.7265822291374207\n",
            "Val loss: 0.5491030720018205, Val f1: 0.7256013751029968\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1403471231460571, Val f1: 1.3981387615203857\n",
            "Val loss: 0.7593246499697367, Val f1: 0.9231537580490112\n",
            "Val loss: 0.6890310287475586, Val f1: 0.824701726436615\n",
            "Val loss: 0.6594629458018711, Val f1: 0.784527599811554\n",
            "Val loss: 0.6426549024052091, Val f1: 0.7621505856513977\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.5829598270356655\n",
            "Train loss: 0.5632826342727199\n",
            "Train loss: 0.5628616368770599\n",
            "Train loss: 0.5599840514695467\n",
            "Train loss: 0.5616510723318372\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5837488360702991, Val f1: 0.7342196702957153\n",
            "Val loss: 0.5664697650707129, Val f1: 0.7107069492340088\n",
            "Val loss: 0.5617048585414887, Val f1: 0.7033019065856934\n",
            "Val loss: 0.5589571248239545, Val f1: 0.6988332867622375\n",
            "Val loss: 0.5579683525221688, Val f1: 0.6955550909042358\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1616603136062622, Val f1: 1.3376483917236328\n",
            "Val loss: 0.7708635926246643, Val f1: 0.8821396231651306\n",
            "Val loss: 0.699065113067627, Val f1: 0.7879825234413147\n",
            "Val loss: 0.6698389564241681, Val f1: 0.749401867389679\n",
            "Val loss: 0.6533899108568827, Val f1: 0.7281323671340942\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.5881646201014519\n",
            "Train loss: 0.5667569727608652\n",
            "Train loss: 0.5604913449287414\n",
            "Train loss: 0.5595711807706463\n",
            "Train loss: 0.5578419466813406\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5803815014660358, Val f1: 0.7871319651603699\n",
            "Val loss: 0.5632095481410171, Val f1: 0.7657829523086548\n",
            "Val loss: 0.5555339205265045, Val f1: 0.7584308981895447\n",
            "Val loss: 0.5523040846212587, Val f1: 0.7551031112670898\n",
            "Val loss: 0.5501968151047116, Val f1: 0.7539982199668884\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1497558951377869, Val f1: 1.4635348320007324\n",
            "Val loss: 0.7638178467750549, Val f1: 0.9662501811981201\n",
            "Val loss: 0.6931920170783996, Val f1: 0.8641588091850281\n",
            "Val loss: 0.6618528366088867, Val f1: 0.824509859085083\n",
            "Val loss: 0.6450563006930881, Val f1: 0.7995193600654602\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.5797889567911625\n",
            "Train loss: 0.571396650690021\n",
            "Train loss: 0.56741872549057\n",
            "Train loss: 0.5663769681062272\n",
            "Train loss: 0.5633333396343958\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5758508034050465, Val f1: 0.7940064668655396\n",
            "Val loss: 0.5601122144496802, Val f1: 0.7676369547843933\n",
            "Val loss: 0.5529366135597229, Val f1: 0.7583886384963989\n",
            "Val loss: 0.5503339874210642, Val f1: 0.7537443041801453\n",
            "Val loss: 0.5478882058745339, Val f1: 0.7525869011878967\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.144616186618805, Val f1: 1.4596272706985474\n",
            "Val loss: 0.7648943463961283, Val f1: 0.9598700404167175\n",
            "Val loss: 0.6927424073219299, Val f1: 0.8621668219566345\n",
            "Val loss: 0.6614838242530823, Val f1: 0.8214324116706848\n",
            "Val loss: 0.6436036626497904, Val f1: 0.79717618227005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIijX2zmvaTI",
        "outputId": "eb928404-213f-4c48-c124-bf3569f6e09e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "fp, fn, tp, tn, accuracy, precision, recall = predict(model_2, val_iterator)\n",
        "print('accuracy:', accuracy)\n",
        "print('precision:', precision)\n",
        "print('recall:', recall)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.6934\n",
            "precision: 0.6750476851049072\n",
            "recall: 0.7655850540806293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7cEy5zJvd7Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
